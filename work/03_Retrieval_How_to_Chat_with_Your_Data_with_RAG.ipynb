{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ybonbs9KfU7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jRANZqj8BSbd"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --quiet lark chromadb\n",
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain langchain_openai langchain_community langchain-text-splitters langchain-postgres\n",
        "# !pip install session_info\n",
        "# !pip install -U pydantic\n",
        "# !pip install lark=1.2.2\n",
        "# !pip install -U ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSuil8W6O5FT",
        "outputId": "9294eb7a-d3ef-479a-93aa-0506788e8bda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.3.14'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure you got version 0.3.14\n",
        "import langchain\n",
        "langchain.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "DN5VHXNNQV8q",
        "outputId": "fb333a9e-43fb-4794-b884-db9430d1e3a4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "dotenv                      NA\n",
              "langchain                   0.3.14\n",
              "langchain_community         0.3.14\n",
              "langchain_core              0.3.31\n",
              "langchain_openai            NA\n",
              "langchain_postgres          0.0.12\n",
              "langchain_text_splitters    NA\n",
              "pydantic                    2.10.5\n",
              "session_info                1.0.0\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "aiohappyeyeballs            2.4.4\n",
              "aiohttp                     3.11.11\n",
              "aiosignal                   1.3.2\n",
              "annotated_types             0.7.0\n",
              "anyio                       NA\n",
              "arrow                       1.3.0\n",
              "attr                        24.3.0\n",
              "attrs                       24.3.0\n",
              "babel                       2.16.0\n",
              "backcall                    0.2.0\n",
              "backoff                     2.2.1\n",
              "bs4                         4.12.3\n",
              "certifi                     2024.12.14\n",
              "chardet                     5.2.0\n",
              "charset_normalizer          3.4.1\n",
              "chromadb                    0.6.3\n",
              "click                       8.1.8\n",
              "colorama                    0.4.6\n",
              "comm                        0.2.2\n",
              "cython_runtime              NA\n",
              "dateutil                    2.9.0.post0\n",
              "debugpy                     1.8.12\n",
              "decorator                   5.1.1\n",
              "deprecated                  1.2.15\n",
              "distro                      1.9.0\n",
              "fastjsonschema              NA\n",
              "fqdn                        NA\n",
              "frozenlist                  1.5.0\n",
              "google                      NA\n",
              "graphlib                    NA\n",
              "greenlet                    3.1.1\n",
              "grpc                        1.69.0\n",
              "h11                         0.14.0\n",
              "hnswlib                     NA\n",
              "httpcore                    1.0.7\n",
              "httpx                       0.28.1\n",
              "idna                        3.10\n",
              "importlib_metadata          NA\n",
              "importlib_resources         NA\n",
              "ipykernel                   6.29.5\n",
              "isoduration                 NA\n",
              "jedi                        0.19.2\n",
              "jinja2                      3.1.5\n",
              "jiter                       0.8.2\n",
              "json5                       0.10.0\n",
              "jsonpatch                   1.33\n",
              "jsonpointer                 3.0.0\n",
              "jsonschema                  4.23.0\n",
              "jsonschema_specifications   NA\n",
              "jupyter_events              0.11.0\n",
              "jupyter_server              2.15.0\n",
              "jupyterlab_server           2.27.3\n",
              "langsmith                   0.2.11\n",
              "lark                        1.2.2\n",
              "markupsafe                  3.0.2\n",
              "monotonic                   NA\n",
              "multidict                   6.1.0\n",
              "nbformat                    5.10.4\n",
              "nt                          NA\n",
              "numpy                       1.26.4\n",
              "onnxruntime                 1.20.1\n",
              "openai                      1.59.9\n",
              "opentelemetry               NA\n",
              "orjson                      3.10.15\n",
              "overrides                   NA\n",
              "packaging                   24.2\n",
              "parso                       0.8.4\n",
              "pgvector                    NA\n",
              "pickleshare                 0.7.5\n",
              "platformdirs                4.3.6\n",
              "posthog                     3.9.2\n",
              "prometheus_client           NA\n",
              "prompt_toolkit              3.0.50\n",
              "propcache                   0.2.1\n",
              "psutil                      6.1.1\n",
              "psycopg                     3.2.4\n",
              "psycopg_binary              0.0.0.0\n",
              "pydantic_core               2.27.2\n",
              "pydantic_settings           2.7.1\n",
              "pydev_ipython               NA\n",
              "pydevconsole                NA\n",
              "pydevd                      3.2.3\n",
              "pydevd_file_utils           NA\n",
              "pydevd_plugins              NA\n",
              "pydevd_tracing              NA\n",
              "pygments                    2.19.1\n",
              "pypika                      0.48.9\n",
              "pythoncom                   NA\n",
              "pythonjsonlogger            NA\n",
              "pywin32_bootstrap           NA\n",
              "pywin32_system32            NA\n",
              "pywintypes                  NA\n",
              "referencing                 NA\n",
              "regex                       2.5.148\n",
              "requests                    2.32.3\n",
              "requests_toolbelt           1.0.0\n",
              "rfc3339_validator           0.1.4\n",
              "rfc3986_validator           0.1.1\n",
              "rich                        NA\n",
              "rpds                        NA\n",
              "send2trash                  NA\n",
              "six                         1.17.0\n",
              "sniffio                     1.3.1\n",
              "soupsieve                   2.6\n",
              "sqlalchemy                  2.0.37\n",
              "storemagic                  NA\n",
              "tenacity                    NA\n",
              "tiktoken                    0.8.0\n",
              "tiktoken_ext                NA\n",
              "tokenizers                  0.21.0\n",
              "tornado                     6.4.2\n",
              "tqdm                        4.67.1\n",
              "traitlets                   5.14.3\n",
              "typing_extensions           NA\n",
              "uri_template                NA\n",
              "urllib3                     2.3.0\n",
              "wcwidth                     0.2.13\n",
              "webcolors                   NA\n",
              "websocket                   1.8.0\n",
              "win32api                    NA\n",
              "win32com                    NA\n",
              "win32con                    NA\n",
              "win32trace                  NA\n",
              "winerror                    NA\n",
              "wrapt                       1.17.2\n",
              "yaml                        6.0.2\n",
              "yarl                        1.18.3\n",
              "zipp                        NA\n",
              "zmq                         24.0.1\n",
              "zoneinfo                    NA\n",
              "zstandard                   0.23.0\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      8.6.3\n",
              "jupyter_core        5.7.2\n",
              "jupyterlab          4.3.4\n",
              "notebook            7.3.2\n",
              "-----\n",
              "Python 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]\n",
              "Windows-10-10.0.26100-SP0\n",
              "-----\n",
              "Session information updated at 2025-01-23 17:03\n",
              "</pre>\n",
              "</details>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObVFSfMIVyLg",
        "outputId": "ddc6d33d-668f-404c-b3de-b6ec72c418a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')\n",
        "\n",
        "# Get the API key from the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was found\n",
        "if api_key:\n",
        "  print(\"API Key loaded successfully.\")\n",
        "else:\n",
        "  print(\"API Key not found in the environment variables.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKBFyIM_NN_-"
      },
      "source": [
        "# Chapter 3. Retrieval: How to Chat with Your Data with RAG\n",
        "\n",
        "ในบทก่อนหน้า คุณได้เรียนรู้วิธีประมวลผลข้อมูลของคุณ และสร้างและจัดเก็บการฝังข้อมูลใน Vector Store ในบทนี้ คุณจะได้เรียนรู้วิธีดึงข้อมูลการฝังข้อมูลและชิ้นส่วนเอกสารที่เกี่ยวข้องมากที่สุดอย่างมีประสิทธิภาพตามคำถามของผู้ใช้ ซึ่งช่วยให้คุณสร้างพรอมต์ที่มีเอกสารที่เกี่ยวข้องเป็นบริบท และด้วยเหตุนี้ จึงปรับปรุงความถูกต้องของผลลัพธ์สุดท้ายของ LLM\n",
        "\n",
        "กระบวนการนี้ ซึ่งเกี่ยวข้องกับการฝังข้อมูลคำถามของผู้ใช้ การดึงเอกสารที่คล้ายคลึงกันจากแหล่งข้อมูล และจากนั้นส่งผ่านเป็นบริบทไปยังพรอมต์ที่ส่งไปยัง LLM เรียกว่าการสร้างเสริมด้วยการดึงข้อมูล (retrieval-augmented generation - RAG)\n",
        "\n",
        "RAG เป็นองค์ประกอบสำคัญของการสร้างแอป LLM ที่เปิดใช้งานการแชท ซึ่งมีความถูกต้อง มีประสิทธิภาพ และทันสมัย ในบทนี้ คุณจะได้เรียนรู้กลยุทธ์ขั้นพื้นฐานถึงขั้นสูงในการสร้างระบบ RAG ที่มีประสิทธิภาพสำหรับแหล่งข้อมูลต่างๆ (เช่น Vector Store และฐานข้อมูล) และโครงสร้างข้อมูล (ที่มีโครงสร้างและไม่มีโครงสร้าง)\n",
        "\n",
        "แต่ก่อนอื่น มาให้เราจำกัดความ RAG และพูดคุยถึงประโยชน์ของมัน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC0OK9okNlIT"
      },
      "source": [
        "## Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "RAG เป็นเทคนิคที่ใช้เพื่อเพิ่มความถูกต้องของผลลัพธ์ที่สร้างโดย LLM โดยการจัดเตรียมบริบทจากแหล่งข้อมูลภายนอก คำศัพท์นี้ถูกตั้งขึ้นครั้งแรกใน เอกสารวิจัย ของนักวิจัยจาก Meta AI Lewis et al (2021) ซึ่งค้นพบว่าโมเดลที่เปิดใช้งาน RAG มีความจริงและเจาะจงมากกว่าโมเดลที่ไม่ใช่ RAG\n",
        "\n",
        "หากไม่มี RAG LLM จะพึ่งพาข้อมูลที่ได้รับการฝึกอบรมไว้ก่อนหน้านี้เท่านั้น ซึ่งอาจล้าสมัย ตัวอย่างเช่น ลองถาม ChatGPT ถึงคำถามเกี่ยวกับเหตุการณ์ปัจจุบันและดูคำตอบ\n",
        "\n",
        "อินพุต:\n",
        "ประเทศใดเป็นผู้ชนะล่าสุดของฟุตบอลโลกชาย\n",
        "\n",
        "เอาต์พุต:\n",
        "ผู้ชนะฟุตบอลโลกครั้งล่าสุดคือ ฝรั่งเศส ซึ่งคว้าแชมป์ในปี 2018\n",
        "\n",
        "คำตอบของ LLM ไม่ถูกต้องและล้าสมัย ผู้ชนะล่าสุด ณ วันนี้คือ อาร์เจนตินา ซึ่งคว้าแชมป์โลกในปี 2022 แม้ว่าคำถามตัวอย่างนี้อาจดูเหมือนเล็กน้อย แต่การหลอนภาพ (hallucination) ของ LLM อาจส่งผลร้ายแรง หากคำตอบของมันถูกนำไปใช้ในการตรวจสอบข้อเท็จจริงหรือการตัดสินใจที่สำคัญ\n",
        "\n",
        "เพื่อแก้ไขปัญหานี้ เราจำเป็นต้องจัดหาข้อมูลที่ถูกต้องและทันสมัยให้กับ LLM ซึ่ง LLM สามารถสร้างคำตอบที่ถูกต้องได้ โดยดำเนินการต่อจากตัวอย่างก่อนหน้า ลองไปที่หน้า ฟุตบอลโลก บน Wikipedia คัดลอกย่อหน้าแนะนำ และจากนั้นต่อท้ายเป็น บริบท ของพรอมต์ของเราไปยัง ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8D27_UaN4L4"
      },
      "source": [
        "**การดึงเอกสารที่เกี่ยวข้อง**\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-1.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "---\n",
        "ระบบ RAG สำหรับแอป AI โดยทั่วไปจะปฏิบัติตามสามขั้นตอนหลัก:\n",
        "\n",
        "1. การจัดทำดัชนี (Indexing)\n",
        "\n",
        "ขั้นตอนนี้เกี่ยวข้องกับการเตรียมข้อมูลแหล่งข้อมูลภายนอกและจัดเก็บการฝังข้อมูลที่แสดงถึงข้อมูลใน Vector Store ซึ่งสามารถดึงข้อมูลได้อย่างง่ายดาย\n",
        "2. การดึงข้อมูล (Retrieval)\n",
        "\n",
        "ขั้นตอนนี้เกี่ยวข้องกับการดึงข้อมูลการฝังข้อมูลและข้อมูลที่เกี่ยวข้องที่จัดเก็บไว้ใน Vector Store ตามคำถามของผู้ใช้\n",
        "3. การสร้าง (Generation)\n",
        "\n",
        "ขั้นตอนนี้เกี่ยวข้องกับการสังเคราะห์พรอมต์เดิมกับเอกสารที่เกี่ยวข้องที่ดึงข้อมูลมาเป็นพรอมต์สุดท้ายที่ส่งไปยังโมเดลสำหรับการทำนาย"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfYp0AhOQ63"
      },
      "source": [
        "ขั้นตอนการจัดทำดัชนีของกระบวนการนี้ได้รับการกล่าวถึงอย่างละเอียดในบทที่ 2 ซึ่งคุณได้เรียนรู้วิธีใช้ตัวโหลดเอกสาร ตัวแบ่งข้อความ การฝังข้อมูล และ Vector Store มาลองทำตัวอย่างตั้งแต่เริ่มต้นอีกครั้ง เริ่มจากขั้นตอนการจัดทำดัชนีก่อนใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gIm65qx2M8tI"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_postgres.vectorstores import PGVector\n",
        "# Load the document, split it into chunks\n",
        "raw_documents = TextLoader('./docs/experiments.txt').load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# embed each chunk and insert it into the vector store\n",
        "model = OpenAIEmbeddings()\n",
        "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
        "db = PGVector.from_documents(documents, model, connection=connection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FueMfAb1z5Lj"
      },
      "source": [
        "ขั้นตอนการจัดทำดัชนีเสร็จสมบูรณ์แล้ว เพื่อดำเนินการขั้นตอนการดึงข้อมูล เราจำเป็นต้องดำเนินการคำนวณความคล้ายคลึงกัน เช่น ค่าความคล้ายคลึงแบบโคไซน์ ระหว่างคำถามของผู้ใช้กับการฝังข้อมูลที่จัดเก็บไว้ของเรา ดังนั้น ชิ้นส่วนที่เกี่ยวข้องของเอกสารที่จัดทำดัชนีของเราจึงถูกดึงข้อมูล\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-2.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "---\n",
        "ขั้นตอนในการดึงข้อมูล\n",
        "\n",
        "1. แปลงคำถามของผู้ใช้เป็นการฝังข้อมูล\n",
        "2. คำนวณการฝังข้อมูลใน Vector Store ที่มีความคล้ายคลึงกันมากที่สุดกับคำถามของผู้ใช้\n",
        "3. ดึงข้อมูลการฝังข้อมูลของเอกสารที่เกี่ยวข้องและชิ้นส่วนข้อความที่สอดคล้องกัน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRihPEiS0mUj"
      },
      "source": [
        "1. แปลงคำถามของผู้ใช้เป็นการฝังข้อมูล"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA82kkvYz4rJ",
        "outputId": "7b991d96-b630-44c2-d586-b3cc27cd1901"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='7ce67a22-d70e-4451-83ab-8350eaa3da28', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'),\n",
              " Document(id='d6944fa6-4490-446d-89c0-a2ab30fcaeb2', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'),\n",
              " Document(id='de7c3fd9-ce60-46f7-9238-3828c6de89bb', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'),\n",
              " Document(id='58df5d77-d84a-483c-875a-f9c9a3be6b40', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.')]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create retriever\n",
        "retriever = db.as_retriever()\n",
        "# fetch relevant documents\n",
        "docs = retriever.invoke(\"Amazon\")\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkAJI_e-0jCu"
      },
      "source": [
        "2. คำนวณการฝังข้อมูลใน Vector Store ที่มีความคล้ายคลึงกันมากที่สุดกับคำถามของผู้ใช้\n",
        "3.ดึงข้อมูลการฝังข้อมูลของเอกสารที่เกี่ยวข้องและชิ้นส่วนข้อความที่สอดคล้องกัน\n",
        "\n",
        "นอกจากนี้ยังมีอาร์กิวเมนต์`k`ซึ่งกำหนดจำนวนเอกสารที่เกี่ยวข้องที่จะดึงข้อมูลจาก Vector Store ตัวอย่างเช่น เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdjUTqStj_MB",
        "outputId": "464f9885-7990-434b-87ff-4eab26cad529"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='de7c3fd9-ce60-46f7-9238-3828c6de89bb', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'),\n",
              " Document(id='d6944fa6-4490-446d-89c0-a2ab30fcaeb2', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.')]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create retriever with k=2\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
        "# fetch the 2 most relevant documents\n",
        "docs = retriever.invoke(\"Amazon\")\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8_Q_DWA05oJ"
      },
      "source": [
        "## Generating LLM Predictions Using Relevant Documents\n",
        "\n",
        "\n",
        "การสร้างการคาดการณ์ LLM โดยใช้เอกสารที่เกี่ยวข้อง\n",
        "\n",
        "เมื่อเราได้ดึงเอกสารที่เกี่ยวข้องตามคำถามของผู้ใช้แล้ว ขั้นตอนสุดท้ายคือการเพิ่มเอกสารเหล่านั้นลงในพรอมต์ดั้งเดิมเป็นบริบท จากนั้นเรียกใช้โมเดลเพื่อสร้างผลลัพธ์สุดท้าย\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-3.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA0eCUsF1W6O"
      },
      "source": [
        "นี่คือตัวอย่างโค้ดที่ดำเนินการต่อจากตัวอย่างก่อนหน้าของเรา เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C6jUYWt09Qm",
        "outputId": "00774db1-fb49-4c31-c97b-6ed567a5a413"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ปกป้องข้อมูลที่ละเอียดอ่อนของลูกค้าให้เหมือน Amazon โดยเหมาะสม ทำให้เกิดคำถามเกี่ยวกับความยั่งยืนในระยะยาวของโมเดลธุรกิจของบริษัทและความมุ่งมั่นของบริษัทในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 337, 'total_tokens': 503, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ccf59b2-593d-462c-aabf-12a6e87e8377-0', usage_metadata={'input_tokens': 337, 'output_tokens': 166, 'total_tokens': 503, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "chain = prompt | llm\n",
        "\n",
        "# fetch relevant documents\n",
        "docs = retriever.get_relevant_documents(\"What is Amazon do? <please only translate your response  to thai>\")\n",
        "\n",
        "# run\n",
        "chain.invoke({\"context\": docs,\"question\": \"What is Amazon do? <please only translate your response  to thai>\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhZ7w2G1l75"
      },
      "source": [
        "สังเกตการเปลี่ยนแปลงต่อไปนี้:\n",
        "\n",
        "- เราใช้ตัวแปร`context`และ`question`แบบไดนามิกในพรอมต์ของเรา ซึ่งช่วยให้เราสามารถกำหนด`ChatPromptTemplate`ที่โมเดลสามารถใช้เพื่อสร้างคำตอบได้\n",
        "- เราได้กำหนดอินเทอร์เฟซ`ChatOpenAI`ให้ทำหน้าที่เป็น LLM ของเรา อุณหภูมิ (Temperature) ถูกตั้งค่าเป็น 0 เพื่อกำจัดความสร้างสรรค์ในการส่งออกจากโมเดล\n",
        "- เราสร้างห่วงโซ่เพื่อประพันธ์พรอมต์และ LLM ข้อควรจำ: ตัวดำเนินการ`|`(หรือเมธอด`pipe`ใน JS) รับเอาต์พุตของ`prompt`และใช้เป็นอินพุตของ`llm`\n",
        "- เราเรียกใช้ `invoke` ห่วงโซ่โดยส่งผ่านตัวแปร`context`(เอกสารที่เกี่ยวข้องที่เราดึงข้อมูลมา)\n",
        "\n",
        "และคำถามของผู้ใช้เพื่อสร้างผลลัพธ์สุดท้าย\n",
        "เราสามารถห่อหุ้มขั้นตอนการเรียกใช้ทั้งสองขั้นตอนข้างต้นในฟังก์ชันเดียว เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8FiJvHa17jF",
        "outputId": "aea1c5ba-afc4-4706-f7b2-4f6eb8fda9b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ปกป้องข้อมูลที่ลูกค้ามีความละเอียดอ่อนของตนให้เหมือน Amazon โดยเหมาะสม ซึ่งเกิดคำถามเกี่ยวกับความยั่งยืนในระยะยาวของรูปแบบธุรกิจของบริษัทและความมุ่งมั่นของบริษัทในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 175, 'prompt_tokens': 337, 'total_tokens': 512, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f659c726-1836-4d9e-8884-2ab747bfc591-0', usage_metadata={'input_tokens': 337, 'output_tokens': 175, 'total_tokens': 512, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "@chain\n",
        "def qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retriever.get_relevant_documents(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "# run\n",
        "qa.invoke(\"What is Amazon do? <please only translate your response  to thai>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGI0gtlk2NS3"
      },
      "source": [
        "สังเกตว่าตอนนี้เรามี`qa`แบบเรียกใช้งานได้ใหม่ ซึ่งสามารถเรียกใช้ได้ด้วยเพียงแค่คำถามและดูแลการดึงเอกสารที่เกี่ยวข้องสำหรับบริบท จัดรูปแบบเป็นพรอมต์ และสุดท้ายสร้างคำตอบ แนวคิดในการห่อหุ้มหลายขั้นตอนในฟังก์ชันเดียวจะเป็นกุญแจสำคัญในการสร้างแอปที่น่าสนใจด้วย LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yXPaleZ2TJ8"
      },
      "source": [
        "คุณยังสามารถส่งคืนเอกสารที่ดึงข้อมูลมาเพื่อตรวจสอบเพิ่มเติม เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj7oHb4C2NdO"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retriever.get_relevant_documents(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return {\"answer\": answer, \"docs\": docs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj88aEjpNWzd",
        "outputId": "f763d22c-2dfa-45ec-c1e1-8c2a58cb3229"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='ปกป้องข้อมูลที่ละเอียดอ่อนของลูกค้าให้เหมือน Amazon โดยเหมาะสม ซึ่งเป็นเหตุให้เกิดคำถามเกี่ยวกับความยั่งยืนในระยะยาวของรูปแบบธุรกิจของบริษัทและความมุ่งมั่นของบริษัทในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 337, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-09ec0867-638c-40ee-a16f-62b0c798acee-0', usage_metadata={'input_tokens': 337, 'output_tokens': 178, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " 'docs': [Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")]}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.invoke(\"What is Amazon do? <please only translate your response  to thai>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWE2U7pdNwxG",
        "outputId": "7c913bf0-40c1-436f-c601-3c0126e97d04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='Amazon ทำงานเพื่อปกป้องข้อมูลที่ละเมิดของลูกค้าของพวกเขา โดยเกิดคำถามเกี่ยวกับความยั่งยืนในระยะยาวของโมเดลธุรกิจของบริษัทและการมุ่งมั่นในด้านความปลอดภัยของข้อมูลของพวกเขา', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 335, 'total_tokens': 491, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed3e52e7-294d-470f-bcfc-c04bc0c02aff-0', usage_metadata={'input_tokens': 335, 'output_tokens': 156, 'total_tokens': 491, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " 'docs': [Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")]}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@chain\n",
        "def qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retriever.get_relevant_documents(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return {\"answer\": answer, \"docs\": docs}\n",
        "qa.invoke(\"งาน amazon ทำอะไร\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfXYl7Ai2qyO"
      },
      "source": [
        "**ขอแสดงความยินดี! ตอนนี้คุณได้สร้างระบบ RAG พื้นฐานเพื่อขับเคลื่อนแอป AI สำหรับการใช้งานส่วนบุคคลแล้ว**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yrQIfBV2t1O"
      },
      "source": [
        "อย่างไรก็ตาม แอป AI ที่พร้อมใช้งานสำหรับการผลิตซึ่งใช้งานโดยผู้ใช้หลายคนจำเป็นต้องใช้ระบบ RAG ที่ก้าวหน้ายิ่งขึ้น เพื่อสร้างระบบ RAG ที่แข็งแกร่ง เราจำเป็นต้องตอบคำถามต่อไปนี้ได้อย่างมีประสิทธิภาพ:\n",
        "\n",
        "- เราจะจัดการกับความผันผวนในคุณภาพของอินพุตของผู้ใช้ได้อย่างไร\n",
        "- เราจะกำหนดเส้นทางการค้นหาข้อมูลที่เกี่ยวข้องจากแหล่งข้อมูลต่างๆ ได้อย่างไร\n",
        "- เราจะแปลงภาษาธรรมชาติเป็นภาษาคำถามของแหล่งข้อมูลเป้าหมายได้อย่างไร\n",
        "- เราจะปรับปรุงกระบวนการจัดทำดัชนีของเรา เช่น การฝังข้อมูล การแบ่งข้อความ ได้อย่างไร\n",
        "\n",
        "**ต่อไป เราจะหารือเกี่ยวกับกลยุทธ์ล่าสุดที่ได้รับการสนับสนุนจากการวิจัยเพื่อตอบคำถามเหล่านี้และสร้างระบบ RAG ที่พร้อมใช้งานสำหรับการผลิต**\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-4.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJtpfoKv3fcn"
      },
      "source": [
        "# Query Transformation\n",
        "\n",
        "หนึ่งในปัญหาสำคัญของระบบ RAG พื้นฐานคือ พึ่งพาคุณภาพของคำถามของผู้ใช้มากเกินไปในการสร้างผลลัพธ์ที่ถูกต้อง ในสภาพแวดล้อมการผลิต ผู้ใช้มีแนวโน้มที่จะสร้างคำถามของตนเองในลักษณะที่ไม่สมบูรณ์คลุมเครือหรือกำกวม ซึ่งนำไปสู่การหลอนภาพของโมเดล\n",
        "\n",
        "การแปลงคำถาม (Query transformation) เป็นส่วนย่อยของกลยุทธ์ที่ออกแบบมาเพื่อปรับเปลี่ยนอินพุตของผู้ใช้เพื่อตอบคำถาม RAG ข้อแรก: เราจะจัดการกับความผันผวนในคุณภาพของอินพุตของผู้ใช้ได้อย่างไร\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-5.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "รูปแสดงให้เห็นถึงขอบเขตของกลยุทธ์การแปลงคำถาม ตั้งแต่กลยุทธ์ที่ทำให้ข้อมูลอินพุตของผู้ใช้มีความเป็นนามธรรมมากขึ้นหรือน้อยลง เพื่อสร้างผลลัพธ์ LLM ที่ถูกต้อง ส่วนถัดไปเริ่มต้นด้วยกลยุทธ์ระดับกลาง\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjCkcQN3_rF"
      },
      "source": [
        "## Rewrite-Retrieve-Read\n",
        "\n",
        "กลยุทธ์ Rewrite-Retrieve-Read ที่เสนอโดยทีมวิจัยของ Microsoft เพียงแค่กระตุ้นให้ LLM เขียนคำถามของผู้ใช้ใหม่ก่อนที่จะดำเนินการดึงข้อมูล\n",
        "\n",
        "เพื่อแสดงให้เห็นถึงกลยุทธ์นี้ ขั้นแรก ให้เรากลับไปที่ห่วงโซ่ที่เราสร้างขึ้นในส่วนก่อนหน้า คราวนี้เรียกใช้ด้วยคำถามของผู้ใช้ที่เขียนไม่ดี:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YVnFBiD2y2-",
        "outputId": "4483c075-3d3b-4806-c309-4aa4db722cf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Based on the context provided, it seems like there is concern about cyberattacks and trust issues between consumers and companies like Amazon. It is not clear what specific actions Amazon is taking in response to these issues.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 358, 'total_tokens': 400, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c2490671-efe2-4249-a3c7-9db2ee03b5c6-0', usage_metadata={'input_tokens': 358, 'output_tokens': 42, 'total_tokens': 400, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@chain\n",
        "def qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retriever.get_relevant_documents(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "qa.invoke(\"เชี่ยยยยยชาย amazon  โคตตระเท่เลย ตกลงมันทำอะไรกันแน่ว่ะ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRccUp6X4Qgt"
      },
      "source": [
        "และนี่คือผลลัพธ์ (โปรดจำไว้ว่า หากคุณเรียกใช้ใหม่ ผลลัพธ์ของคุณอาจแตกต่างจากนี้):\n",
        "\n",
        "จากบริบทที่กำหนด ไม่มีข้อมูลที่ให้ไว้เกี่ยวกับ “Amazon”\n",
        "โมเดลไม่สามารถตอบคำถามได้เนื่องจากถูกข้อมูลที่ไม่เกี่ยวข้องที่ให้ไว้ในคำถามของผู้ใช้รบกวน\n",
        "\n",
        "ตอนนี้มาใช้พรอมต์ Rewrite-Retrieve-Read กัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSWx60uh4K4y",
        "outputId": "2249403c-0f5e-4192-962e-58ffad6f5573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "new_query : Amazon โคตรเท่เลย ทำงานอะไร\"\n",
            "formatted : messages=[HumanMessage(content=\"\\nAnswer the question based only on the following context:\\n[Document(id='7ce67a22-d70e-4451-83ab-8350eaa3da28', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'), Document(id='d6944fa6-4490-446d-89c0-a2ab30fcaeb2', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'), Document(id='de7c3fd9-ce60-46f7-9238-3828c6de89bb', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'), Document(id='58df5d77-d84a-483c-875a-f9c9a3be6b40', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.')]\\nQuestion: เชี่ยยยยย amazon โคตตระเท่เลย ตกลงมันทำงานอะไรกันแน่วะ\\n\", additional_kwargs={}, response_metadata={})]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Based on the context provided, it seems like the question is expressing surprise or admiration towards Amazon and asking what exactly they do.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 356, 'total_tokens': 382, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-baf54e58-99c4-4990-a60b-f2c1102cc835-0', usage_metadata={'input_tokens': 356, 'output_tokens': 26, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Provide a better search query for web search engine to answer the given question,\n",
        "ํYou are only anwswer in Thai langauge\n",
        "end the queries with \"**\".\n",
        "Question: {x}\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "def parse_rewriter_output(message):\n",
        "    return message.content.strip('\"').strip(\"**\")\n",
        "\n",
        "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
        "\n",
        "@chain\n",
        "def qa_rrr(input):\n",
        "  # rewrite the query\n",
        "  new_query = rewriter.invoke(input)\n",
        "  print(f\"new_query : {new_query}\")\n",
        "  # fetch relevant documents\n",
        "  docs = retriever.get_relevant_documents(new_query)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  print(f\"formatted : {formatted}\")\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "# run\n",
        "qa_rrr.invoke(\"เชี่ยยยยย amazon โคตตระเท่เลย ตกลงมันทำงานอะไรกันแน่วะ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKpcZnZW461B"
      },
      "source": [
        "สังเกตว่าตอนนี้เรามี LLM เขียนคำถามเริ่มต้นของผู้ใช้ใหม่ให้ชัดเจนยิ่งขึ้น และเป็นคำถามที่เน้นมากขึ้นนั้นที่ถูกส่งผ่านไปยังตัวดึงข้อมูลเพื่อดึงเอกสารที่เกี่ยวข้องมากที่สุด โปรดทราบว่าเทคนิคนี้สามารถใช้กับวิธีการดึงข้อมูลใดๆ ก็ได้ ไม่ว่าจะเป็น Vector Store เช่นเดียวกับที่เรามีอยู่ หรือตัวอย่างเช่น เครื่องมือค้นหาเว็บ ข้อเสียของวิธีนี้คือ จะทำให้เกิดความล่าช้าเพิ่มเติมในห่วงโซ่ของคุณ เนื่องจากตอนนี้เราจำเป็นต้องดำเนินการเรียกใช้ LLM สองครั้งตามลำดับ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDHP_BKz5DE2"
      },
      "source": [
        "## Multi Query Retrieval\n",
        "\n",
        "**การดึงข้อมูลหลายคำถาม**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-6.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "คำถามเดียวของผู้ใช้อาจไม่เพียงพอที่จะจับภาพขอบเขตข้อมูลทั้งหมดที่จำเป็นในการตอบคำถามอย่างครอบคลุม กลยุทธ์การดึงข้อมูลหลายคำถามแก้ไขปัญหานี้โดยสั่งให้ LLM สร้างคำถามหลายคำถามโดยยึดตามคำถามเริ่มต้นของผู้ใช้ ดำเนินการดึงข้อมูลแบบขนานของแต่ละคำถามจากแหล่งข้อมูล และจากนั้นแทรกผลลัพธ์ที่ดึงข้อมูลมาเป็นบริบทพรอมต์เพื่อสร้างผลลัพธ์สุดท้ายของโมเดล รูปที่ 3-6 แสดงให้เห็น"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ31vGFY5WwU"
      },
      "source": [
        "**กลยุทธ์นี้มีประโยชน์อย่างยิ่งสำหรับกรณีการใช้งานที่คำถามเดียวอาจต้องพึ่งพามุมมองหลายมุมมองเพื่อให้คำตอบที่ครอบคลุม**\n",
        "\n",
        "นี่คือตัวอย่างโค้ดของการดึงข้อมูลหลายคำถามในการดำเนินการ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBQAh5Oc47OQ"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an AI language model assistant.\n",
        "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.\n",
        "By generating multiple perspectives on the user question,\n",
        "your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
        "ํYou are only anwswer in Thai langauge.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\"\"\")\n",
        "\n",
        "def parse_queries_output(message):\n",
        "    return message.content.split('\\n')\n",
        "\n",
        "query_gen = perspectives_prompt | llm | parse_queries_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmo1lUWpUaqo",
        "outputId": "e6637ef8-59ea-463f-d110-0dd7fdfa6d04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Amazon ทำงานอะไร?',\n",
              " 'Amazon มีเป้าหมายอะไรอยู่?',\n",
              " 'Amazon มีปัญหาอะไรเกิดขึ้นบ้าง?',\n",
              " 'Amazon มีบทบาทในการทำงานอะไร?',\n",
              " 'Amazon มีผลกระทบต่อสังคมอย่างไร?']"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_gen.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxDGfcLJ5ll-"
      },
      "source": [
        "สังเกตว่าเทมเพลตพรอมต์ได้รับการออกแบบมาเพื่อสร้างรูปแบบต่างๆ ของคำถามโดยยึดตามคำถามเริ่มต้นของผู้ใช้\n",
        "\n",
        "จากนั้น เราจะนำรายการคำถามที่สร้างขึ้นไปดึงเอกสารที่เกี่ยวข้องมากที่สุดสำหรับแต่ละรายการแบบขนาน และจากนั้นรวมกันเพื่อรับยูเนียนที่ไม่ซ้ำกันของเอกสารที่เกี่ยวข้องทั้งหมด เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_18PM-Q5lx7"
      },
      "outputs": [],
      "source": [
        "def get_unique_union(document_lists):\n",
        "  # Flatten list of lists, and dedupe them\n",
        "  deduped_docs = {\n",
        "      doc.page_content: doc for sublist in document_lists for doc in sublist\n",
        "    }\n",
        "  # return a flat list of unique docs\n",
        "  return list(deduped_docs.values())\n",
        "\n",
        "retrieval_chain = query_gen | retriever.batch #| get_unique_union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlrkyXYAciWj",
        "outputId": "0afcd5de-c6e7-4b5f-d3fa-2fbbcf2e7dcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")],\n",
              " [Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")],\n",
              " [Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")],\n",
              " [Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")],\n",
              " [Document(id='7f31d708-6c33-4a44-9370-2912a7b4ae22', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='f29e815f-8c20-4ece-be92-090b0c6cea8d', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='4a0b8fba-2ea3-4a84-b4a2-e1265dca5566', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              "  Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\")]]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_chain.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7YzHEY4UioX"
      },
      "outputs": [],
      "source": [
        "## Extra example of get_unique_union function\n",
        "# xs = retrieval_chain.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")\n",
        "# for x in xs:\n",
        "#   for s in x:\n",
        "#     print(list(s.page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPHQwLp1ahsF",
        "outputId": "e69374d7-5741-4619-a848-f1613c572ed8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2b4ab845-6a9f-43c5-8a04-1a9d886f10ae', metadata={'source': './docs/experiments.txt'}, page_content=\"like Amazon to adequately protect the sensitive information of their customers, raising questions about the long-term viability of the company's business model and its commitment to data security,\"),\n",
              " Document(id='1', metadata={'topic': 'animals', 'location': 'pond'}, page_content='there are cats in the pond'),\n",
              " Document(id='7ce67a22-d70e-4451-83ab-8350eaa3da28', metadata={'source': './docs/experiments.txt'}, page_content='cyberattacks looms large, and the trust between consumers and the companies they do business with, like Amazon, hangs precariously in the balance.'),\n",
              " Document(id='dc35e004-495f-58d4-8c5f-32d73a512c66', metadata={'source': 'source2'}, page_content='Content of doc2'),\n",
              " Document(id='d8223846-5361-458c-bb7f-2206a6de476b', metadata={'source': './docs/test.txt'}, page_content='This is test file'),\n",
              " Document(id='57959c1c-4efc-4300-8c8c-8dff2cb01f18', metadata={'source': './docs/experiments.txt'}, page_content='In a startling turn of events that has sent shockwaves through the global scientific community, a team of renegade researchers, operating out of a clandestine laboratory hidden deep within the Amazon'),\n",
              " Document(id='c4c911f5-0154-4b7e-97cc-eb45440f5e63', metadata={'source': './docs/experiments.txt'}, page_content='and fantasy, are constantly blurring, a world where the unexpected can and will occur, a world that is, in a word, truly extraordinary, a world that continues to defy our comprehension and astound us')]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_chain = query_gen | retriever.batch | get_unique_union\n",
        "retrieval_chain.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aydGh1tE5v23"
      },
      "source": [
        "เนื่องจากเรากำลังดึงข้อมูลเอกสารจากตัวดึงข้อมูลเดียวกันโดยใช้หลายคำถาม (ที่เกี่ยวข้อง) จึงมีความเป็นไปได้ว่าเอกสารบางส่วนจะซ้ำกัน ก่อนที่จะใช้เอกสารเหล่านั้นเป็นบริบทเพื่อตอบคำถาม\n",
        "- เราจำเป็นต้องลบข้อมูลซ้ำซ้อน เพื่อให้ได้อินสแตนซ์เดียวของแต่ละรายการ\n",
        "- ที่นี่เราลบข้อมูลซ้ำซ้อนของเอกสารโดยใช้เนื้อหาของเอกสาร (สตริง) เป็นคีย์ในพจนานุกรม เนื่องจากพจนานุกรมสามารถมีรายการเดียวสำหรับแต่ละคีย์ได้เท่านั้น\n",
        "- หลังจากที่เราได้วนลูปผ่านเอกสารทั้งหมดแล้ว เราเพียงแค่รับค่าทั้งหมดของพจนานุกรม ซึ่งตอนนี้ปราศจากข้อมูลซ้ำซ้อนแล้ว\n",
        "\n",
        "สังเกตการใช้`.batch`ของเราด้วย ซึ่งเรียกใช้คำถามที่สร้างขึ้นทั้งหมดแบบขนานและส่งคืนรายการของผลลัพธ์—ในกรณีนี้ รายการของรายการเอกสาร ซึ่งจากนั้นเราก็ทำให้แบนราบและลบข้อมูลซ้ำซ้อนตามที่อธิบายไว้ข้างต้น\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqE2S-F6c8c"
      },
      "source": [
        "ขั้นตอนสุดท้ายคือการสร้างพรอมต์ที่รวมคำถามของผู้ใช้และเอกสารที่เกี่ยวข้องที่ดึงข้อมูลมา และอินเทอร์เฟซของโมเดลเพื่อสร้างการคาดการณ์ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXkCkwVH5wWv",
        "outputId": "9062234d-a306-4565-accd-0ff5f2f995dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ตามเนื้อหาที่ให้มา Amazon ทำงานเกี่ยวกับการปกป้องข้อมูลที่ละเอียดอ่อนของลูกค้าของพวกเขา มีเป้าหมายในการปกป้องข้อมูลลูกค้าอย่างเหมาะสม และมีปัญหาเกี่ยวกับความยั่งยืนในระยะยาวของโมเดลธุรกิจของบริษัทและการมุ่งมั่นในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 207, 'prompt_tokens': 506, 'total_tokens': 713, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f39a9381-876c-40e5-b62e-068587c5da92-0', usage_metadata={'input_tokens': 506, 'output_tokens': 207, 'total_tokens': 713, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "@chain\n",
        "def multi_query_qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retrieval_chain.invoke(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "# run\n",
        "multi_query_qa.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtuhj2Uz600L"
      },
      "source": [
        "สังเกตว่านี่ไม่แตกต่างจากห่วงโซ่ QA ก่อนหน้ามากนัก เนื่องจากตรรกะใหม่ทั้งหมดสำหรับการดึงข้อมูลหลายคำถามนั้นมีอยู่ใน`retrieval_chain` นี่คือกุญแจสำคัญในการใช้เทคนิคเหล่านี้ให้เกิดประโยชน์สูงสุด โดยการนำเทคนิคแต่ละอย่างไปใช้เป็นห่วงโซ่แบบสแตนด์อโลน (ในกรณีนี้`retrieval_chain`) ซึ่งทำให้สามารถนำไปใช้และแม้แต่รวมกันได้ง่าย\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd48TQxf7Ghz"
      },
      "source": [
        "## RAG-Fusion\n",
        "\n",
        "กลยุทธ์`RAG-Fusion`มีความคล้ายคลึงกับกลยุทธ์การดึงข้อมูลหลายคำถาม ยกเว้นว่าเราใช้ขั้นตอนการจัดอันดับใหม่ขั้นสุดท้ายกับเอกสารที่ดึงข้อมูลมาทั้งหมด ขั้นตอนการจัดอันดับใหม่นี้ใช้ขั้นตอนวิธี Reciprocal Rank Fusion (RRF) ซึ่งเกี่ยวข้องกับการรวมอันดับของผลการค้นหาที่แตกต่างกันเพื่อสร้างอันดับเดียวที่รวมกัน โดยการรวมอันดับจากคำถามที่แตกต่างกัน เราดึงเอกสารที่เกี่ยวข้องมากที่สุดไปยังด้านบนสุดของรายการสุดท้าย\n",
        "\n",
        "RRF เหมาะอย่างยิ่งสำหรับการรวมผลลัพธ์จากคำถามที่มีสเกลหรือการกระจายของคะแนนที่อาจแตกต่างกัน\n",
        "\n",
        "มาสาธิต RAG-Fusion ในโค้ดกัน ขั้นแรก เราสร้างพรอมต์ที่คล้ายกับกลยุทธ์การค้นหาแบบหลายคำถามเพื่อสร้างรายการคำถามโดยยึดตามคำถามของผู้ใช้ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCJ6mxMT60-9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\")\n",
        "\n",
        "def parse_queries_output(message):\n",
        "    return message.content.split('\\n')\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "generate_queries = prompt_rag_fusion | llm  #| parse_queries_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTtb12U2bNjV",
        "outputId": "aebe0324-6d15-404c-bb75-3d2be8b2d62e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='1. อะไรคือหน้าที่ของพนักงาน Amazon?\\n2. Amazon มีเป้าหมายใดที่ต้องการบรรลุ?\\n3. ปัญหาที่ Amazon พบบ่อยที่สุดคืออะไร?\\n4. ทำไม Amazon มีปัญหาในการดำเนินงานบ้าง?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 95, 'total_tokens': 211, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-16164a22-e90d-4b6f-bee4-34c9e18bf144-0', usage_metadata={'input_tokens': 95, 'output_tokens': 116, 'total_tokens': 211, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_queries.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLEXPpX1bfXa",
        "outputId": "3315a068-a02c-433a-d021-d2a4e06cd1ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1. อะไรคือหน้าที่ของพนักงาน Amazon?',\n",
              " '2. Amazon มีเป้าหมายในการทำงานอะไรบ้าง?',\n",
              " '3. ปัญหาที่พนักงาน Amazon อาจพบบ่อยคือ?',\n",
              " '4. Amazon มีปัญหาใดที่อาจเกิดขึ้นในการทำงาน?']"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_queries = prompt_rag_fusion | llm | parse_queries_output\n",
        "generate_queries.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VAAd4hN7gy1"
      },
      "source": [
        "- เมื่อเราสร้างคำถามของเราแล้ว เราจะดึงเอกสารที่เกี่ยวข้องสำหรับแต่ละคำถามและส่งผ่านไปยังฟังก์ชันเพื่อ`จัดอันดับใหม่`(นั่นคือ`จัดลำดับใหม่`ตามความเกี่ยวข้อง) รายการสุดท้ายของเอกสารที่เกี่ยวข้อง\n",
        "\n",
        "- ฟังก์ชัน `reciprocal_rank_fusion` รับรายการผลการค้นหาของแต่ละคำถาม ดังนั้น รายการของรายการเอกสาร โดยที่รายการภายในแต่ละรายการของเอกสารจะถูกจัดเรียงตามความเกี่ยวข้องกับคำถามนั้น อัลกอริทึม RRF จากนั้นจะคำนวณคะแนนใหม่สำหรับแต่ละเอกสารโดยยึดตามอันดับ (หรือตำแหน่ง) ในรายการต่างๆ และจัดเรียงเพื่อสร้างรายการที่จัดอันดับใหม่สุดท้าย\n",
        "- หลังจากคำนวณคะแนนที่หลอมรวมแล้ว ฟังก์ชันจะจัดเรียงเอกสารตามลำดับคะแนนที่ลดลงเพื่อรับรายการที่จัดอันดับใหม่สุดท้าย ซึ่งจะถูกส่งคืน\n",
        "\n",
        "มาดูกัน เริ่มต้นใน Python:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FggLoMQl7mLw"
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents\n",
        "       and an optional parameter k used in the RRF formula\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each document\n",
        "    # Documents will be keyed by their contents to ensure uniqueness\n",
        "    fused_scores = {}\n",
        "    documents = {}\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Use the document contents as the key for uniqueness\n",
        "            doc_str = doc.page_content\n",
        "            # If the document hasn't been seen yet,\n",
        "            # - initialize score to 0\n",
        "            # - save it for later\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "                documents[doc_str] = doc\n",
        "            # Update the score of the document using the RRF formula:\n",
        "            # 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_doc_strs = sorted(\n",
        "        fused_scores, key=lambda d: fused_scores[d], reverse=True\n",
        "    )\n",
        "    # retrieve the corresponding doc for each doc_str\n",
        "    return [\n",
        "        documents[doc_str]\n",
        "        for doc_str in reranked_doc_strs\n",
        "    ]\n",
        "\n",
        "retrieval_chain = generate_queries | retriever.batch | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMI8IgY28LOg"
      },
      "source": [
        "สังเกตว่าฟังก์ชันยังใช้พารามิเตอร์`k`ด้วย ซึ่งกำหนดว่าเอกสารในชุดผลลัพธ์ของแต่ละคำถามมีอิทธิพลต่อรายการเอกสารสุดท้ายมากน้อยเพียงใด ค่าที่สูงขึ้นบ่งชี้ว่าเอกสารที่มีอันดับต่ำกว่ามีอิทธิพลมากขึ้น\n",
        "\n",
        "สุดท้าย เราจะรวมห่วงโซ่การดึงข้อมูลใหม่ของเรา (ตอนนี้ใช้ reciprocal rank fusion) เข้ากับห่วงโซ่เต็มรูปแบบที่เราเคยเห็นมาก่อน เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLcPxpTe8LcO",
        "outputId": "2baa4f70-a8b8-4499-e2cb-fa472a952e88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Amazon ทำงานในการปกป้องข้อมูลที่ละเอียดของลูกค้าของพวกเขา มีเป้าหมายในการสร้างรายได้ในระยะยาวและมีการมุ่งมั่นในด้านความปลอดภัยของข้อมูล อย่างไรก็ตาม มีปัญหาเกิดขึ้นเกี่ยวกับความยั่งยืนในระยะยาวของรูปแบบธุรกิจของบริษัทและการมุ่งมั่นในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 150, 'total_tokens': 381, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1255d163-b21c-494b-80f4-96ea092b1881-0', usage_metadata={'input_tokens': 150, 'output_tokens': 231, 'total_tokens': 381, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "@chain\n",
        "def multi_query_qa(input):\n",
        "  # fetch relevant documents\n",
        "  docs = retrieval_chain.invoke(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "multi_query_qa.invoke(\"amazon ทำงานอะไร?, มีเป้าหมายอะไรอยู่?, มีปัญหาอะไรเกิดขึ้นบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij-UHobq8Z4t"
      },
      "source": [
        "**จุดแข็งของ RAG-Fusion อยู่ที่ความสามารถในการจับภาพการแสดงออกที่ตั้งใจของผู้ใช้ นำทางคำถามที่ซับซ้อน และขยายขอบเขตของเอกสารที่ดึงข้อมูลมา ซึ่งช่วยให้เกิดการค้นพบโดยบังเอิญ**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSz6FPN59Mrz"
      },
      "source": [
        "## Hypothetical Document Embeddings - HyDE\n",
        "\n",
        "**การฝังข้อมูลเอกสารสมมติ**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-7.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "การฝังข้อมูลเอกสารสมมติ (HyDE) เป็นกลยุทธ์ที่เกี่ยวข้องกับการสร้างเอกสารสมมติตามคำถามของผู้ใช้ ฝังข้อมูลเอกสาร และดึงเอกสารที่เกี่ยวข้องโดยยึดตามความคล้ายคลึงกันของเวกเตอร์ สัญชาตญาณเบื้องหลัง HyDE คือ เอกสารสมมติที่สร้างโดย LLM จะมีความคล้ายคลึงกับเอกสารที่เกี่ยวข้องมากที่สุดมากกว่าคำถามเดิม\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7QB7W09f_i"
      },
      "source": [
        "อันดับแรก กำหนดพรอมต์เพื่อสร้างเอกสารสมมติ:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm_9i-gY8aGi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"\n",
        "Please write a scientific paper passage to answer the question\n",
        "Answer only Thai language\n",
        "Question: {question}\n",
        "Passage:\"\"\")\n",
        "\n",
        "generate_doc = (\n",
        "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JVfyro2eE0W",
        "outputId": "33f68292-3dda-4a05-a5fc-75e5d53fd1b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Amazon เป็นบริษัทอีคอมเมิร์ซที่มีธุรกิจหลากหลาย ซึ่งรวมถึงการขายสินค้าออนไลน์, บริการคลาวด์, การพัฒนาเทคโนโลยีด้านการเรียนรู้เชิงลึก (deep learning), การวิจัยและพัฒนาด้านการประมวลผลข้อมูล (data processing), และการพัฒนาหุ่นยนต์ นอกจากนี้ Amazon ยังมีบริการสตรีมมิ่งวิดีโอ (video streaming) และบริการด้านการเล่นเกม (gaming) ซึ่งทำให้ Amazon เป็นหนึ่งในบริษัทที่มีผลกระทบในวงการเทคโนโลยีและการค้าขายออนไลน์อย่างมากในปัจจุบัน ดังนั้น Amazon ทำงานอะไรก็ได้ตามที่กล่าวมาข้างต้น โดยมีการพัฒนาและให้บริการหลากหลายด้านให้กับลูกค้าและผู้ใช้บริการของตน'"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_doc.invoke(\"amazon ทำงานอะไร?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qWyC9m89pmr"
      },
      "source": [
        "ถัดไป เราใช้เอกสารสมมติที่สร้างขึ้นข้างต้นเป็นอินพุตไปยัง retriever ซึ่งจะสร้างการฝังข้อมูลและค้นหาเอกสารที่คล้ายคลึงกันใน Vector Store เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow9QIENo9pvS"
      },
      "outputs": [],
      "source": [
        "retrieval_chain = generate_doc | retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aethte2W9xTf"
      },
      "source": [
        "สุดท้าย เราใช้เอกสารที่ดึงข้อมูลมา ส่งผ่านเป็นบริบทไปยังพรอมต์สุดท้าย และสั่งให้โมเดลสร้างผลลัพธ์ เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9nxJp3y9xn0",
        "outputId": "e7c20c54-20ac-4258-dba0-40fdc9621609"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Amazon ทำงานในการปกป้องข้อมูลที่ละเมิดของลูกค้าของพวกเขา เพิ่มคำถามเกี่ยวกับความยั่งยืนในระยะยาวของโมเดลธุรกิจของบริษัทและความมุ่งมั่นของบริษัทในด้านความปลอดภัยของข้อมูล', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 338, 'total_tokens': 493, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ee80de32-1b22-411c-859a-9aa2e189e9b3-0', usage_metadata={'input_tokens': 338, 'output_tokens': 155, 'total_tokens': 493, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer only Thai language\n",
        "Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "@chain\n",
        "def qa(input):\n",
        "  # fetch relevant documents from the hyde retrieval chain defined earlier\n",
        "  docs = retrieval_chain.invoke(input)\n",
        "  # format prompt\n",
        "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
        "  # generate answer\n",
        "  answer = llm.invoke(formatted)\n",
        "  return answer\n",
        "\n",
        "qa.invoke(\"amazon ทำงานอะไร?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPEkn94B99yy"
      },
      "source": [
        "## Summary\n",
        "\n",
        "เพื่อสรุปสิ่งที่เราครอบคลุมในส่วนนี้ การแปลงคำถามประกอบด้วยการนำคำถามเดิมของผู้ใช้มา และดำเนินการดังต่อไปนี้:\n",
        "\n",
        "- เขียนใหม่เป็นหนึ่งหรือหลายคำถาม\n",
        "-  รวมผลลัพธ์ของคำถามเหล่านั้นเป็นชุดเดียวของผลลัพธ์ที่เกี่ยวข้องมากที่สุด\n",
        "\n",
        "การเขียนคำถามใหม่สามารถมีหลายรูปแบบ แต่โดยปกติแล้วจะทำในลักษณะที่คล้ายคลึงกัน: คุณใช้คำถามเดิมของผู้ใช้ พรอมต์ที่คุณเขียน และขอให้ LLM เขียนคำถามใหม่หรือหลายคำถาม ตัวอย่างบางส่วน:\n",
        "\n",
        "- ลบข้อความที่ไม่เกี่ยวข้อง/ไม่สัมพันธ์กันออกจากคำถาม\n",
        "- ยึดคำถามกับประวัติการสนทนาในอดีต ตัวอย่างเช่น เพื่อให้เข้าใจคำถาม เช่น`and what about in LA`เราจำเป็นต้องรวมกับคำถามสมมติในอดีตเกี่ยวกับสภาพอากาศใน SF เพื่อให้ได้คำถามที่มีประโยชน์ เช่น`weather in LA`\n",
        "- การขยายขอบเขตการค้นหาเอกสารที่เกี่ยวข้องโดยการดึงเอกสารสำหรับคำถามที่เกี่ยวข้องด้วย\n",
        "- แยกย่อยคำถามที่ซับซ้อนออกเป็นหลายคำถามที่ง่ายกว่า จากนั้นรวมผลลัพธ์สำหรับคำถามทั้งหมดในพรอมต์สุดท้ายเพื่อสร้างคำตอบ\n",
        "\n",
        "กลยุทธ์การเขียนใหม่ที่เหมาะสมที่จะใช้จะขึ้นอยู่กับกรณีการใช้งานของคุณ\n",
        "\n",
        "**ตอนนี้เราได้กล่าวถึงกลยุทธ์การแปลงคำถามหลักแล้ว มาพูดคุยกันถึงคำถามสำคัญข้อที่สองที่จะตอบเพื่อสร้างระบบ RAG ที่แข็งแกร่ง: เราจะกำหนดเส้นทางการค้นหาข้อมูลที่เกี่ยวข้องจากแหล่งข้อมูลต่างๆ ได้อย่างไร**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIoF22xVewM0"
      },
      "source": [
        "# Query Routing\n",
        "**การกำหนดเส้นทางคำถาม**\n",
        "\n",
        "\n",
        "แม้ว่าการใช้ Vector Store เดียวจะมีประโยชน์ แต่ข้อมูลที่จำเป็นอาจอาศัยอยู่ในแหล่งข้อมูลต่างๆ รวมถึงฐานข้อมูลเชิงสัมพันธ์หรือ Vector Store อื่นๆ\n",
        "\n",
        "ตัวอย่างเช่น คุณอาจมี Vector Store สองแห่ง: หนึ่งสำหรับเอกสาร LangChain Python และอีกหนึ่งสำหรับเอกสาร LangChain JS เมื่อได้รับคำถามของผู้ใช้ เราต้องการ กำหนดเส้นทาง คำถามไปยังแหล่งข้อมูลที่อนุมานได้ที่เกี่ยวข้องเพื่อดึงข้อมูลเอกสารที่เกี่ยวข้อง การกำหนดเส้นทางคำถาม เป็นกลยุทธ์ที่ใช้ในการส่งต่อคำถามของผู้ใช้ไปยังแหล่งข้อมูลที่เกี่ยวข้อง\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocamFeOyfIs7"
      },
      "source": [
        "## Logical Routing\n",
        "**การกำหนดเส้นทางเชิงตรรกะ**\n",
        "\n",
        "ใน การกำหนดเส้นทางเชิงตรรกะ เราให้ LLM มีความรู้เกี่ยวกับแหล่งข้อมูลต่างๆ ที่เรามีอยู่ จากนั้นปล่อยให้ LLM หาเหตุผลว่าควรใช้แหล่งข้อมูลใดโดยยึดตามคำถามของผู้ใช้ในรูปแสดงให้เห็น\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-8.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "เพื่อให้บรรลุเป้าหมายนี้ เราใช้โมเดลการเรียกใช้ฟังก์ชัน เช่น GPT-4 เพื่อช่วยจำแนกแต่ละคำถามออกเป็นหนึ่งในเส้นทางที่มีอยู่ **การเรียกใช้ฟังก์ชัน (function call)** เกี่ยวข้องกับการกำหนดรูปแบบแผนผังที่โมเดลสามารถใช้เพื่อสร้างอาร์กิวเมนต์ของฟังก์ชันโดยยึดตามคำถาม ซึ่งช่วยให้เราสามารถสร้างผลลัพธ์ที่มีโครงสร้างซึ่งสามารถใช้เพื่อเรียกใช้ฟังก์ชันอื่นๆ ได้ โค้ด Python ต่อไปนี้กำหนดรูปแบบแผนผังสำหรับตัวกำหนดเส้นทางของเราโดยยึดตามเอกสารสามฉบับสำหรับภาษาต่างๆ:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TGK3FP09-Gx"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "# กำหนดโมเดลข้อมูลโดยใช้คลาส BaseModel จาก Pydantic\n",
        "# ==============================================================================\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "# ==============================================================================\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "# ==============================================================================\n",
        "system = \"\"\"\n",
        "You are an expert at routing a user question to the appropriate data source.\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "# Define router\n",
        "# ==============================================================================\n",
        "router = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWYcCrXjgBfN"
      },
      "source": [
        "ตอนนี้เราเรียกใช้ LLM เพื่อดึงข้อมูลแหล่งข้อมูลโดยยึดตามรูปแบบแผนผังที่กำหนดไว้ เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB2U7l3Nf456",
        "outputId": "5b938272-edea-468e-aeee-c4c9c4b325c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'python_docs'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"\n",
        "Why doesn't the following code work:\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})\n",
        "result.datasource\n",
        "# \"python_docs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl6MNoLJgS1w"
      },
      "source": [
        "สังเกตว่า LLM ได้สร้างผลลัพธ์ JSON ซึ่งสอดคล้องกับรูปแบบแผนผังที่เราได้กำหนดไว้ก่อนหน้านี้ ซึ่งจะเป็นประโยชน์ในงานอื่นๆ อีกมากมาย\n",
        "\n",
        "เมื่อเราได้ดึงข้อมูลแหล่งข้อมูลที่เกี่ยวข้องแล้ว เราสามารถส่งผ่านค่าไปยังฟังก์ชันอื่นเพื่อดำเนินการตรรกะเพิ่มเติมตามที่จำเป็น เริ่มต้นใน Python:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEzAx1wrgKI_",
        "outputId": "45837090-aa75-461c-96f4-ce81c258d99e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'chain for python_docs'"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableLambda  # Import RunnableLambda\n",
        "\n",
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "full_chain = router | RunnableLambda(choose_route)\n",
        "\n",
        "full_chain.invoke({\"question\": question})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLbjinCzgynw"
      },
      "source": [
        "สังเกตว่าเราไม่ได้ทำการเปรียบเทียบสตริงที่ตรงกัน แต่กลับแปลงเอาต์พุตที่สร้างขึ้นเป็นตัวพิมพ์เล็กก่อน จากนั้นจึงทำการจับคู่สตริงย่อย ซึ่งทำให้ห่วงโซ่ของเรามีความยืดหยุ่นมากขึ้นต่อการที่ LLM ออกนอกสคริปต์ และสร้างเอาต์พุตที่ไม่ตรงกับรูปแบบแผนผังที่เราขอ\n",
        "\n",
        "**หมายเหตุ:**\n",
        "นี่เป็นอีกธีมหนึ่งที่ควรคำนึงถึงเมื่อสร้างแอปพลิเคชัน LLM ของคุณ: ทำให้แอปพลิเคชันมีความยืดหยุ่นต่อลักษณะแบบสุ่มของเอาต์พุต LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d-hNVWYg0eg"
      },
      "source": [
        "การกำหนดเส้นทางเชิงตรรกะเหมาะสมที่สุดเมื่อคุณมีรายการแหล่งข้อมูลที่กำหนดไว้ซึ่งสามารถดึงข้อมูลที่เกี่ยวข้องและใช้โดย LLM เพื่อสร้างผลลัพธ์ที่ถูกต้อง แหล่งข้อมูลเหล่านี้สามารถอยู่ในช่วงตั้งแต่ Vector Store ไปจนถึงฐานข้อมูลและแม้แต่ API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzfy0d3rhL_2"
      },
      "source": [
        "## Semantic Routing\n",
        "\n",
        "**การกำหนดเส้นทางเชิงความหมาย**\n",
        "\n",
        "ต่างจากการกำหนดเส้นทางเชิงตรรกะ `การกำหนดเส้นทางเชิงความหมาย` เกี่ยวข้องกับการฝังข้อมูลพรอมต์ต่างๆ ที่แสดงถึงแหล่งข้อมูลต่างๆ ควบคู่ไปกับคำถามของผู้ใช้ และจากนั้นดำเนินการค้นหาความคล้ายคลึงกันของเวกเตอร์เพื่อดึงข้อมูลพรอมต์ที่คล้ายคลึงกันมากที่สุด รูปนี้ แสดงให้เห็น\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-9.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fppxWWc-7vO"
      },
      "source": [
        "ต่อไปนี้คือตัวอย่างของการกำหนดเส้นทางเชิงความหมาย:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZa1XiDPgvoZ"
      },
      "outputs": [],
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Two prompts\n",
        "# ==============================================================================\n",
        "physics_template = \"\"\"\n",
        "You are a very smart physics professor.\n",
        "You are great at answering questions about physics in a concise and easy to understand manner.\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"\n",
        "You are a very good mathematician.\n",
        "You are great at answering math questions.\n",
        "You are so good because you are able to break down hard problems into their component parts,\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "# ==============================================================================\n",
        "embeddings = OpenAIEmbeddings()\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "# ==============================================================================\n",
        "@chain\n",
        "def prompt_router(query):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "\n",
        "    # Pick the prompt most similar to the input question\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "# ==============================================================================\n",
        "semantic_router = (\n",
        "    prompt_router\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flfD94Fas_Mm",
        "outputId": "6967a488-d68c-418d-ffb9-586c0a4cd201"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'As a physics professor, I can still answer this math question. The answer is 2.'"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"\n",
        " who you are anwsering me on this question between mathematician or physics professor?\n",
        " The question is 1 plus 1\n",
        "\"\"\"\n",
        "\n",
        "result = semantic_router.invoke(question) # Pass the question string directly\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjwCTUPO_0Vt"
      },
      "source": [
        "ตอนนี้คุณได้เห็นวิธีการกำหนดเส้นทางคำถามของผู้ใช้ไปยังแหล่งข้อมูลที่เกี่ยวข้องแล้ว มาพูดคุยกันถึงคำถามสำคัญข้อที่สามในการสร้างระบบ RAG ที่แข็งแกร่ง: เราจะแปลงภาษาธรรมชาติเป็นภาษาคำถามของแหล่งข้อมูลเป้าหมายได้อย่างไร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlDwDZ_N_7HF"
      },
      "source": [
        "# Query Construction\n",
        "**การสร้างคำถาม**\n",
        "\n",
        "ตามที่กล่าวไว้ก่อนหน้านี้ การสร้างเสริมด้วยการดึงข้อมูล (Retrieval-Augmented Generation: RAG) เป็นกลยุทธ์ที่มีประสิทธิภาพในการฝังและดึงข้อมูลที่ไม่ถูกโครงสร้างที่เกี่ยวข้องจาก Vector Store โดยยึดตามคำถาม แต่ข้อมูลส่วนใหญ่ที่มีให้ใช้ในแอปพลิเคชันการผลิตนั้นมีโครงสร้างและมักจะถูกเก็บไว้ในฐานข้อมูลเชิงสัมพันธ์ นอกจากนี้ ข้อมูลที่ไม่ถูกโครงสร้างที่ฝังอยู่ใน Vector Store ยังมีเมตาดาต้าที่มีโครงสร้างซึ่งมีข้อมูลที่สำคัญอีกด้วย\n",
        "\n",
        "การสร้างคำถาม คือกระบวนการแปลงคำถามภาษาธรรมชาติให้เป็นภาษาคำถามของฐานข้อมูลหรือแหล่งข้อมูลที่คุณกำลังโต้ตอบด้วย ดู รูปที่นี้\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-10.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "ตัวอย่างเช่น พิจารณาคำถาม`what are movies about aliens in the year 1980?` คำถามนี้ประกอบด้วยหัวข้อที่ไม่มีโครงสร้างซึ่งสามารถดึงข้อมูลได้ผ่านการฝังข้อมูล (aliens) แต่ยังประกอบด้วยส่วนประกอบที่มีโครงสร้างที่เป็นไปได้ (“year == 1980”)\n",
        "\n",
        "ส่วนต่อไปนี้จะเจาะลึกลงไปในรูปแบบต่างๆ ของการสร้างคำถาม\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JekNszjAYQ3"
      },
      "source": [
        "## Text-to-metadata-filter\n",
        "\n",
        "**ตัวกรองข้อความเป็นเมตาดาต้า**\n",
        "\n",
        "Vector Store ส่วนใหญ่มีขีดความสามารถในการจำกัดการค้นหาเวกเตอร์ของคุณโดยยึดตามเมตาดาต้า ในระหว่างกระบวนการฝังข้อมูล เราสามารถแนบคู่คีย์-ค่าเมตาดาต้าไปยังเวกเตอร์ในดัชนี และจากนั้นระบุนิพจน์ตัวกรองในภายหลังเมื่อคุณค้นหาในดัชนี\n",
        "\n",
        "LangChain มี Self-Query Retriever ที่ทำหน้าที่เป็นนามธรรมของตรรกะนี้ และทำให้การแปลคำถามภาษาธรรมชาติเป็นคำถามที่มีโครงสร้างสำหรับแหล่งข้อมูลต่างๆ ง่ายขึ้น การค้นหาด้วยตนเองใช้ LLM เพื่อดึงข้อมูลและดำเนินการกรองเมตาดาต้าที่เกี่ยวข้องโดยยึดตามคำถามของผู้ใช้และรูปแบบแผนผังเมตาดาต้าที่กำหนดไว้ก่อนหน้านี้ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZE0txZi_tSS"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "fields = [\n",
        "    AttributeInfo(\n",
        "        name=\"genre\",\n",
        "        description=\"The genre of the movie\",\n",
        "        type=\"string or list[string]\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"year\",\n",
        "        description=\"The year the movie was released\",\n",
        "        type=\"integer\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"director\",\n",
        "        description=\"The name of the movie director\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "description = \"Brief summary of a movie\"\n",
        "llm = OpenAI()\n",
        "\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm, db, description, fields,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaIHHbRCBDw9"
      },
      "source": [
        "**ผลลัพธ์นี้ส่งผลให้ได้ตัวดึงข้อมูลที่จะรับคำถามของผู้ใช้ และแบ่งออกเป็น**\n",
        "\n",
        "ตัวกรองที่จะนำไปใช้กับเมตาดาต้าของแต่ละเอกสารก่อน\n",
        "คำถามที่จะใช้สำหรับการค้นหาเชิงความหมายในเอกสาร\n",
        "ในการทำเช่นนี้ เราต้องอธิบายว่าฟิลด์ใดที่เมตาดาต้าของเอกสารของเรามีอยู่ คำอธิบายนั้นจะถูกรวมอยู่ในพรอมต์ ตัวดึงข้อมูลจะดำเนินการดังต่อไปนี้:\n",
        "\n",
        "1. ส่งพรอมต์การสร้างคำถามไปยัง LLM\n",
        "2. แยกวิเคราะห์ตัวกรองเมตาดาต้าและคำค้นหาที่เขียนใหม่จากเอาต์พุต LLM\n",
        "3. แปลงตัวกรองเมตาดาต้าที่สร้างโดย LLM ให้เป็นรูปแบบที่เหมาะสมสำหรับ Vector Store ของเรา\n",
        "4. ออกคำสั่งค้นหาความคล้ายคลึงกันกับ Vector Store โดยกรองเพื่อจับคู่เฉพาะเอกสารที่มีเมตาดาต้าผ่านตัวกรองที่สร้างขึ้นเท่านั้น"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWLO1WzpRsbl"
      },
      "source": [
        "## Text-to-SQL\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-11.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "SQL และฐานข้อมูลเชิงสัมพันธ์เป็นแหล่งข้อมูลที่มีโครงสร้างที่สำคัญ แต่ไม่โต้ตอบโดยตรงกับภาษาธรรมชาติ แม้ว่าเราสามารถใช้ LLM เพื่อแปลคำถามของผู้ใช้เป็นคำสั่ง SQL ได้โดยตรง แต่ก็มีความคลาดเคลื่อนเล็กน้อย\n",
        "\n",
        "นี่คือกลยุทธ์ที่มีประโยชน์บางอย่างสำหรับการแปลข้อความเป็น SQL ที่มีประสิทธิภาพ:\n",
        "\n",
        "**คำอธิบายฐานข้อมูล:**\n",
        "\n",
        "- เพื่อยึดคำสั่ง SQL LLM จะต้องได้รับคำอธิบายที่ถูกต้องเกี่ยวกับฐานข้อมูล พรอมต์การแปลข้อความเป็น SQL ทั่วไปใช้แนวคิดที่รายงานในเอกสารหลายฉบับ: ให้ LLM พร้อมด้วยคำอธิบาย`CREATE TABLE` สำหรับแต่ละตาราง รวมถึงชื่อคอลัมน์และชนิดข้อมูล เราสามารถให้แถวตัวอย่างสองสามแถว (เช่น 3 แถว) จากตารางได้เช่นกัน\n",
        "\n",
        "**ตัวอย่างแบบ Few-shot:**\n",
        "- การป้อนพรอมต์ด้วยตัวอย่างแบบ Few-shot ของการจับคู่คำถาม-คำสั่งสามารถ ปรับปรุงความแม่นยำในการสร้างคำสั่ง ได้ สิ่งนี้สามารถทำได้โดยเพียงแค่ต่อท้ายตัวอย่างแบบคงที่มาตรฐานในพรอมต์เพื่อแนะนำตัวแทนเกี่ยวกับวิธีที่ควรสร้างคำสั่งโดยยึดตามคำถาม\n",
        "การจัดการข้อผิดพลาด\n",
        "\n",
        "เมื่อเผชิญกับข้อผิดพลาด เราสามารถพยายามกู้คืนได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aItVzrNSMUW"
      },
      "source": [
        "นี่คือตัวอย่างโค้ดแบบเต็ม เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQwdScLCSL9U"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# replace this with the connection details of your db\n",
        "# ==============================================================================\n",
        "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# convert question to sql query\n",
        "# ==============================================================================\n",
        "write_query = create_sql_query_chain(llm, db)\n",
        "\n",
        "# Execute SQL query\n",
        "# ==============================================================================\n",
        "execute_query = QuerySQLDataBaseTool(db=db)\n",
        "\n",
        "# combined\n",
        "# ==============================================================================\n",
        "chain = write_query | execute_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BECy2EOESqhB"
      },
      "source": [
        "ที่นี่เราแปลงคำถามของผู้ใช้เป็นคำสั่ง SQL ที่เหมาะสมกับไวยากรณ์ของฐานข้อมูลของเราก่อน จากนั้นเราจึงดำเนินการคำสั่ง SQL นั้นบนฐานข้อมูลของเรา โปรดทราบว่าการดำเนินการคำสั่ง SQL ที่กำหนดโดย LLM จากอินพุตของผู้ใช้โดยพลการบนฐานข้อมูลของคุณเป็นอันตรายในแอปพลิเคชันการผลิต\n",
        "\n",
        "ในการใช้แนวคิดเหล่านี้ในการผลิต คุณต้องพิจารณาการรักษาความปลอดภัยจำนวนหนึ่งเพื่อลดความเสี่ยงของการรันคำสั่งที่ไม่ต้องการในฐานข้อมูลของคุณ ต่อไปนี้คือตัวอย่างบางส่วน:\n",
        "\n",
        "- คุณควรเรียกใช้คำสั่งบนฐานข้อมูลของคุณด้วยผู้ใช้ที่มีสิทธิ์อ่านเท่านั้น\n",
        "- ผู้ใช้ฐานข้อมูลที่เรียกใช้คำสั่งควรมีสิทธิ์เข้าถึงเฉพาะตารางที่คุณต้องการให้สามารถค้นหาได้\n",
        "- คุณควรเพิ่มการหมดเวลา (timeout) ให้กับคำสั่งที่เรียกใช้โดยแอปพลิเคชันนี้ ซึ่งจะช่วยให้มั่นใจได้ว่าแม้ว่าจะสร้างคำสั่งที่ใช้ทรัพยากรมากเกินไป ก็จะถูกยกเลิกก่อนที่จะใช้ทรัพยากรฐานข้อมูลของคุณมากเกินไป\n",
        "นี่ไม่ใช่รายการข้อควรพิจารณาด้านความปลอดภัยที่ครบถ้วน ความปลอดภัยของแอปพลิเคชัน LLM เป็นพื้นที่ที่กำลังพัฒนาอยู่ในขณะนี้ โดยมีการเพิ่มมาตรการรักษาความปลอดภัยเพิ่มเติมในคำแนะนำเมื่อพบช่องโหว่ใหม่ๆ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo1mwpspUKsc"
      },
      "source": [
        "## Text-to-Cypher\n",
        "\n",
        "Vector Store และฐานข้อมูลมีประโยชน์สำหรับข้อมูลที่มีโครงสร้างและไม่มีโครงสร้าง แต่ขาดข้อมูลเกี่ยวกับความสัมพันธ์ระหว่างจุดข้อมูลและเวกเตอร์ กราฟความรู้ เป็นประเภทของฐานข้อมูลที่มีโครงสร้างกราฟที่มีประโยชน์ ซึ่งจัดการกับข้อมูลที่มีความสัมพันธ์แบบหลายต่อหลายหรือลำดับชั้นที่ยากต่อการแสดงในรูปแบบตารางได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "เช่นเดียวกับฐานข้อมูลเชิงสัมพันธ์ ฐานข้อมูลกราฟใช้ภาษาคำถามเฉพาะที่เรียกว่า Cypher ซึ่งอาศัยไวยากรณ์แบบ ascii-art\n",
        "\n",
        "`(:Person {name:\"Tomaz\"})-[:LIVES_IN]->(:Country {name:\"Slovenia\"})\n",
        "`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QZir6kiUzlr"
      },
      "source": [
        "รูปแบบนั้นอธิบายโหนดที่มีป้ายกำกับว่า`Person`และคุณสมบัติชื่อ`Tomaz`ที่มีความสัมพันธ์`LIVES_IN`กับโหนด`Country`ของ`Slovenia` เช่นเดียวกับตัวอย่างก่อนหน้านี้ Text-to-Cypher สามารถแปลภาษาธรรมชาติเป็นคำสั่ง Cypher ได้\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain_community.graphs import Neo4jGraph # Assuming you are using Neo4j\n",
        "\n",
        "# Replace with your Neo4j connection details\n",
        "graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"password\")  \n",
        "\n",
        "graph.refresh_schema()\n",
        "\n",
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
        "    qa_llm = ChatOpenAI(temperature=0),\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "cypher_chain.run(\"How many open tickets there are?\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peSCs-4CVmb4"
      },
      "source": [
        "ตอนนี้คุณเข้าใจวิธีการสร้างคำถามแล้ว มาพูดคุยกันถึงหมวดหมู่คำถามถัดไปในการสร้างระบบ RAG ที่มีประสิทธิภาพ: **เราจะปรับปรุงกระบวนการจัดทำดัชนีของเราได้อย่างไร?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Ad9Z3qYUxe"
      },
      "source": [
        "# Indexing Optimization\n",
        "\n",
        "**การเพิ่มประสิทธิภาพการจัดทำดัชนี**\n",
        "\n",
        "ขั้นตอนการจัดทำดัชนี RAG พื้นฐานเกี่ยวข้องกับการแบ่งข้อความแบบง่ายๆ และการฝังข้อมูลชิ้นส่วนของเอกสารที่กำหนด อย่างไรก็ตาม วิธีการพื้นฐานนี้นำไปสู่ผลลัพธ์การดึงข้อมูลที่ไม่สอดคล้องกันและการเกิดภาพหลอนค่อนข้างสูง โดยเฉพาะอย่างยิ่งเมื่อแหล่งข้อมูลมีภาพและตาราง\n",
        "\n",
        "มีกลยุทธ์ต่างๆ เพื่อเพิ่มความถูกต้องและประสิทธิภาพของขั้นตอนการจัดทำดัชนี"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmg7_VtmYlIq"
      },
      "source": [
        "## Multi-Vector Retriever\n",
        "**การดึงข้อมูลหลายเวกเตอร์**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-12.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "เอกสารที่ประกอบด้วยข้อความและตารางผสมกันไม่สามารถแบ่งออกเป็นชิ้นส่วนโดยข้อความอย่างง่ายๆ และฝังข้อมูลเป็นบริบทของตารางทั้งหมดได้อย่างง่ายดาย เนื่องจากบริบทของตารางทั้งหมดอาจสูญหายไปได้ เพื่อแก้ปัญหานี้ เราสามารถแยกเอกสารที่เราต้องการใช้สำหรับการสังเคราะห์คำตอบออกจากข้อมูลอ้างอิงที่เราต้องการใช้สำหรับตัวดึงข้อมูล รูปดังกล่าว แสดงให้เห็นวิธีการ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSNHVwVcY6Ti"
      },
      "source": [
        "ตัวอย่างเช่น ในกรณีของเอกสารที่มีตาราง เราสามารถสร้างและฝังข้อมูลสรุปขององค์ประกอบตารางก่อน โดยตรวจสอบให้แน่ใจว่าแต่ละสรุปมีการอ้างอิง`id`ไปยังตารางดิบที่สมบูรณ์ จากนั้น เราจะจัดเก็บตารางดิบที่อ้างอิงใน Docstore ที่แยกต่างหาก สุดท้าย เมื่อคำถามของผู้ใช้ดึงข้อมูลสรุปตาราง เราจะส่งตารางดิบที่อ้างอิงทั้งหมดเป็นบริบทไปยังพรอมต์สุดท้ายที่ส่งไปยัง LLM สำหรับการสังเคราะห์คำตอบ วิธีการนี้ช่วยให้เราสามารถจัดเตรียมบริบทข้อมูลที่สมบูรณ์ให้กับโมเดลเพื่อตอบคำถามได้\n",
        "\n",
        "นี่คือตัวอย่าง เริ่มต้น เราจะใช้ LLM เพื่อสร้างสรุปของเอกสาร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7drA-T5VD4p",
        "outputId": "263b9cd5-acf3-4d1c-915d-aec88a0e1323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The document discusses LLM-powered autonomous agents, focusing on the components of planning, memory, and tool use. It explores techniques such as task decomposition, self-reflection, and memory retrieval for these agents. It also presents case studies like scientific discovery agents and generative agents simulations. The challenges of finite context length, reliability of natural language interface, and long-term planning are highlighted. The document concludes with citations and references to relevant research papers.',\n",
              " 'The document discusses the importance of high-quality human data for training deep learning models. It covers topics such as the role of human raters in data quality, the wisdom of the crowd in data aggregation, methods to measure rater agreement, and two paradigms for data annotation. Additionally, it explores techniques like influence functions and tracking prediction changes during training to improve data quality and identify mislabeled data points. The document also includes references to related studies and provides insights into how to think about and work with high-quality human data for AI applications.']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
        "docs.extend(loader.load())\n",
        "\n",
        "import uuid\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | ChatOpenAI(max_retries=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
        "summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbfct2ZKZYne"
      },
      "source": [
        "ถัดไป ให้เราสร้าง Vector Store และ Doc Store เพื่อจัดเก็บสรุปดิบและการฝังข้อมูลของพวกมัน:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejddnT33ZGAL"
      },
      "outputs": [],
      "source": [
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "# ==============================================================================\n",
        "vectorstore = Chroma(collection_name=\"summaries\",\n",
        "                     embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "# ==============================================================================\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever\n",
        "# ==============================================================================\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n",
        "# Docs linked to summaries\n",
        "# ==============================================================================\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]\n",
        "\n",
        "# Add\n",
        "# ==============================================================================\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DqfeBwZsD0"
      },
      "source": [
        "สุดท้าย ให้เราเรียกใช้เอกสารบริบทที่สมบูรณ์ที่เกี่ยวข้องโดยยึดตามคำถาม:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX_G8wxDZhys",
        "outputId": "4d5be626-ded5-48af-c451-3eca843a80a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'doc_id': '2e119843-42de-4957-b22f-85dde2dcbda3'}, page_content='The document discusses LLM-powered autonomous agents, focusing on the components of planning, memory, and tool use. It explores techniques such as task decomposition, self-reflection, and memory retrieval for these agents. It also presents case studies like scientific discovery agents and generative agents simulations. The challenges of finite context length, reliability of natural language interface, and long-term planning are highlighted. The document concludes with citations and references to relevant research papers.')"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Memory in agents\"\n",
        "sub_docs = vectorstore.similarity_search(query,k=1)\n",
        "sub_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hdb3qNBZ9Ko"
      },
      "source": [
        "## RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
        "\n",
        "**RAPTOR:** การประมวลผลเชิงนามธรรมแบบเรียกซ้ำสำหรับการดึงข้อมูลแบบมีโครงสร้างต้นไม้\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure3-13.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "ระบบ RAG จำเป็นต้องจัดการกับคำถามระดับล่างที่อ้างอิงข้อเท็จจริงเฉพาะที่พบในเอกสารเดียวหรือคำถามระดับสูงที่กลั่นกรองแนวคิดที่ครอบคลุมเอกสารหลายฉบับ การจัดการคำถามทั้งสองประเภทอาจเป็นเรื่องท้าทายด้วยการดึงข้อมูล kNN แบบทั่วไปมากกว่าชิ้นส่วนเอกสาร\n",
        "\n",
        "การประมวลผลเชิงนามธรรมแบบเรียกซ้ำสำหรับการดึงข้อมูลแบบมีโครงสร้างต้นไม้ (RAPTOR) เป็นกลยุทธ์ที่มีประสิทธิภาพซึ่งเกี่ยวข้องกับการสร้างสรุปเอกสารที่จับภาพแนวคิดระดับสูง ฝังข้อมูลและจัดกลุ่มเอกสารเหล่านั้น จากนั้นสรุปแต่ละกลุ่ม สิ่งนี้ทำซ้ำๆ กัน สร้างต้นไม้ของสรุปที่มีแนวคิดระดับสูงมากขึ้นเรื่อยๆ จากนั้นสรุปและเอกสารเริ่มต้นจะถูกจัดทำดัชนีร่วมกัน ให้ความครอบคลุมคำถามของผู้ใช้ในระดับต่ำถึงระดับสูง รูปดังกล่าว แสดงให้เห็น\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5con8iVKaaAR"
      },
      "source": [
        "## ColBERT\n",
        "**ColBERT: การเพิ่มประสิทธิภาพการฝังข้อมูล**\n",
        "\n",
        "หนึ่งในความท้าทายของการใช้โมเดลการฝังข้อมูลในระหว่างขั้นตอนการจัดทำดัชนีคือ การบีบอัดข้อความลงในการแสดงข้อมูลแบบเวกเตอร์ที่มีความยาวคงที่ (fixed-length) ซึ่งจับภาพเนื้อหาเชิงความหมายของเอกสาร แม้ว่าการบีบอัดนี้จะมีประโยชน์สำหรับการดึงข้อมูล แต่การฝังข้อมูลที่ไม่เกี่ยวข้องหรือซ้ำซ้อนอาจนำไปสู่การเกิดภาพหลอนในเอาต์พุต LLM สุดท้าย\n",
        "\n",
        "วิธีแก้ปัญหานี้คือการดำเนินการดังต่อไปนี้:\n",
        "\n",
        "สร้างการฝังข้อมูลเชิงบริบทสำหรับโทเค็นแต่ละตัวในเอกสารและคำถาม\n",
        "คำนวณและให้คะแนนความคล้ายคลึงกันระหว่างโทเค็นคำถามแต่ละตัวกับโทเค็นเอกสารทั้งหมด\n",
        "รวมคะแนนความคล้ายคลึงกันสูงสุดของการฝังข้อมูลคำถามแต่ละรายการกับการฝังข้อมูลเอกสารใดๆ เพื่อรับคะแนนสำหรับแต่ละเอกสาร\n",
        "สิ่งนี้ส่งผลให้ได้วิธีการฝังข้อมูลแบบละเอียดและมีประสิทธิภาพสำหรับการดึงข้อมูลที่ดีขึ้น โชคดีที่โมเดลการฝังข้อมูลที่เรียกว่า ColBERT นำเสนอโซลูชันสำหรับปัญหานี้\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LdHh5vxzYRP"
      },
      "source": [
        "```python\n",
        "# RAGatouille is a library makes it simple to use ColBERT\n",
        "#! pip install -U ragatouille\n",
        "# ==============================================================================\n",
        "from ragatouille import RAGPretrainedModel\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "import requests\n",
        "# ==============================================================================\n",
        "def get_wikipedia_page(title: str):\n",
        "    \"\"\"\n",
        "    Retrieve the full text content of a Wikipedia page.\n",
        "    :param title: str - Title of the Wikipedia page.\n",
        "    :return: str - Full text content of the page as raw string.\n",
        "    \"\"\"\n",
        "    # Wikipedia API endpoint\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "    }\n",
        "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
        "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
        "    response = requests.get(URL, params=params, headers=headers)\n",
        "    data = response.json()\n",
        "    # Extracting page content\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "    return page[\"extract\"] if \"extract\" in page else None\n",
        "\n",
        "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
        "## Create an index\n",
        "# ==============================================================================\n",
        "RAG.index(\n",
        "    collection=[full_document],\n",
        "    index_name=\"Miyazaki-123\",\n",
        "    max_document_length=180,\n",
        "    split_documents=True,\n",
        ")\n",
        "\n",
        "#query\n",
        "# ==============================================================================\n",
        "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
        "results\n",
        "\n",
        "#utilize langchain retriever\n",
        "# ==============================================================================\n",
        "retriever = RAG.as_langchain_retriever(k=3)\n",
        "retriever.invoke(\"What animation studio did Miyazaki found?\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Rd0by8nQD9"
      },
      "source": [
        "# Summary\n",
        "**สรุป**\n",
        "\n",
        "บทนี้ได้กล่าวถึงกลยุทธ์ที่ทันสมัยต่างๆ เพื่อดึงข้อมูลเอกสารที่เกี่ยวข้องมากที่สุดอย่างมีประสิทธิภาพโดยยึดตามคำถามของผู้ใช้ และสังเคราะห์เอกสารเหล่านั้นกับพรอมต์ของคุณเพื่อช่วยให้ LLM สร้างผลลัพธ์ที่ถูกต้องและทันสมัย\n",
        "\n",
        "ตามที่กล่าวไว้ ระบบการสร้างเสริมด้วยการดึงข้อมูล (RAG) ที่พร้อมใช้งานสำหรับการผลิตต้องใช้กลยุทธ์ที่มีประสิทธิภาพอย่างหลากหลายที่สามารถดำเนินการแปลงคำถาม การสร้างคำถาม การกำหนดเส้นทาง และการเพิ่มประสิทธิภาพการจัดทำดัชนี\n",
        "\n",
        "การแปลงคำถามช่วยให้แอป AI ของคุณสามารถแปลงคำถามของผู้ใช้ที่คลุมเครือหรือผิดรูปแบบให้เป็นคำถามแทนที่เหมาะสมที่สุดสำหรับการดึงข้อมูล การสร้างคำถามช่วยให้แอป AI ของคุณสามารถแปลงคำถามของผู้ใช้เป็นไวยากรณ์ของภาษาคำถามของฐานข้อมูลหรือแหล่งข้อมูลที่ข้อมูลที่มีโครงสร้างอาศัยอยู่ การกำหนดเส้นทางช่วยให้แอป AI ของคุณสามารถกำหนดเส้นทางคำถามของผู้ใช้แบบไดนามิกเพื่อดึงข้อมูลที่เกี่ยวข้องจากแหล่งข้อมูลที่เกี่ยวข้อง สุดท้าย กลยุทธ์การเพิ่มประสิทธิภาพการจัดทำดัชนีช่วยให้แอป AI ของคุณสามารถปรับปรุงคุณภาพของการฝังข้อมูล และดำเนินการดึงข้อมูลเอกสารที่แม่นยำซึ่งมีข้อมูลกึ่งโครงสร้าง รวมถึงตาราง\n",
        "\n",
        "**ในบทต่อไป เราจะสร้างบนพื้นฐานความรู้เหล่านี้เพื่อเพิ่มหน่วยความจำให้กับแชทบอท AI ของคุณ เพื่อให้สามารถจดจำและเรียนรู้จากการโต้ตอบแต่ละครั้ง ซึ่งจะช่วยให้ผู้ใช้สามารถ 'แชท' กับแอปพลิเคชันในการสนทนาแบบหลายเทิร์นเช่น ChatGPT**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
