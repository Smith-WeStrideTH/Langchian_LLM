{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhSm1wNcgtS3"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ks3N66EnS8wl"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pprint import pprint\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJTAyZR3S9-z",
        "outputId": "a94fb3e2-12f2-4cf1-a45e-7dab9a9d2b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')\n",
        "\n",
        "# Get the API key from the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was found\n",
        "if api_key:\n",
        "  print(\"API Key loaded successfully.\")\n",
        "else:\n",
        "  print(\"API Key not found in the environment variables.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDzrGIAKzXL"
      },
      "source": [
        "# Chapter 8. Patterns to Make the Most of LLMs\n",
        "\n",
        "LLM ในปัจจุบันมีข้อจำกัดที่สำคัญ แต่ไม่ได้หมายความว่าแอปพลิเคชัน LLM ในฝันของคุณเป็นไปไม่ได้ที่จะสร้าง ประสบการณ์ที่คุณออกแบบสำหรับผู้ใช้ของแอปพลิเคชันของคุณจำเป็นต้องทำงานรอบๆ และในอุดมคติจะทำงานร่วมกับข้อจำกัดเหล่านั้น\n",
        "\n",
        "**บทที่ 5 ได้กล่าวถึงการแลกเปลี่ยนที่สำคัญที่เราเผชิญเมื่อสร้างแอปพลิเคชัน LLM**:\n",
        "\n",
        "- การแลกเปลี่ยนระหว่างตัวแทน (ความสามารถในการดำเนินการโดยอิสระ) และความน่าเชื่อถือ (ระดับที่เราสามารถไว้วางใจผลลัพธ์ของมัน) โดยสัญชาตญาณ แอปพลิเคชัน LLM ใดๆ ก็ตามจะมีประโยชน์กับเรามากกว่า หากดำเนินการเพิ่มเติมโดยไม่ต้องมีส่วนร่วมจากเรา แต่ถ้าเราปล่อยให้ตัวแทนไปไกลเกินไป แอปพลิเคชันจะหลีกเลี่ยงไม่ได้ที่จะทำสิ่งที่เราไม่ต้องการให้ทำ\n",
        "\n",
        "รูปนี้ แสดงให้เห็นถึงการแลกเปลี่ยนนี้\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"https://github.com/Smith-WeStrideTH/Langchian_LLM/blob/main/work/pics/Figure8-1.png?raw=1\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbSelUY9MFiy"
      },
      "source": [
        "เพื่อยืมแนวคิดจากสาขาอื่นๆ *(In finance, the efficient frontier in portfolio optimization; in economics, a production-possibility frontier; in engineering, the Pareto front.)* เราสามารถมองเห็นการแลกเปลี่ยนเป็นแนวเขตแดน - จุดทั้งหมดบนเส้นโค้งนี้คือสถาปัตยกรรม LLM ที่เหมาะสมที่สุดสำหรับแอปพลิเคชันบางอย่าง ทำเครื่องหมายตัวเลือกที่แตกต่างกันระหว่างตัวแทนและความน่าเชื่อถือ (ดูบทที่ 5 สำหรับภาพรวมของสถาปัตยกรรมแอปพลิเคชัน LLM ที่แตกต่างกัน) ตัวอย่างเช่น สังเกตว่าสถาปัตยกรรมเชนมีตัวแทนค่อนข้างต่ำ แต่มีความน่าเชื่อถือสูงกว่า ในขณะที่สถาปัตยกรรมตัวแทนมีตัวแทนสูงกว่าแต่มีความน่าเชื่อถือต่ำกว่า\n",
        "\n",
        "มาสัมผัสกันโดยย่อเกี่ยวกับวัตถุประสงค์เพิ่มเติม (แต่ยังคงสำคัญ) อีกหลายประการที่คุณอาจต้องการให้แอปพลิเคชัน LLM ของคุณมี แอปพลิเคชัน LLM แต่ละตัวจะถูกออกแบบมาสำหรับการผสมผสานวัตถุประสงค์อย่างน้อยหนึ่งอย่างที่แตกต่างกัน:\n",
        "\n",
        "- **เวลาแฝง (Latency)**\n",
        "ลดเวลาในการรับคำตอบขั้นสุดท้ายให้น้อยที่สุด\n",
        "\n",
        "- **ภูมิภาคปกครองตนเอง (Autonomy)**\n",
        "ลดการหยุดชะงักสำหรับอินพุตของมนุษย์\n",
        "\n",
        "- **ความแปรปรวน (Variance)**\n",
        "ลดความแปรปรวนระหว่างการเรียกใช้งาน\n",
        "\n",
        "นี่ไม่ได้มีจุดประสงค์เพื่อเป็นรายการที่ครอบคลุมของวัตถุประสงค์ที่เป็นไปได้ทั้งหมด แต่เป็นเพียงตัวอย่างของการแลกเปลี่ยนที่คุณเผชิญเมื่อสร้างแอปพลิเคชันของคุณ แต่ละวัตถุประสงค์จะทำให้วัตถุประสงค์อื่นๆ เป็นโมฆะหากได้รับน้ำหนักเต็มที่ (ตัวอย่างเช่น แอปพลิเคชันที่มีเวลาแฝงขั้นต่ำคือแอปพลิเคชันที่ไม่ได้ทำอะไรเลย) ดังรูปที่แสดงให้เห็นนี้\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"https://github.com/Smith-WeStrideTH/Langchian_LLM/blob/main/work/pics/Figure8-2.png?raw=1\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7rxn6vKNjHF"
      },
      "source": [
        "สิ่งที่เราต้องการจริงๆ ในฐานะนักพัฒนาแอปพลิเคชันคือการขยายขอบเขตของความสามารถ\n",
        "\n",
        "\"สำหรับระดับความน่าเชื่อถือที่เท่ากัน เราต้องการให้ระบบมีความสามารถในการทำงานที่หลากหลายมากขึ้น\"\n",
        "และ \"สำหรับระดับความสามารถในการทำงานที่เท่ากัน เราต้องการให้ระบบมีความน่าเชื่อถือมากขึ้น\"\n",
        "**บทนี้กล่าวถึงเทคนิคต่างๆ ที่คุณสามารถใช้เพื่อให้บรรลุเป้าหมายนี้:**\n",
        "\n",
        "- **เอาต์พุตแบบสตรีม/กลางทาง (Streaming/intermediate output):**\n",
        "การรับรู้ถึงความคืบหน้าหรือได้รับผลลัพธ์บางส่วนระหว่างการประมวลผลจะช่วยให้ผู้ใช้รู้สึกสบายใจมากขึ้นแม้ว่าระบบจะใช้เวลานานในการตอบสนอง\n",
        "\n",
        "- **เอาต์พุตที่มีโครงสร้าง (Structured output):**\n",
        "การกำหนดรูปแบบเอาต์พุตที่ชัดเจน เช่น JSON หรือ XML จะช่วยให้ LLM ผลิตผลลัพธ์ที่สอดคล้องกับความคาดหวังของเราได้มากขึ้น\n",
        "\n",
        "- **การมีส่วนร่วมของมนุษย์ (Human in the loop):**\n",
        "การอนุญาตให้มนุษย์เข้ามามีส่วนร่วมในกระบวนการทำงานของระบบ เช่น การหยุดชั่วคราว การอนุมัติผลลัพธ์ การแก้ไข หรือการยกเลิกคำสั่ง จะช่วยเพิ่มความยืดหยุ่นและความน่าเชื่อถือของระบบ\n",
        "\n",
        "- **โหมดข้อความทับซ้อน (Double texting modes):**\n",
        "หากแอปพลิเคชัน LLM ใช้เวลานานในการตอบสนอง ผู้ใช้มักจะส่งคำสั่งใหม่เข้ามาโดยไม่รอคำตอบจากคำสั่งก่อนหน้า ดังนั้น ระบบควรออกแบบให้สามารถรับมือกับสถานการณ์นี้ได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD0_0E8iSTPM"
      },
      "source": [
        "# Structured Output\n",
        "\n",
        "\n",
        "บ่อยครั้งที่จำเป็นอย่างยิ่งที่จะต้องให้ LLM ส่งคืนเอาต์พุตที่มีโครงสร้าง อาจเป็นเพราะการใช้งานเอาต์พุตนั้นต่อไปต้องการข้อมูลในรูปแบบเฉพาะ (เช่น การกำหนดชื่อและชนิดข้อมูลของฟิลด์ต่างๆ ในเอาต์พุต) หรือเพื่อลดความแปรปรวนของผลลัพธ์ที่อาจเกิดขึ้นหากปล่อยให้ LLM สร้างเอาต์พุตในรูปแบบข้อความอิสระ\n",
        "\n",
        "มีกลยุทธ์ที่แตกต่างกันหลายอย่างที่คุณสามารถใช้ได้กับ LLM ที่แตกต่างกัน:\n",
        "\n",
        "- **การเขียนพรอมต์:**\n",
        "วิธีนี้เกี่ยวข้องกับการขอให้ LLM (อย่างสุภาพ) ส่งคืนเอาต์พุตในรูปแบบที่ต้องการ (เช่น JSON, XML หรือ CSV)\n",
        "ข้อดีของการเขียนพรอมต์คือสามารถใช้ได้กับ LLM เกือบทุกตัว\n",
        "ข้อเสียคือ การเขียนพรอมต์เพียงอย่างเดียวอาจไม่เพียงพอที่จะรับประกันว่า LLM จะส่งคืนเอาต์พุตในรูปแบบที่ต้องการเสมอไป\n",
        "\n",
        "- **การเรียกใช้งานเครื่องมือ:**\n",
        "วิธีนี้ใช้ได้กับ LLM ที่ได้รับการปรับแต่งให้เลือกจากรายการรูปแบบเอาต์พุตที่เป็นไปได้ และสร้างผลลัพธ์ที่สอดคล้องกับรูปแบบที่เลือก\n",
        "โดยทั่วไปจะเกี่ยวข้องกับการเขียน ชื่อ เพื่อระบุรูปแบบเอาต์พุต คำอธิบาย เพื่อช่วยให้ LLM ตัดสินใจว่าควรเลือกใช้รูปแบบใด และรูปแบบของเอาต์พุตที่ต้องการ (โดยทั่วไปจะอยู่ในรูปแบบ JSONSchema) สำหรับแต่ละรูปแบบเอาต์พุตที่เป็นไปได้\n",
        "\n",
        "- **โหมด JSON:**\n",
        "นี่คือโหมดที่มีอยู่ใน LLM บางตัว (เช่น โมเดล OpenAI รุ่นล่าสุด) ที่บังคับให้เอาต์พุตเป็นเอกสาร JSON ที่ถูกต้อง\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRL1QCPhSvkq"
      },
      "source": [
        "โมเดลต่างๆอาจรองรับรูปแบบที่แตกต่างกันเหล่านี้ด้วยพารามิเตอร์ที่แตกต่างกันเล็กน้อย เพื่อให้ง่ายต่อการรับเอาต์พุตที่มีโครงสร้างจาก LLM โมเดล LangChain ได้ใช้ส่วนต่อประสานทั่วไป ซึ่งเป็นเมธอดที่เรียกว่า .`with_structured_output` โดยการเรียกใช้เมธอดนี้ และส่งผ่าน JSON schema หรือโมเดล Pydantic (ใน Python) หรือ Zod (ใน JS) โมเดลจะเพิ่มพารามิเตอร์ของโมเดลและตัวแยกวิเคราะห์เอาต์พุตที่จำเป็นเพื่อสร้างและส่งคืนเอาต์พุตที่มีโครงสร้าง เมื่อโมเดลใดโมเดลหนึ่งใช้กลยุทธ์ข้างต้นมากกว่าหนึ่งวิธี คุณสามารถกำหนดค่าว่าจะใช้วิธีใด\n",
        "\n",
        "มาสร้างรูปแบบที่จะใช้กัน:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XYXsgjunG1bU"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-F53IlDTBeO"
      },
      "source": [
        "**สังเกตว่าเราใส่ใจในการเพิ่มคำอธิบายสำหรับแต่ละฟิลด์**\n",
        "\n",
        "นี่เป็นสิ่งสำคัญ เพราะร่วมกับชื่อของฟิลด์ ข้อมูลนี้จะเป็นสิ่งที่ LLM ใช้ในการตัดสินใจว่าส่วนใดของเอาต์พุตควรอยู่ในแต่ละฟิลด์\n",
        "เราสามารถกำหนดรูปแบบด้วยสัญกรณ์ JSONSchema ดิบได้ ซึ่งจะมีลักษณะดังนี้:\n",
        "```\n",
        "{'properties': {'setup': {'description': 'The setup of the joke',\n",
        "   'title': 'Setup',\n",
        "   'type': 'string'},\n",
        "  'punchline': {'description': 'The punchline to the joke',\n",
        "   'title': 'Punchline',\n",
        "   'type': 'string'}},\n",
        " 'required': ['setup', 'punchline'],\n",
        " 'title': 'Joke',\n",
        " 'type': 'object'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4htIdTgcTTPZ"
      },
      "source": [
        "และตอนนี้มาดูการสร้างเอาต์พุตที่สอดคล้องกับ schema นี้โดย LLM\n",
        "\n",
        "| Feature        | Structured Outputs | JSON mode | Adheres to schema | Compatible models                                    | Enabling                                        |\n",
        "|----------------|--------------------|-----------|--------------------|----------------------------------------------------|-------------------------------------------------|\n",
        "| JSON mode      | Yes               | Yes        | Yes (see supported schemas) | • gpt-4o-mini<br>• gpt-4o-2024-08-06 and later<br>• gpt-3.5-turbo<br>• gpt-4-*<br>• gpt-4o-* | `response_format: { type: \"json_schema\", json_schema: {\"strict\": true, \"schema\": ...} }` |\n",
        "| Structured Output | Yes               | Yes        | Yes (see supported schemas) | • gpt-4o-mini<br>• gpt-4o-2024-08-06 and later<br>• gpt-3.5-turbo<br>• gpt-4-*<br>• gpt-4o-* | `response_format: { type: \"json_object\" }` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lztv_ASWW8K",
        "outputId": "20bb0310-611f-4c28-db98-7bb4ff107aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!'\n",
            "Why was the cat sitting on the computer?\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "model = model.with_structured_output(Joke)\n",
        "\n",
        "result = model.invoke(\"Tell me a joke about cats\")\n",
        "print(result)\n",
        "print(result.setup)\n",
        "print(result.punchline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TR5ije5XQZP"
      },
      "source": [
        "| Feature | ChatOpenAI | OpenAI |\n",
        "|---|---|---|\n",
        "| **Purpose** | Designed for conversational models (e.g., gpt-3.5-turbo, gpt-4) | Designed for older models (e.g., text-davinci-003) |\n",
        "| **API Endpoint** | chat/completions | completions |\n",
        "| **Input** | List of messages (conversation history) | Single string |\n",
        "| **Output** | Single message | Single string |\n",
        "| **Key Use Cases** | Conversations, chatbots, maintaining context | Simple input/output interactions |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1dLcNOTS1w",
        "outputId": "2bf146c5-aefc-4351-ae0a-de812e7195f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!'\n",
            "Why was the cat sitting on the computer?\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Define your data structure\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "\n",
        "# Generate a response and parse it into your structure\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and harmless AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke about cats\"}\n",
        "    ],\n",
        "    response_format=Joke,  # Pass your Pydantic model\n",
        ")\n",
        "\n",
        "# Extract the parsed joke\n",
        "joke = completion.choices[0].message.parsed\n",
        "\n",
        "# Print the joke\n",
        "print(joke)\n",
        "print(joke.setup)\n",
        "print(joke.punchline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NGKHGGNZiXg"
      },
      "source": [
        "```python\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "model = model.with_structured_output(Joke)\n",
        "result = model.invoke(\"Tell me a joke about cats\")\n",
        "```\n",
        "**มีสองสามสิ่งที่ควรสังเกต:**\n",
        "\n",
        "- เราสร้างอินสแตนซ์ของโมเดลตามปกติ โดยระบุชื่อโมเดลที่จะใช้และพารามิเตอร์อื่นๆ\n",
        "- (Low Temperature)อุณหภูมิต่ำมักจะเหมาะกับการสร้างเอาต์พุตที่มีโครงสร้าง เนื่องจากจะช่วยลดโอกาสที่ LLM จะสร้างเอาต์พุตที่ไม่ถูกต้องซึ่งไม่เป็นไปตาม Schema\n",
        "- หลังจากนั้น เราจะแนบ Schema เข้ากับโมเดล ซึ่งจะส่งคืนอ็อบเจ็กต์ใหม่ ซึ่งจะสร้างเอาต์พุตที่ตรงกับ Schema ที่ให้ไว้ เมื่อคุณส่งอ็อบเจ็กต์ Pydantic หรือ Zod สำหรับ Schema สิ่งนี้จะถูกใช้สำหรับการตรวจสอบความถูกต้องด้วย นั่นคือ หาก LLM สร้างเอาต์พุตที่ไม่เป็นไปตามนั้น ข้อผิดพลาดในการตรวจสอบความถูกต้องจะถูกส่งคืนให้คุณแทนที่จะเป็นเอาต์พุตที่ล้มเหลว\n",
        "- สุดท้าย เราเรียกใช้โมเดลด้วยอินพุต (แบบอิสระ) ของเรา และรับเอาต์พุตที่ตรงกับโครงสร้างที่เราต้องการ\n",
        "\n",
        "รูปแบบของการใช้อินพุตที่มีโครงสร้างนี้มีประโยชน์มากทั้งในฐานะเครื่องมือแบบสแตนด์อโลน ดังที่คุณเห็นด้านบน และเป็นส่วนหนึ่งของแอปพลิเคชันขนาดใหญ่ ตัวอย่างเช่น กลับไปดูบทที่ 5 ซึ่งเราใช้ความสามารถนี้เพื่อใช้ขั้นตอนการกำหนดเส้นทางของสถาปัตยกรรมเราเตอร์"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpN6QxDza0Lx"
      },
      "source": [
        "# Intermediate Output\n",
        "**เอาต์พุตระดับกลาง**\n",
        "\n",
        "ยิ่งสถาปัตยกรรม LLM ของคุณซับซ้อนมากขึ้นเท่าไหร่ ก็มีแนวโน้มที่จะใช้เวลาในการประมวลผลนานขึ้นเท่านั้น หากคุณลองนึกถึงไดอะแกรมสถาปัตยกรรมในบทที่ 5 และ 6 ทุกครั้งที่คุณเห็นหลายขั้นตอน (หรือโหนด) ที่เชื่อมต่อกันเป็นลำดับหรือเป็นวง นั่นเป็นตัวบ่งชี้ว่าเวลาที่ใช้สำหรับการเรียกใช้งานทั้งหมดกำลังเพิ่มขึ้น\n",
        "\n",
        "ความหน่วงที่เพิ่มขึ้นนี้ หากไม่ได้รับการแก้ไข อาจเป็นอุปสรรคต่อการนำแอปพลิเคชัน LLM ไปใช้โดยผู้ใช้ เนื่องจากผู้ใช้ส่วนใหญ่คาดหวังว่าแอปพลิเคชันคอมพิวเตอร์จะสร้างเอาต์พุตภายในไม่กี่วินาที มีกลยุทธ์หลายประการที่จะทำให้ความหน่วงที่สูงขึ้นนั้นยอมรับได้มากขึ้น **แต่ทั้งหมดอยู่ภายใต้แนวคิดของเอาต์พุตแบบสตรีมมิ่ง**\n",
        "- นั่นคือการรับเอาต์พุตจากแอปพลิเคชันในขณะที่แอปพลิเคชันยังคงทำงานอยู่\n",
        "\n",
        "สำหรับส่วนนี้ เราจะใช้สถาปัตยกรรมสุดท้ายที่อธิบายไว้ในหัวข้อ\n",
        "**“การจัดการกับเครื่องมือจำนวนมาก”** ในบทที่ 6 โปรดกลับไปดูบทนั้นสำหรับโค้ดแบบเต็ม\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6XpxLH0eCnq"
      },
      "source": [
        "ในการสร้างเอาต์พุตระดับกลางด้วย LangGraph สิ่งที่คุณต้องทำคือเรียกกราฟด้วยเมธอด `stream` ซึ่งจะให้ผลลัพธ์ของแต่ละโหนดทันทีที่แต่ละโหนดเสร็จสิ้น ลองมาดูว่ามันมีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vZ7QhdQnenX1"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from typing import Annotated, TypedDict\n",
        "\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "@tool\n",
        "def calculator(query: str) -> str:\n",
        "    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\n",
        "    return ast.literal_eval(query)\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "tools = [search, calculator]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "model = ChatOpenAI(temperature=0.1)\n",
        "\n",
        "tools_retriever = InMemoryVectorStore.from_documents(\n",
        "    [Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
        "    embeddings,\n",
        ").as_retriever()\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    selected_tools: list[str]\n",
        "\n",
        "def model_node(state: State) -> State:\n",
        "    selected_tools = [\n",
        "        tool for tool in tools if tool.name in state[\"selected_tools\"]\n",
        "    ]\n",
        "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
        "    return {\"messages\": res}\n",
        "\n",
        "def select_tools(state: State) -> State:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    tool_docs = tools_retriever.invoke(query)\n",
        "    return {\"selected_tools\": [doc.metadata[\"name\"] for doc in tool_docs]}\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"select_tools\", select_tools)\n",
        "builder.add_node(\"model\", model_node)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "builder.add_edge(START, \"select_tools\")\n",
        "builder.add_edge(\"select_tools\", \"model\")\n",
        "builder.add_conditional_edges(\"model\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"model\")\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "TKog4nV4esSe",
        "outputId": "6cb0e3da-db54-4764-83ed-06f35b40ed03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "image_bytes = graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
        "\n",
        "display(Image(data=image_bytes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GSICHQ0ZUCO1"
      },
      "outputs": [],
      "source": [
        "# from langchain.schema import HumanMessage\n",
        "\n",
        "# input = {\n",
        "#   \"messages\": [\n",
        "#     HumanMessage(\"2+2\")\n",
        "#   ]\n",
        "# }\n",
        "# for c in graph.stream(input, stream_mode='updates'):\n",
        "#     print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc8zUwxCe5a_"
      },
      "source": [
        "สังเกตว่ารายการเอาต์พุตแต่ละรายการเป็นพจนานุกรม โดยมีชื่อของโหนดที่ส่งออกมาเป็นคีย์ และเอาต์พุตของโหนดนั้นเป็นค่า สิ่งนี้ให้ข้อมูลสำคัญสองส่วนแก่คุณ:\n",
        "\n",
        "- แอปพลิเคชันอยู่ที่ใดในปัจจุบัน นั่นคือ หากคุณนึกถึงไดอะแกรมสถาปัตยกรรมที่แสดงในบทก่อนๆ เราอยู่ในส่วนใดของไดอะแกรมนั้นในปัจจุบัน\n",
        "- การอัปเดตแต่ละครั้งไปยังสถานะที่ใช้ร่วมกันของแอปพลิเคชัน ซึ่งรวมกันเป็นเอาต์พุตสุดท้ายของกราฟ\n",
        "\n",
        "นอกจากนี้ LangGraph ยังรองรับโหมดสตรีมเพิ่มเติม:\n",
        "\n",
        "- `updates` (การอัปเดต) นี่คือโหมดเริ่มต้น อธิบายไว้ข้างต้น\n",
        "- `values` (ค่า) โหมดนี้จะให้สถานะปัจจุบันของกราฟทุกครั้งที่สถานะเปลี่ยนแปลง นั่นคือหลังจากที่โหนดแต่ละชุดประมวลผลเสร็จสิ้น โหมดนี้มีประโยชน์เมื่อวิธีการแสดงเอาต์พุตของคุณต่อผู้ใช้ติดตามรูปร่างของสถานะกราฟอย่างใกล้ชิด\n",
        "- `debug` (การแก้ไขข้อบกพร่อง) โหมดนี้จะให้เหตุการณ์โดยละเอียดทุกครั้งที่มีบางอย่างเกิดขึ้นในกราฟของคุณ รวมถึง:\n",
        "\n",
        "  - เหตุการณ์ `checkpoint` (จุดตรวจสอบ) ทุกครั้งที่มีการบันทึกจุดตรวจสอบใหม่ของสถานะปัจจุบันลงในฐานข้อมูล\n",
        "  - เหตุการณ์ `task` (งาน) ที่ส่งออกมาเมื่อโหนดกำลังจะเริ่มทำงาน\n",
        "  - เหตุการณ์ `task_result` (ผลลัพธ์ของงาน) ที่ส่งออกมาเมื่อโหนดทำงานเสร็จสิ้น\n",
        "\n",
        "- สุดท้าย คุณสามารถรวมโหมดเหล่านี้ได้ ตัวอย่างเช่น การขอทั้ง updates และ values โดยการส่งรายการ\n",
        "\n",
        "คุณควบคุมโหมดสตรีมด้วยอาร์กิวเมนต์ `stream_mode` ไปยัง `stream()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hrONNT3gJ8T"
      },
      "source": [
        "# Streaming LLM Output Token-by-Token\n",
        "\n",
        "**การสตรีมเอาต์พุต LLM แบบโทเค็นต่อโทเค็น**\n",
        "\n",
        "บางครั้งคุณอาจต้องการรับเอาต์พุตแบบสตรีมมิ่งจากการเรียก LLM แต่ละครั้งภายในแอปพลิเคชัน LLM ขนาดใหญ่ของคุณ นี่อาจมีประโยชน์ ตัวอย่างเช่น เมื่อสร้างแชทบอทแบบโต้ตอบ ซึ่งคุณต้องการให้แสดงแต่ละคำทันทีที่ LLM สร้างขึ้น"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS3myX3dgjOv",
        "outputId": "bd9c02be-3119-45a7-e356-987efe04ed8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 30th president of the United States was Calvin Coolidge, who served from 1923 to 1929. He was born on July 4, 1872, in Plymouth Notch, Vermont. Coolidge became president following the sudden death of Warren G. Harding, and he was later elected to a full term in 1924. His presidency is often associated with a period of economic prosperity known as the \"Roaring Twenties,\" characterized by significant growth in industry and consumerism.\n",
            "\n",
            "Calvin Coolidge passed away on January 5, 1933, at the age of 60. His death was attributed to a heart attack, which he suffered at his home in Northampton, Massachusetts. Coolidge's health had been declining for some time, and he had experienced a number of health issues in the years leading up to his death. Despite his relatively young age at the time of his passing, Coolidge had already made a significant impact on American politics and governance.\n",
            "\n",
            "Coolidge's presidency is often remembered for his belief in limited government and his commitment to fiscal conservatism. He famously stated, \"The business of America is business,\" reflecting his pro-business stance and the economic policies of the time. His administration focused on reducing taxes, minimizing government intervention in the economy, and promoting a laissez-faire approach to business. This philosophy contributed to the economic boom of the 1920s, but it also set the stage for the challenges that would arise during the Great Depression, which began shortly after he left office.\n",
            "\n",
            "In the years following his presidency, Coolidge largely withdrew from public life, choosing to focus on his family and personal interests. His death marked the end of an era in American politics, and he is often regarded as a symbol of the 1920s' economic optimism. Today, historians continue to debate his legacy, weighing his contributions to American prosperity against the economic challenges that followed his administration. Coolidge's life and presidency remain a significant part of American history, illustrating the complexities of leadership during a transformative period in the nation's development."
          ]
        }
      ],
      "source": [
        "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
        "\n",
        "app = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Or any other OpenAI model you prefer\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?, please write at least 300 words\")\n",
        "]\n",
        "\n",
        "# Initialize the callback handler\n",
        "handler = AsyncIteratorCallbackHandler()\n",
        "\n",
        "# Call the model with the handler, using messages directly\n",
        "output = app.astream_events(messages, version=\"v2\", callbacks=[handler])\n",
        "\n",
        "async def process_stream():\n",
        "    async for token in app.astream(messages):\n",
        "        print(token.content, end=\"\", flush=True)\n",
        "await process_stream()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkGRXU2jgIHD"
      },
      "source": [
        "โค้ดนี้แสดงวิธีการสตรีมเอาต์พุตจากโมเดลภาษาโดยใช้ฟังก์ชัน asynchronous ใน Python\n",
        "\n",
        "*   `async def process_stream():` บรรทัดนี้กำหนดฟังก์ชัน asynchronous ชื่อ `process_stream` ฟังก์ชัน asynchronous ช่วยให้คุณทำงานต่างๆ พร้อมกันได้โดยไม่บล็อกเธรดหลักของการประมวลผล ซึ่งมีประโยชน์สำหรับงานต่างๆ เช่น การสตรีมข้อมูล ซึ่งอาจใช้เวลา\n",
        "\n",
        "*   `async for token in app.astream(messages):` นี่คือหัวใจสำคัญของตรรกะการสตรีม\n",
        "\n",
        "    *   `app.astream(messages):` ส่วนนี้น่าจะเรียกใช้เมธอด asynchronous (`.astream()`) บนอ็อบเจ็กต์โมเดลภาษาของคุณ (`app`) เมธอดนี้เริ่มต้นสตรีมของโทเค็นที่สร้างโดยโมเดลเพื่อตอบสนองต่อข้อความที่ให้ไว้\n",
        "    *   `async for token in ...:` นี่คือลูป `for` แบบ asynchronous ที่วนซ้ำโทเค็นที่มาจากสตรีม โทเค็นแต่ละโทเค็นแสดงถึงข้อความส่วนหนึ่ง (คำหรือส่วนของคำ) ที่สร้างโดยโมเดล\n",
        "    *   `print(token.content, end=\"\", flush=True):` บรรทัดนี้พิมพ์เนื้อหาของแต่ละโทเค็นเมื่อมาถึง\n",
        "\n",
        "        *   `token.content:` ส่วนนี้เข้าถึงเนื้อหาข้อความจริงของโทเค็น\n",
        "        *   `end=\"\":` ส่วนนี้ป้องกันการพิมพ์อักขระขึ้นบรรทัดใหม่หลังโทเค็นแต่ละตัว เพื่อให้เอาต์พุตปรากฏเป็นสตรีมต่อเนื่อง\n",
        "        *   `flush=True:` ส่วนนี้ช่วยให้แน่ใจว่าเอาต์พุตจะแสดงบนคอนโซลทันที แม้ว่าบัฟเฟอร์เอาต์พุตจะไม่เต็ม สิ่งนี้สำคัญสำหรับการดูสตรีมแบบเรียลไทม์\n",
        "\n",
        "*   `await process_stream():` บรรทัดนี้เรียกใช้ฟังก์ชัน `process_stream` และรอให้เสร็จสมบูรณ์ เนื่องจาก `process_stream` เป็นฟังก์ชัน asynchronous การใช้ `await` ช่วยให้โปรแกรมทำงานอื่นๆ ต่อไปได้ในขณะที่กำลังประมวลผลสตรีม สิ่งสำคัญคือต้องใช้ `await` ที่นี่ มิฉะนั้นโปรแกรมอาจออกจากโปรแกรมก่อนที่สตรีมจะถูกใช้และพิมพ์จนหมด\n",
        "\n",
        "**สรุป:**\n",
        "\n",
        "โค้ดนี้ตั้งค่าสตรีมของเอาต์พุตข้อความจากโมเดลภาษาของคุณ (`app`) วนซ้ำโทเค็นแต่ละตัวในสตรีมขณะที่สร้างขึ้น และพิมพ์ไปยังคอนโซลทันที สร้างการแสดงผลแบบเรียลไทม์ของเอาต์พุตของโมเดล\n",
        "\n",
        "การใช้ `async` และ `await` ทำให้กระบวนการนี้ไม่บล็อก ช่วยให้โปรแกรมของคุณจัดการงานอื่นๆ ได้ในขณะที่รอให้สตรีมเสร็จสมบูรณ์ สิ่งนี้สำคัญสำหรับแอปพลิเคชันแบบโต้ตอบที่การตอบสนองมีความสำคัญ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdNzkbvhOwL"
      },
      "source": [
        "สิ่งนี้จะส่งออกแต่ละคำ (ในทางเทคนิคคือแต่ละโทเค็น) ทันทีที่ได้รับจาก LLM คุณสามารถดูรายละเอียดเพิ่มเติมเกี่ยวกับรูปแบบนี้ได้จาก [LangChain](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-nodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZOrTnzRhtAe"
      },
      "source": [
        "# Human-in-the-Loop Modalities\n",
        "**รูปแบบการมีส่วนร่วมของมนุษย์**\n",
        "\n",
        "ขณะที่เราก้าวขึ้นบันไดแห่งความเป็นอิสระ (หรือความสามารถในการดำเนินการด้วยตนเอง) เราพบว่าตัวเองต้องสูญเสียการควบคุม (หรือการกำกับดูแล) มากขึ้นเรื่อยๆ เพื่อแลกกับความสามารถ (หรือความเป็นอิสระ) รูปแบบสถานะที่ใช้ร่วมกันที่ใช้ใน LangGraph **(ดูบทที่ 5 สำหรับบทนำ)** ทำให้ง่ายต่อการสังเกต ขัดจังหวะ และแก้ไขแอปพลิเคชัน ทำให้สามารถใช้โหมด \"มนุษย์ในวงจร\" (human-in-the-loop) ที่หลากหลาย หรือวิธีการที่นักพัฒนา/ผู้ใช้ปลายทางของแอปพลิเคชันสามารถมีอิทธิพลต่อสิ่งที่ LLM กำลังทำอยู่\n",
        "\n",
        "สำหรับส่วนนี้ เราจะใช้อีกครั้งกับสถาปัตยกรรมสุดท้ายที่อธิบายไว้ใน **\"การจัดการกับเครื่องมือจำนวนมาก\" ในบทที่ 6** โปรดกลับไปดูบทนั้นสำหรับโค้ดแบบเต็ม สำหรับโหมดมนุษย์ในวงจรทั้งหมด เราต้องแนบตัวตรวจสอบจุด (checkpointer) เข้ากับกราฟก่อน โปรดดู **\"Adding Memory to StateGraph\" ในบทที่ 4**\n",
        "\n",
        "สำหรับรายละเอียดเพิ่มเติมเกี่ยวกับเรื่องนี้:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sFrnmON_gUMO"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = builder.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxr4o8fKii4o"
      },
      "source": [
        "**สิ่งนี้จะส่งคืนอินสแตนซ์ของกราฟที่จัดเก็บสถานะเมื่อสิ้นสุดแต่ละขั้นตอน**\n",
        "- ดังนั้นการเรียกใช้ทุกครั้งหลังจากครั้งแรกจะไม่เริ่มต้นจากศูนย์\n",
        "- ทุกครั้งที่กราฟถูกเรียกใช้ กราฟจะเริ่มต้นโดยใช้ตัวตรวจสอบจุด (checkpointer) เพื่อดึงสถานะที่บันทึกล่าสุด หากมีและรวมอินพุตใหม่กับสถานะก่อนหน้า\n",
        "- จากนั้นจึงดำเนินการโหนดแรก\n",
        "\n",
        "**นี่เป็นกุญแจสำคัญในการเปิดใช้งานรูปแบบการมีส่วนร่วมของมนุษย์ (human-in-the-loop modalities) ซึ่งทั้งหมดขึ้นอยู่กับการที่กราฟจดจำสถานะก่อนหน้า**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"https://github.com/Smith-WeStrideTH/Langchian_LLM/blob/main/work/pics/Figure8-3.png?raw=1\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4CUoEnxjL6q"
      },
      "source": [
        "โหมดแรกคือ \"การขัดจังหวะ\" `(interrupt)` เป็นรูปแบบการควบคุมที่ง่ายที่สุด ผู้ใช้กำลังดูเอาต์พุตแบบสตรีมมิ่งของแอปพลิเคชันขณะที่กำลังสร้าง และขัดจังหวะด้วยตนเองเมื่อเห็นสมควร (ดูรูป) สถานะจะถูกบันทึก ณ ขั้นตอนสุดท้ายก่อนที่ผู้ใช้จะกดปุ่มขัดจังหวะ จากตรงนั้น ผู้ใช้สามารถเลือกที่จะ:\n",
        "\n",
        "- ดำเนินการต่อจากจุดนั้นเป็นต้นไป และการคำนวณจะดำเนินต่อไปราวกับว่าไม่มีการขัดจังหวะ `(ดู \"Resume\")`\n",
        "- ส่งอินพุตใหม่ไปยังแอปพลิเคชัน (เช่น ข้อความใหม่ในแชทบอท) ซึ่งจะยกเลิกขั้นตอนในอนาคตที่ค้างอยู่และเริ่มจัดการกับอินพุตใหม่ `(ดู \"Restart\")`\n",
        "- ไม่ทำอะไรเลย และไม่มีอะไรอื่นจะทำงาน\n",
        "\n",
        "มาดูวิธีการทำเช่นนี้ใน LangGraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AnszMt0dicZj",
        "outputId": "f08cca93-1f3c-40d1-8f57-a4e9e650ced6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 1 executed: Imports done, Memory Stores initialized.\n"
          ]
        }
      ],
      "source": [
        "# for simulating state and history persistence.\n",
        "\n",
        "import asyncio\n",
        "import copy\n",
        "import time\n",
        "import uuid\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "# --- Simple In-Memory State Storage ---\n",
        "MEMORY_STATE = {}\n",
        "MEMORY_HISTORY = {}\n",
        "\n",
        "print(\"Cell 1 executed: Imports done, Memory Stores initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmWM7ahif58"
      },
      "source": [
        "---\n",
        "\n",
        "<img align=\"top\" src=\"https://github.com/Smith-WeStrideTH/Langchian_LLM/blob/main/work/pics/Figure8-4.png?raw=1\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRil62WCkZOB"
      },
      "source": [
        "โหมดการควบคุมที่สองคือ \"การอนุญาต\" `(authorize)` ซึ่งผู้ใช้กำหนดล่วงหน้าว่าต้องการให้แอปพลิเคชันส่งมอบการควบคุมให้พวกเขา ทุกครั้งที่โหนดเฉพาะกำลังจะถูกเรียก (ดูรูปนี้) โดยทั่วไปจะใช้สำหรับการยืนยันเครื่องมือ ก่อนที่จะเรียกเครื่องมือใดๆ (หรือเครื่องมือเฉพาะ) แอปพลิเคชันจะหยุดชั่วคราวและขอการยืนยัน ซึ่ง ณ จุดนั้น ผู้ใช้สามารถทำได้อีกครั้ง:\n",
        "\n",
        "- `Resume`ดำเนินการคำนวณต่อ โดยยอมรับการเรียกเครื่องมือ\n",
        "- ส่งข้อความใหม่เพื่อนำบอทไปในทิศทางที่แตกต่างกัน ซึ่งในกรณีนี้เครื่องมือจะไม่ถูกเรียก\n",
        "- ไม่ทำอะไรเลย"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- State Management Functions ---\n",
        "\n",
        "def load_state(thread_id):\n",
        "    \"\"\"Loads the current state for a thread_id from memory.\"\"\"\n",
        "    # Return a copy to prevent accidental modification of the stored state\n",
        "    # Default to an empty state if the thread_id is new\n",
        "    return copy.deepcopy(MEMORY_STATE.get(thread_id, {\"messages\": [], \"step\": 0, \"status\": \"new\"}))\n",
        "\n",
        "def save_state(thread_id, state):\n",
        "    \"\"\"Saves the current state and adds it to the history.\"\"\"\n",
        "    current_state = copy.deepcopy(state)\n",
        "    MEMORY_STATE[thread_id] = current_state\n",
        "\n",
        "    # Initialize history list if it doesn't exist\n",
        "    if thread_id not in MEMORY_HISTORY:\n",
        "        MEMORY_HISTORY[thread_id] = []\n",
        "\n",
        "    # Add a copy of the state to the history\n",
        "    MEMORY_HISTORY[thread_id].append(current_state)\n",
        "\n",
        "def get_state(config):\n",
        "    \"\"\"Convenience function to get state using a config dict.\"\"\"\n",
        "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    print(f\"\\n[Functions] Getting state for {thread_id}\")\n",
        "    return load_state(thread_id)\n",
        "\n",
        "def update_state(config, updates):\n",
        "    \"\"\"Convenience function to update state using a config dict.\"\"\"\n",
        "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    print(f\"\\n[Functions] Updating state for {thread_id} with: {updates}\")\n",
        "    state = load_state(thread_id)\n",
        "    # Merge the updates into the state\n",
        "    state.update(updates)\n",
        "    state[\"messages\"].append(\"(State manually updated)\") # Add a note\n",
        "    save_state(thread_id, state) # Save the modified state\n",
        "    return state\n",
        "\n",
        "def get_history(config):\n",
        "     \"\"\"Convenience function to get history using a config dict.\"\"\"\n",
        "     thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "     print(f\"\\n[Functions] Getting history for {thread_id}\")\n",
        "     # Return a copy of the history list\n",
        "     return copy.deepcopy(MEMORY_HISTORY.get(thread_id, []))\n",
        "\n",
        "# --- Simulated Process ---\n",
        "\n",
        "def run_simple_flow(config, input_message=None, interrupt_at_step=None):\n",
        "    \"\"\"\n",
        "    Simulates running a few steps of a process for a given thread_id.\n",
        "    Updates state in MEMORY_STATE and MEMORY_HISTORY.\n",
        "    Can be interrupted at a specific step number.\n",
        "    \"\"\"\n",
        "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    print(f\"\\n[Functions] Running flow for {thread_id} (Interrupt @ step {interrupt_at_step})\")\n",
        "\n",
        "    state = load_state(thread_id)\n",
        "    state[\"status\"] = \"running\"\n",
        "\n",
        "    # --- Step 0: Process Input ---\n",
        "    if input_message and state[\"step\"] == 0: # Only add input on first run\n",
        "        state[\"messages\"].append(f\"Human: {input_message}\")\n",
        "        state[\"step\"] += 1\n",
        "        print(f\"  Step {state['step']}: Processed input.\")\n",
        "        save_state(thread_id, state) # Save after input processing\n",
        "        if interrupt_at_step == state[\"step\"]:\n",
        "            print(f\"  INTERRUPTING after step {state['step']}\")\n",
        "            state[\"status\"] = \"paused\"\n",
        "            save_state(thread_id, state)\n",
        "            return state\n",
        "\n",
        "    # --- Step 1: Thinking ---\n",
        "    if state[\"step\"] < 1:\n",
        "        state[\"messages\"].append(\"AI: Thinking...\")\n",
        "        state[\"step\"] += 1\n",
        "        print(f\"  Step {state['step']}: AI is thinking.\")\n",
        "        save_state(thread_id, state) # Save after each step\n",
        "        if interrupt_at_step == state[\"step\"]:\n",
        "            print(f\"  INTERRUPTING after step {state['step']}\")\n",
        "            state[\"status\"] = \"paused\"\n",
        "            save_state(thread_id, state)\n",
        "            return state # Stop processing\n",
        "\n",
        "    # --- Step 2: Doing something ---\n",
        "    if state[\"step\"] < 2:\n",
        "        state[\"messages\"].append(\"AI: Performing Action X...\")\n",
        "        state[\"step\"] += 1\n",
        "        print(f\"  Step {state['step']}: AI performing action.\")\n",
        "        save_state(thread_id, state)\n",
        "        if interrupt_at_step == state[\"step\"]:\n",
        "            print(f\"  INTERRUPTING after step {state['step']}\")\n",
        "            state[\"status\"] = \"paused\"\n",
        "            save_state(thread_id, state)\n",
        "            return state # Stop processing\n",
        "\n",
        "    # --- Step 3: Finishing ---\n",
        "    if state[\"step\"] < 3:\n",
        "        state[\"messages\"].append(\"AI: Finishing up.\")\n",
        "        state[\"step\"] += 1\n",
        "        print(f\"  Step {state['step']}: AI finishing.\")\n",
        "        state[\"status\"] = \"finished\"\n",
        "        save_state(thread_id, state) # Save final state\n",
        "\n",
        "    print(f\"[Functions] Flow finished for {thread_id}. Final Status: {state['status']}\")\n",
        "    return state\n",
        "\n",
        "print(\"Cell 2 executed: Core functions defined.\")"
      ],
      "metadata": {
        "id": "H3iUlIe7sK-c",
        "outputId": "420eece1-1828-48b5-dcb0-dfdc79873d48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 2 executed: Core functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6FIWIgk5Pg"
      },
      "source": [
        "สิ่งนี้จะรันกราฟจนกว่าจะถึงจุดที่กำลังจะเข้าสู่โหนดที่เรียกว่า `tools` ซึ่งจะให้โอกาสคุณในการตรวจสอบสถานะปัจจุบัน และตัดสินใจว่าจะดำเนินการต่อหรือไม่ สังเกตว่า `interrupt_before` เป็นรายการที่ลำดับไม่สำคัญ หากคุณส่งชื่อโหนดหลายชื่อ มันจะขัดจังหวะก่อนที่จะเข้าสู่แต่ละชื่อ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epBBBs8xe5gf"
      },
      "source": [
        "## Resume\n",
        "**การดำเนินการต่อ**\n",
        "\n",
        "ในการดำเนินการต่อจากกราฟที่ถูกขัดจังหวะ เช่น เมื่อใช้หนึ่งในสองรูปแบบก่อนหน้านี้ คุณเพียงแค่ต้องเรียกกราฟอีกครั้งด้วยอินพุต null (หรือ None ใน Python) สิ่งนี้ถูกใช้เป็นสัญญาณเพื่อดำเนินการประมวลผลอินพุตที่ไม่ใช่ null ก่อนหน้าต่อไป:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ASSapgUpkqZQ",
        "outputId": "5d4aedc1-4300-4371-8dcb-12c1cf6d91b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 1. Resume Example ===\n",
            "\n",
            "Step A: Run flow and interrupt at step 2\n",
            "\n",
            "[Functions] Running flow for thread-1d932a (Interrupt @ step 2)\n",
            "  Step 1: Processed input.\n",
            "  Step 2: AI performing action.\n",
            "  INTERRUPTING after step 2\n",
            "\n",
            "State after interrupt for thread-1d932a:\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...'], 'step': 2, 'status': 'paused'}\n",
            "\n",
            "Step B: Resume the flow\n",
            "(Human says: OK, continue!)\n",
            "\n",
            "[Functions] Running flow for thread-1d932a (Interrupt @ step None)\n",
            "  Step 3: AI finishing.\n",
            "[Functions] Flow finished for thread-1d932a. Final Status: finished\n",
            "\n",
            "Final state after resume for thread-1d932a:\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.'], 'step': 3, 'status': 'finished'}\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 1. Resume Example\n",
        "# ========================================\n",
        "print(\"\\n\\n=== 1. Resume Example ===\")\n",
        "# Ensure a unique thread_id for this example run\n",
        "resume_thread_id = f\"thread-{uuid.uuid4().hex[:6]}\"\n",
        "config_resume = {\"configurable\": {\"thread_id\": resume_thread_id}}\n",
        "input_message_resume = \"Please start the process.\"\n",
        "\n",
        "print(\"\\nStep A: Run flow and interrupt at step 2\")\n",
        "# Run the flow, telling it to stop after completing step 2\n",
        "state_after_interrupt = run_simple_flow(config_resume, input_message_resume, interrupt_at_step=2)\n",
        "\n",
        "print(f\"\\nState after interrupt for {resume_thread_id}:\")\n",
        "print(state_after_interrupt)\n",
        "assert state_after_interrupt.get(\"status\") == \"paused\" # Verify it paused\n",
        "\n",
        "print(\"\\nStep B: Resume the flow\")\n",
        "print(\"(Human says: OK, continue!)\")\n",
        "# Run the flow again with the *same config* and *no input message*.\n",
        "# It will load the 'paused' state and continue from step 3.\n",
        "state_after_resume = run_simple_flow(config_resume) # No input, no interrupt specified\n",
        "\n",
        "print(f\"\\nFinal state after resume for {resume_thread_id}:\")\n",
        "print(state_after_resume)\n",
        "assert state_after_resume.get(\"status\") == \"finished\" # Verify it finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-obFVK96jAQm"
      },
      "source": [
        "## Restart\n",
        "**การเริ่มต้นใหม่**\n",
        "\n",
        "หากคุณต้องการให้กราฟที่ถูกขัดจังหวะเริ่มต้นใหม่จากโหนดแรก โดยมีอินพุตใหม่เพิ่มเติม คุณเพียงแค่ต้องเรียกใช้กราฟด้วยอินพุตใหม่:\n",
        "\n",
        "**คำอธิบายเพิ่มเติม:**\n",
        "\n",
        "*   **การเริ่มต้นใหม่ (Restart):** การเริ่มกระบวนการทำงานของกราฟใหม่ตั้งแต่ต้น โดยไม่ใช้สถานะที่บันทึกไว้ก่อนหน้านี้\n",
        "\n",
        "กล่าวคือ หากกราฟถูกขัดจังหวะและคุณต้องการให้มันเริ่มทำงานใหม่โดยใช้อินพุตใหม่ คุณไม่จำเป็นต้องทำอะไรเป็นพิเศษนอกจากการเรียกใช้ฟังก์ชันกราฟอีกครั้งด้วยอินพุตใหม่นั้นเอง กราฟจะเริ่มต้นการประมวลผลใหม่ตั้งแต่ต้นโดยใช้อินพุตใหม่นี้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sqeqCI1pi7U4",
        "outputId": "e3665b91-9230-4a81-9121-08c0bc0c9752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 2. Restart Example ===\n",
            "\n",
            "Starting a new flow with thread_id: thread-0375f8\n",
            "\n",
            "[Functions] Running flow for thread-0375f8 (Interrupt @ step None)\n",
            "  Step 1: Processed input.\n",
            "  Step 2: AI performing action.\n",
            "  Step 3: AI finishing.\n",
            "[Functions] Flow finished for thread-0375f8. Final Status: finished\n",
            "\n",
            "Final state for new flow (thread-0375f8):\n",
            "{'messages': ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.'], 'step': 3, 'status': 'finished'}\n",
            "\n",
            "Checking state of original thread (thread-1d932a)...\n",
            "\n",
            "[Functions] Getting state for thread-1d932a\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.'], 'step': 3, 'status': 'finished'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================================\n",
        "# 2. Restart Example\n",
        "# ========================================\n",
        "print(\"\\n\\n=== 2. Restart Example ===\")\n",
        "# Use a *new* unique thread_id to signify a fresh start\n",
        "restart_thread_id = f\"thread-{uuid.uuid4().hex[:6]}\"\n",
        "config_restart = {\"configurable\": {\"thread_id\": restart_thread_id}}\n",
        "input_message_restart = \"Start a new different process.\"\n",
        "\n",
        "print(f\"\\nStarting a new flow with thread_id: {restart_thread_id}\")\n",
        "# Run the flow from the beginning for this new thread\n",
        "final_state_restart = run_simple_flow(config_restart, input_message_restart)\n",
        "\n",
        "print(f\"\\nFinal state for new flow ({restart_thread_id}):\")\n",
        "print(final_state_restart)\n",
        "\n",
        "# Verify the original thread from Cell 3 still exists\n",
        "print(f\"\\nChecking state of original thread ({resume_thread_id})...\")\n",
        "original_state = get_state(config_resume)\n",
        "print(original_state)\n",
        "assert original_state.get(\"status\") == \"finished\" # It should still be finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CpKURrwjdMS"
      },
      "source": [
        "สิ่งนี้จะรักษาสถานะปัจจุบันของกราฟ รวมเข้ากับอินพุตใหม่ และเริ่มต้นใหม่อีกครั้งจากโหนดแรก\n",
        "\n",
        "หากคุณต้องการลบสถานะปัจจุบัน เพียงแค่เปลี่ยน `thread_id` ซึ่งจะเริ่มต้นการโต้ตอบใหม่จากศูนย์ ค่าสตริงใดๆ ก็เป็น `thread_id` ที่ถูกต้อง เราขอแนะนำให้ใช้ UUID (หรือตัวระบุเฉพาะอื่นๆ) เป็น thread ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdF50FTIjldK"
      },
      "source": [
        "## Edit state\n",
        "\n",
        "**การแก้ไขสถานะ**\n",
        "\n",
        "บางครั้งคุณอาจต้องการอัปเดตสถานะของกราฟก่อนที่จะดำเนินการต่อ ซึ่งสามารถทำได้ด้วยเมธอด `update_state` โดยปกติแล้วคุณจะต้องตรวจสอบสถานะปัจจุบันก่อนด้วย `get_state`\n",
        "\n",
        "ลองมาดูว่ามีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2MtvIdPjTEk",
        "outputId": "8047c4fc-960a-4eaa-f22f-f5935e3a0258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 3. Edit State Example ===\n",
            "\n",
            "Current state for thread-0375f8 before edit:\n",
            "\n",
            "[Functions] Getting state for thread-0375f8\n",
            "{'messages': ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.'], 'step': 3, 'status': 'finished'}\n",
            "\n",
            "Simulating Human Edit: Adding a note to messages and a custom flag.\n",
            "\n",
            "[Functions] Updating state for thread-0375f8 with: {'messages': ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.', 'Human Note: Priority High!'], 'priority_flag': True}\n",
            "\n",
            "State for thread-0375f8 AFTER edit:\n",
            "{'messages': ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.', 'Human Note: Priority High!', '(State manually updated)'], 'step': 3, 'status': 'finished', 'priority_flag': True}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================================\n",
        "# 3. Edit State Example\n",
        "# ========================================\n",
        "print(\"\\n\\n=== 3. Edit State Example ===\")\n",
        "# Use the config from the 'restart' example in Cell 4\n",
        "config_edit = config_restart # Assuming Cell 4 ran and defined this\n",
        "edit_thread_id = config_edit[\"configurable\"][\"thread_id\"]\n",
        "\n",
        "print(f\"\\nCurrent state for {edit_thread_id} before edit:\")\n",
        "current_state = get_state(config_edit)\n",
        "print(current_state)\n",
        "\n",
        "print(\"\\nSimulating Human Edit: Adding a note to messages and a custom flag.\")\n",
        "# Prepare the dictionary of things you want to change or add\n",
        "updates_to_make = {\n",
        "    # Append to the existing messages list\n",
        "    \"messages\": current_state.get(\"messages\", []) + [\"Human Note: Priority High!\"],\n",
        "    \"priority_flag\": True # Add a new key/value\n",
        "}\n",
        "\n",
        "# Apply the updates\n",
        "edited_state = update_state(config_edit, updates_to_make)\n",
        "\n",
        "print(f\"\\nState for {edit_thread_id} AFTER edit:\")\n",
        "print(edited_state)\n",
        "assert edited_state.get(\"priority_flag\") is True # Verify edit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjtsSduSj1XE"
      },
      "source": [
        "สิ่งนี้จะสร้างจุดตรวจสอบใหม่ที่มีการอัปเดตของคุณ ดังนั้นหลังจากนี้ คุณก็พร้อมที่จะดำเนินการกราฟต่อจากจุดใหม่นี้ ดูที่ `Resume` เพื่อดูวิธีการ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orstCS2bkCW2"
      },
      "source": [
        "## Fork\n",
        "**การแตกแขนง**\n",
        "\n",
        "คุณยังสามารถเรียกดูประวัติของสถานะทั้งหมดที่กราฟเคยผ่านมา และสามารถเยี่ยมชมสถานะใดก็ได้อีกครั้ง เช่น เพื่อรับคำตอบอื่น ซึ่งมีประโยชน์มากในแอปพลิเคชันที่สร้างสรรค์มากขึ้น ซึ่งคาดว่าการรันกราฟแต่ละครั้งจะสร้างเอาต์พุตที่แตกต่างกัน\n",
        "\n",
        "มาดูกันว่ามันมีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eS5O1_BJjsO3",
        "outputId": "74b5f416-fd1e-4518-aabc-1d6bb81fc781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 4. Fork Example ===\n",
            "\n",
            "Getting history for base thread: thread-1d932a\n",
            "\n",
            "[Functions] Getting history for thread-1d932a\n",
            "\n",
            "Found state to fork from at index 2 (step 2, status paused):\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...'], 'step': 2, 'status': 'paused'}\n",
            "Creating fork with new thread_id: fork-1e3a0c\n",
            "\n",
            "Initial state of forked thread fork-1e3a0c:\n",
            "\n",
            "[Functions] Getting state for fork-1e3a0c\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...', '(Forked from thread-1d932a at step 2)'], 'step': 2, 'status': 'running_from_fork'}\n",
            "\n",
            "Step C: Running the flow on the FORKED branch\n",
            "\n",
            "[Functions] Running flow for fork-1e3a0c (Interrupt @ step None)\n",
            "  Step 3: AI finishing.\n",
            "[Functions] Flow finished for fork-1e3a0c. Final Status: finished\n",
            "\n",
            "Final state of forked thread fork-1e3a0c:\n",
            "{'messages': ['Human: Please start the process.', 'AI: Performing Action X...', '(Forked from thread-1d932a at step 2)', 'AI: Finishing up.'], 'step': 3, 'status': 'finished'}\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 4. Fork Example\n",
        "# ========================================\n",
        "print(\"\\n\\n=== 4. Fork Example ===\")\n",
        "# Use the config from the 'resume' example in Cell 3 as the base\n",
        "config_fork_base = config_resume # Assuming Cell 3 ran and defined this\n",
        "base_thread_id = config_fork_base[\"configurable\"][\"thread_id\"]\n",
        "\n",
        "print(f\"\\nGetting history for base thread: {base_thread_id}\")\n",
        "history = get_history(config_fork_base)\n",
        "\n",
        "# Find the state where the status was 'paused' (or just take an earlier state)\n",
        "state_to_fork_from = None\n",
        "fork_point_index = -1\n",
        "for i, state in enumerate(reversed(history)): # Search backwards\n",
        "    if state.get(\"status\") == \"paused\":\n",
        "        state_to_fork_from = state\n",
        "        fork_point_index = len(history) - 1 - i # Original index\n",
        "        break\n",
        "\n",
        "# Fallback: If no 'paused' state found (maybe history saving issue), fork from state before last\n",
        "if not state_to_fork_from and len(history) >= 2:\n",
        "     state_to_fork_from = history[-2]\n",
        "     fork_point_index = len(history) - 2\n",
        "     print(\"Warning: 'paused' state not found in history, forking from second-to-last state.\")\n",
        "elif not state_to_fork_from:\n",
        "     print(\"Error: Not enough history found to fork.\")\n",
        "     # Skip the rest of the fork logic if no suitable state\n",
        "     state_to_fork_from = None\n",
        "\n",
        "\n",
        "if state_to_fork_from:\n",
        "    print(f\"\\nFound state to fork from at index {fork_point_index} (step {state_to_fork_from.get('step')}, status {state_to_fork_from.get('status')}):\")\n",
        "    print(state_to_fork_from)\n",
        "\n",
        "    # 1. Create a new thread ID for the fork\n",
        "    forked_thread_id = f\"fork-{uuid.uuid4().hex[:6]}\"\n",
        "    config_fork_branch = {\"configurable\": {\"thread_id\": forked_thread_id}}\n",
        "    print(f\"Creating fork with new thread_id: {forked_thread_id}\")\n",
        "\n",
        "    # 2. Manually \"plant\" the chosen historical state for the new thread\n",
        "    # Make a deep copy to avoid modifying the original history\n",
        "    forked_state = copy.deepcopy(state_to_fork_from)\n",
        "    forked_state[\"messages\"].append(f\"(Forked from {base_thread_id} at step {forked_state['step']})\")\n",
        "    # Important: Set status so the flow can run\n",
        "    forked_state[\"status\"] = \"running_from_fork\"\n",
        "\n",
        "    # Directly save this copied state as the *current* state for the new thread\n",
        "    save_state(forked_thread_id, forked_state)\n",
        "\n",
        "    print(f\"\\nInitial state of forked thread {forked_thread_id}:\")\n",
        "    print(get_state(config_fork_branch)) # Verify the planted state\n",
        "\n",
        "    print(\"\\nStep C: Running the flow on the FORKED branch\")\n",
        "    # Run the flow for the new thread config. It will load the state we just planted.\n",
        "    # No input message needed as we're continuing. No interrupt needed.\n",
        "    final_forked_state = run_simple_flow(config_fork_branch)\n",
        "\n",
        "    print(f\"\\nFinal state of forked thread {forked_thread_id}:\")\n",
        "    print(final_forked_state)\n",
        "    assert final_forked_state.get(\"status\") == \"finished\" # It should finish"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect"
      ],
      "metadata": {
        "id": "SdqYwqVrw1me"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "32Wr2cGnkn71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "19ed2ae9-210a-4669-adbe-7572a8f1b2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 5. Inspect Final Memory Stores ===\n",
            "\n",
            "--- Final MEMORY_STATE ---\n",
            "\n",
            "Thread ID: thread-0b222c\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.']\n",
            "\n",
            "Thread ID: thread-2e49a1\n",
            "  Status: paused\n",
            "  Step: 2\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...']\n",
            "\n",
            "Thread ID: thread-8184e7\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.']\n",
            "\n",
            "Thread ID: thread-e52ec8\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.']\n",
            "\n",
            "Thread ID: thread-1d932a\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...', 'AI: Finishing up.']\n",
            "\n",
            "Thread ID: thread-7f3d86\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.']\n",
            "\n",
            "Thread ID: thread-0375f8\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Start a new different process.', 'AI: Performing Action X...', 'AI: Finishing up.', 'Human Note: Priority High!', '(State manually updated)']\n",
            "  Priority: True\n",
            "\n",
            "Thread ID: fork-1e3a0c\n",
            "  Status: finished\n",
            "  Step: 3\n",
            "  Messages: ['Human: Please start the process.', 'AI: Performing Action X...', '(Forked from thread-1d932a at step 2)', 'AI: Finishing up.']\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 5. Inspect Final Memory Stores\n",
        "# ========================================\n",
        "print(\"\\n\\n=== 5. Inspect Final Memory Stores ===\")\n",
        "\n",
        "print(\"\\n--- Final MEMORY_STATE ---\")\n",
        "if not MEMORY_STATE:\n",
        "    print(\"  (empty)\")\n",
        "else:\n",
        "    for thread_id, state in MEMORY_STATE.items():\n",
        "        print(f\"\\nThread ID: {thread_id}\")\n",
        "        print(f\"  Status: {state.get('status')}\")\n",
        "        print(f\"  Step: {state.get('step')}\")\n",
        "        print(f\"  Messages: {state.get('messages')}\")\n",
        "        if \"priority_flag\" in state: print(f\"  Priority: {state.get('priority_flag')}\")\n",
        "\n",
        "# Optionally print history details\n",
        "# print(\"\\n--- Final MEMORY_HISTORY ---\")\n",
        "# print(MEMORY_HISTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3cD2iaukUM_"
      },
      "source": [
        "สังเกตว่าเราเก็บประวัติไว้ในรายการ/อาร์เรย์ในทั้งสองภาษาเพราะ `get_state_history` ส่งคืน iterator ของสถานะ (เพื่อให้สามารถใช้งานแบบ lazy  หรือ `การประมวลผลหรือดึงข้อมูลเฉพาะเมื่อจำเป็นเท่านั้น` ได้) สถานะที่ส่งคืนจากเมธอด history จะถูกเรียงลำดับโดยสถานะล่าสุดอยู่ก่อนและสถานะที่เก่าที่สุดอยู่หลังสุด\n",
        "\n",
        "พลังที่แท้จริงของการควบคุมแบบ human-in-the-loop มาจากการผสมผสานพวกมันเข้าด้วยกันในแบบที่เหมาะสมกับแอปพลิเคชันของคุณ\n",
        "\n",
        "**คำอธิบายเพิ่มเติมเพื่อความเข้าใจ:**\n",
        "\n",
        "`get_state_history` คืนค่า iterator แทนที่จะเป็น list โดยตรง เพื่อประสิทธิภาพ หากประวัติมีขนาดใหญ่มาก การสร้าง list ทั้งหมดในหน่วยความจำอาจใช้ทรัพยากรมาก การใช้ iterator ช่วยให้เราเข้าถึงสถานะทีละรายการเมื่อเราต้องการเท่านั้น\n",
        "\n",
        "การเรียงลำดับสถานะจากล่าสุดไปเก่าสุดช่วยให้เข้าถึงสถานะปัจจุบันได้อย่างรวดเร็วโดยไม่ต้องวนซ้ำทั้งหมด\n",
        "\n",
        "ข้อความสุดท้ายเน้นย้ำถึงความยืดหยุ่นของการควบคุมแบบ human-in-the-loop โดยอนุญาตให้นักพัฒนาผสมผสานวิธีการควบคุมต่างๆ (เช่น การขัดจังหวะและการอนุญาต) เพื่อให้เหมาะสมกับความต้องการเฉพาะของแอปพลิเคชัน\n",
        "\n",
        "**สรุป:**\n",
        "\n",
        "โค้ดนี้จัดการประวัติสถานะโดยใช้ iterator เพื่อประสิทธิภาพ และเน้นว่าการควบคุมแบบ human-in-the-loop สามารถปรับแต่งได้โดยการรวมวิธีการควบคุมต่างๆ เข้าด้วยกัน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF1DXHDqlHPa"
      },
      "source": [
        "# Multitasking LLMs\n",
        "\n",
        "**การจัดการ LLM แบบ Multitasking**\n",
        "\n",
        "ส่วนนี้ครอบคลุมปัญหาการจัดการอินพุตพร้อมกันสำหรับแอปพลิเคชัน LLM นี่เป็นปัญหาที่เกี่ยวข้องอย่างยิ่ง เนื่องจาก LLM ค่อนข้างช้า โดยเฉพาะอย่างยิ่งเมื่อสร้างเอาต์พุตยาวหรือเมื่อเชื่อมโยงในสถาปัตยกรรมหลายขั้นตอน (เช่นที่คุณทำได้กับ LangGraph) แม้ว่า LLM จะเร็วขึ้น แต่สิ่งนี้จะยังคงเป็นความท้าทาย เนื่องจากจะเปิดประตูสำหรับกรณีการใช้งานที่ซับซ้อนมากขึ้นเรื่อยๆ ในลักษณะเดียวกับที่แม้แต่คนที่ทำงานได้มีประสิทธิภาพมากที่สุดก็ยังคงต้องเผชิญกับความจำเป็นในการจัดลำดับความสำคัญของความต้องการที่แข่งขันกันในเวลาของพวกเขา\n",
        "\n",
        "มาดูตัวเลือกต่างๆ กัน\n",
        "\n",
        "- **Refuse concurrent inputs(ปฏิเสธอินพุตพร้อมกัน)**\n",
        "\n",
        "อินพุตใดๆ ที่ได้รับขณะประมวลผลอินพุตก่อนหน้าจะถูกปฏิเสธ นี่เป็นกลยุทธ์ที่ง่ายที่สุด แต่ไม่น่าจะครอบคลุมทุกความต้องการ เนื่องจากหมายถึงการส่งมอบการจัดการ concurrency ให้กับผู้เรียก\n",
        "\n",
        "- **Handle independently (จัดการอย่างอิสระ)**\n",
        "\n",
        "อีกทางเลือกหนึ่งที่ง่ายคือการจัดการอินพุตใหม่ใดๆ เป็นการเรียกใช้แบบอิสระ สร้างเธรดใหม่ (เตือนความจำ: นี่คือคอนเทนเนอร์สำหรับจดจำสถานะ) และสร้างเอาต์พุตในบริบทนั้น สิ่งนี้มีข้อเสียที่ชัดเจนคือต้องแสดงให้ผู้ใช้เห็นเป็นการเรียกใช้สองครั้งที่แยกจากกันและไม่สามารถประนีประนอมได้ ซึ่งไม่สามารถทำได้หรือเป็นที่ต้องการเสมอไป ในทางกลับกัน มันมีข้อดีคือการปรับขนาดให้มีขนาดใหญ่เท่าใดก็ได้ และเป็นสิ่งที่คุณจะใช้ในแอปพลิเคชันของคุณในระดับหนึ่งอย่างแน่นอน ตัวอย่างเช่น นี่คือวิธีที่คุณจะคิดเกี่ยวกับปัญหาการทำให้แชทบอท \"แชท\" กับผู้ใช้สองคนพร้อมกัน\n",
        "\n",
        "- **Queue concurrent inputs (จัดคิวอินพุตพร้อมกัน)**\n",
        "\n",
        "อินพุตใดๆ ที่ได้รับขณะประมวลผลอินพุตก่อนหน้าจะถูกจัดคิวและจัดการเมื่ออินพุตปัจจุบันเสร็จสิ้น กลยุทธ์นี้มีข้อดีบางประการ:\n",
        "\n",
        "*   รองรับการรับคำขอพร้อมกันจำนวนเท่าใดก็ได้\n",
        "*   เนื่องจากเรารอให้อินพุตปัจจุบันประมวลผลเสร็จ จึงไม่สำคัญว่าอินพุตใหม่จะมาถึงทันทีหลังจากที่เราเริ่มจัดการอินพุตปัจจุบันหรือทันทีก่อนที่เราจะประมวลผลเสร็จ ผลลัพธ์สุดท้ายจะเหมือนกัน เนื่องจากเราจะประมวลผลอินพุตปัจจุบันให้เสร็จก่อนที่จะไปยังอินพุตถัดไป\n",
        "\n",
        "กลยุทธ์นี้มีข้อเสียบางประการเช่นกัน:\n",
        "\n",
        "*   อาจใช้เวลาสักครู่ในการประมวลผลอินพุตที่อยู่ในคิวทั้งหมด ในความเป็นจริง คิวอาจเติบโตอย่างไม่สิ้นสุดหากอินพุตถูกสร้างขึ้นในอัตราที่เร็วกว่าการประมวลผล\n",
        "*   อินพุตอาจล้าสมัยเมื่อถึงเวลาที่ได้รับการประมวลผล เนื่องจากอยู่ในคิวก่อนที่จะเห็นการตอบสนองต่ออินพุตก่อนหน้า และไม่เปลี่ยนแปลงหลังจากนั้น กลยุทธ์นี้ไม่เหมาะสมเมื่ออินพุตใหม่ขึ้นอยู่กับการตอบกลับก่อนหน้า\n",
        "\n",
        "- **Interrupt (ขัดจังหวะ)**\n",
        "\n",
        "เมื่อได้รับอินพุตใหม่ในขณะที่กำลังประมวลผลอินพุตอื่น ให้ละทิ้งการประมวลผลอินพุตปัจจุบันและเริ่มต้นเชนใหม่ด้วยอินพุตใหม่ กลยุทธ์นี้อาจแตกต่างกันไปตามสิ่งที่เก็บไว้ของการรันที่ถูกขัดจังหวะ นี่คือตัวเลือกบางส่วน:\n",
        "\n",
        "*   ไม่เก็บอะไรเลย อินพุตก่อนหน้าจะถูกลืมไปโดยสมบูรณ์ ราวกับว่าไม่เคยถูกส่งหรือประมวลผล\n",
        "*   เก็บขั้นตอนสุดท้ายที่เสร็จสมบูรณ์ ในแอปที่ใช้ checkpointing (ซึ่งจัดเก็บความคืบหน้าขณะที่เคลื่อนผ่านการคำนวณ) ให้เก็บสถานะที่สร้างโดยขั้นตอนสุดท้ายที่เสร็จสมบูรณ์ ละทิ้งการอัปเดตสถานะที่ค้างอยู่จากขั้นตอนที่กำลังดำเนินการ และเริ่มจัดการอินพุตใหม่ในบริบทนั้น\n",
        "*   เก็บขั้นตอนที่กำลังดำเนินการไว้ด้วย พยายามขัดจังหวะขั้นตอนปัจจุบันในขณะที่ดูแลเพื่อบันทึกการอัปเดตสถานะที่ไม่สมบูรณ์ที่กำลังสร้างขึ้นในขณะนั้น สิ่งนี้ไม่น่าจะใช้ได้ทั่วไปนอกเหนือจากสถาปัตยกรรมที่ง่ายที่สุด\n",
        "*   รอให้โหนดปัจจุบัน (แต่ไม่ใช่โหนดถัดไป) ประมวลผลเสร็จ จากนั้นบันทึกและขัดจังหวะ\n",
        "\n",
        "ตัวเลือกนี้มีข้อดีบางประการเมื่อเทียบกับการจัดคิวอินพุตพร้อมกัน:\n",
        "\n",
        "*   อินพุตใหม่จะได้รับการจัดการโดยเร็วที่สุด ลดโอกาสในการสร้างเอาต์พุตที่ล้าสมัยและความหน่วงในการสร้าง\n",
        "*   สำหรับตัวแปร \"ไม่เก็บอะไรเลย\" เอาต์พุตสุดท้ายจะไม่ขึ้นอยู่กับเวลาที่ได้รับอินพุตใหม่\n",
        "\n",
        "แต่ก็มีข้อเสียเช่นกัน:\n",
        "\n",
        "*   ในทางปฏิบัติ กลยุทธ์นี้ยังคงจำกัดอยู่เพียงการประมวลผลอินพุตทีละรายการ เนื่องจากอินพุตเก่าใดๆ จะถูกละทิ้งเมื่อได้รับอินพุตใหม่\n",
        "*   การเก็บการอัปเดตสถานะบางส่วนสำหรับการรันครั้งต่อไปต้องออกแบบสถานะโดยคำนึงถึงสิ่งนั้น หากไม่ใช่เช่นนั้น แอปพลิเคชันของคุณอาจจบลงในสถานะที่ไม่ถูกต้อง ตัวอย่างเช่น โมเดลแชท OpenAI กำหนดให้ข้อความ AI ที่ขอการเรียกเครื่องมือต้องตามด้วยข้อความเครื่องมือพร้อมเอาต์พุตของเครื่องมือทันที หากการรันของคุณถูกขัดจังหวะระหว่างนั้น คุณต้องล้างสถานะกลางอย่างระมัดระวังหรือเสี่ยงต่อการไม่สามารถดำเนินการต่อไปได้\n",
        "*   เอาต์พุตสุดท้ายที่สร้างขึ้นมีความไวต่อเวลาที่ได้รับอินพุตใหม่ เนื่องจากอินพุตใหม่จะได้รับการจัดการในบริบทของความคืบหน้า (ที่ไม่สมบูรณ์) ที่ทำไว้ก่อนหน้านี้ในการจัดการอินพุตก่อนหน้า สิ่งนี้อาจส่งผลให้เกิดผลลัพธ์ที่เปราะบางหรือไม่สามารถคาดเดาได้หากคุณไม่ออกแบบให้สอดคล้องกัน\n",
        "\n",
        "- **Fork and merge**\n",
        "\n",
        "อีกทางเลือกหนึ่งคือการจัดการอินพุตใหม่แบบขนาน โดย fork สถานะของเธรดเมื่อได้รับอินพุตใหม่และรวมสถานะสุดท้ายเมื่อการจัดการอินพุตเสร็จสิ้น ตัวเลือกนี้กำหนดให้ต้องออกแบบสถานะของคุณให้สามารถรวมได้โดยไม่มีข้อขัดแย้ง (เช่น การใช้ conflict-free replicated data types [CRDTs] หรืออัลกอริธึมการแก้ไขข้อขัดแย้งอื่นๆ) หรือให้ผู้ใช้แก้ไขข้อขัดแย้งด้วยตนเองก่อนที่คุณจะสามารถเข้าใจเอาต์พุตหรือส่งอินพุตใหม่ในเธรดนี้ หากตรงตามข้อกำหนดใดข้อกำหนดหนึ่งเหล่านี้ นี่น่าจะเป็นตัวเลือกที่ดีที่สุดโดยรวม เนื่องจากอินพุตใหม่ได้รับการจัดการอย่างทันท่วงที เอาต์พุตเป็นอิสระจากเวลาที่ได้รับ และรองรับการรันพร้อมกันจำนวนเท่าใดก็ได้\n",
        "\n",
        "กลยุทธ์เหล่านี้บางส่วนถูกนำมาใช้ใน LangGraph Cloud ซึ่งจะกล่าวถึงในบทที่ 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ZZd8z2m9q1"
      },
      "source": [
        "# Summary\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้ เราได้กลับมาพิจารณาถึงข้อแลกเปลี่ยนหลักที่คุณต้องเผชิญเมื่อสร้างแอปพลิเคชัน LLM นั่นคือ ความสามารถในการดำเนินการด้วยตนเอง (agency) กับความน่าเชื่อถือ (reliability) เราได้เรียนรู้ว่ามีกลยุทธ์ที่จะเอาชนะอุปสรรคบางส่วนและได้รับความน่าเชื่อถือมากขึ้นโดยไม่สูญเสียความสามารถในการดำเนินการด้วยตนเอง และในทางกลับกัน\n",
        "\n",
        "เราเริ่มต้นด้วยการกล่าวถึงเอาต์พุตที่มีโครงสร้าง ซึ่งสามารถปรับปรุงความสามารถในการคาดการณ์ของข้อความที่สร้างโดย LLM ต่อไป เราได้พูดคุยเกี่ยวกับการส่งออกเอาต์พุตแบบสตรีมมิ่ง/ระดับกลางจากแอปพลิเคชันของคุณ ซึ่งสามารถทำให้แอปพลิเคชันที่มีความหน่วงสูง (ผลข้างเคียงที่หลีกเลี่ยงไม่ได้ของความสามารถในการดำเนินการด้วยตนเองในปัจจุบัน) ใช้งานได้ง่าย\n",
        "\n",
        "เรายังได้กล่าวถึงการควบคุมแบบ human-in-the-loop ที่หลากหลาย นั่นคือ เทคนิคในการคืนการกำกับดูแลบางส่วนให้กับผู้ใช้ปลายทางของแอปพลิเคชัน LLM ของคุณ ซึ่งมักจะสร้างความแตกต่างในการทำให้สถาปัตยกรรมที่มีความสามารถในการดำเนินการด้วยตนเองสูงมีความน่าเชื่อถือ สุดท้าย เราได้พูดคุยเกี่ยวกับปัญหาของการจัดการอินพุตพร้อมกันไปยังแอปพลิเคชันของคุณ ซึ่งเป็นปัญหาที่สำคัญอย่างยิ่งสำหรับแอป LLM เนื่องจากมีความหน่วงสูง"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}