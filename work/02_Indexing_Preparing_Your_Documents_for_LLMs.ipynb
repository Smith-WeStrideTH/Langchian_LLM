{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ihTN1InwEe4O",
        "outputId": "b511e90f-513e-4b11-b44b-e2ff134ae425"
      },
      "outputs": [],
      "source": [
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain langchain_openai langchain_community langchain-text-splitters langchain-postgres\n",
        "# !pip install pypdf\n",
        "# !pip install reportlab\n",
        "# !pip uninstall psycopg\n",
        "# !pip install psycopg[binary]\n",
        "# !pip install -qU langchain_postgres\n",
        "# !pip install pgvector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkAYHVxfuznc",
        "outputId": "8dea89ee-5c8f-4f4e-bd81-cb9185034395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')\n",
        "\n",
        "# Get the API key from the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was found\n",
        "if api_key:\n",
        "  print(\"API Key loaded successfully.\")\n",
        "else:\n",
        "  print(\"API Key not found in the environment variables.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rLWMyxdUPtH"
      },
      "source": [
        "# Chapter 2. Indexing: Preparing Your Documents for LLMs\n",
        "ในบทก่อนหน้า คุณได้เรียนรู้เกี่ยวกับส่วนประกอบสำคัญที่ใช้ในการสร้างแอปพลิเคชัน LLM โดยใช้ LangChain คุณยังได้สร้างแชทบอท AI แบบง่ายๆ ซึ่งประกอบด้วยพร้อมท์ที่ส่งไปยังโมเดลและเอาต์พุตที่สร้างโดยโมเดล แต่มีข้อจำกัดที่สำคัญสำหรับแชทบอทแบบง่ายนี้\n",
        "\n",
        "จะเกิดอะไรขึ้นหากกรณีการใช้งานของคุณต้องการความรู้ที่โมเดลไม่ได้รับการฝึกอบรม ตัวอย่างเช่น สมมติว่าคุณต้องการใช้ AI เพื่อถามคำถามเกี่ยวกับบริษัท แต่ข้อมูลถูกเก็บไว้ในเอกสาร PDF หรือในเอกสารที่เป็นส่วนตัวของคุณหรือบริษัทของคุณ แม้ว่าเราจะเห็นผู้ให้บริการโมเดลเสริมชุดข้อมูลการฝึกอบรมเพื่อรวมข้อมูลสาธารณะของโลกมากขึ้นเรื่อยๆ (ไม่ว่าจะจัดเก็บในรูปแบบใด) แต่ข้อจำกัดที่สำคัญสองประการยังคงมีอยู่ในคลังความรู้ของ LLM:\n",
        "\n",
        "- **ข้อมูลส่วนบุคคล** : ข้อมูลที่ไม่เปิดเผยต่อสาธารณะนั้น ตามนิยามแล้วจะไม่รวมอยู่ในข้อมูลการฝึกอบรมของ LLM\n",
        "\n",
        "- **เหตุการณ์ปัจจุบัน**: การฝึกอบรม LLM เป็นกระบวนการที่ใช้ต้นทุนสูงและใช้เวลานาน ซึ่งอาจใช้เวลาหลายปี โดยการรวบรวมข้อมูลเป็นหนึ่งในขั้นตอนแรก ซึ่งส่งผลให้เกิดสิ่งที่เรียกว่าการตัดข้อมูลความรู้ หรือวันที่เกินกว่าที่ LLM จะไม่มีความรู้เกี่ยวกับเหตุการณ์ในโลกจริง โดยปกติแล้วจะเป็นวันที่ชุดข้อมูลการฝึกอบรมเสร็จสิ้น ซึ่งอาจอยู่ห่างออกไปตั้งแต่ไม่กี่เดือนถึงหลายปี ขึ้นอยู่กับโมเดลที่เป็นปัญหา\n",
        "ในทั้งสองกรณี โมเดลมีแนวโน้มที่จะ \"หลอน\" และตอบกลับด้วยข้อมูลที่ไม่ถูกต้อง การปรับแต่งพร้อมท์จะไม่แก้ไขปัญหานี้ด้วย เนื่องจากมันอาศัยความรู้ปัจจุบันของโมเดล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GlTB0zwVC3e"
      },
      "source": [
        "**เป้าหมาย: การเลือกบริบทที่เกี่ยวข้องสำหรับ LLM**\n",
        "\n",
        "หากข้อมูลส่วนบุคคล/ข้อมูลปัจจุบันเพียงอย่างเดียวที่คุณต้องการสำหรับกรณีการใช้งาน LLM ของคุณคือข้อความ 1-2 หน้า บทนี้จะสั้นลงมาก: สิ่งที่คุณต้องทำเพื่อให้ข้อมูลนั้นพร้อมใช้งานสำหรับ LLM คือการรวมข้อความทั้งหมดนั้นไว้ในทุกๆ พร้อมท์ที่คุณส่งไปยังโมเดล\n",
        "\n",
        "ความท้าทายในการทำให้ข้อมูลพร้อมใช้งานสำหรับ LLM นั้นเป็นปัญหาของปริมาณเป็นอันดับแรก คุณมีข้อมูลมากกว่าที่สามารถใส่ในแต่ละพร้อมท์ที่คุณส่งไปยัง LLM ได้ ดังนั้นนี่คือปัญหาที่ต้องแก้ไข: คุณจะรวมชุดย่อยขนาดเล็กใดจากคอลเลกชันข้อความขนาดใหญ่ของคุณในแต่ละครั้งที่คุณเรียกใช้โมเดล หรืออีกนัยหนึ่ง คุณจะเลือก (ด้วยความช่วยเหลือของโมเดล) ข้อความใดที่เกี่ยวข้องมากที่สุดเพื่อตอบคำถามแต่ละข้อ\n",
        "\n",
        "ในบทนี้และบทถัดไป คุณจะได้เรียนรู้วิธีการเอาชนะความท้าทายนี้ในสองขั้นตอน:\n",
        "\n",
        "1. **การสร้างดัชนีเอกสาร (Indexing)** กล่าวคือ การเตรียมข้อมูลล่วงหน้าในลักษณะที่แอปพลิเคชันของคุณสามารถค้นหาเอกสารที่เกี่ยวข้องมากที่สุดสำหรับแต่ละคำถามได้อย่างง่ายดาย\n",
        "2. **การดึงข้อมูลภายนอก (Retrieving)** จากดัชนีและใช้เป็น บริบท สำหรับ LLM เพื่อสร้างเอาต์พุตที่ถูกต้องตามข้อมูลของคุณ\n",
        "บทนี้เน้นที่การสร้างดัชนี ซึ่งเป็นขั้นตอนแรก ซึ่งเกี่ยวข้องกับการเตรียมข้อมูลล่วงหน้าของคุณให้เป็นรูปแบบที่สามารถเข้าใจและค้นหาได้ด้วย LLM แต่ก่อนที่เราจะเริ่มต้น มาพูดคุยกันถึง เหตุผล ที่เอกสารของคุณต้องได้รับการเตรียมข้อมูลล่วงหน้า"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPT2yUBWk4x"
      },
      "source": [
        "**สมมติว่าคุณต้องการใช้ LLM เพื่อวิเคราะห์ผลการดำเนินงานทางการเงินและความเสี่ยงในรายงานประจำปี 2565 ของ Tesla**\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure2-1.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "ซึ่งจัดเก็บเป็นข้อความในรูปแบบ PDF เป้าหมายของคุณคือสามารถถามคำถาม เช่น Tesla เผชิญกับความเสี่ยงสำคัญใดในปี 2565 และได้รับคำตอบที่คล้ายมนุษย์โดยอิงจากบริบทจากส่วนของปัจจัยเสี่ยงในเอกสาร\"\n",
        "\n",
        "เมื่อแบ่งย่อยแล้ว จะมีขั้นตอนสำคัญสี่ขั้นตอน ที่คุณต้องดำเนินการเพื่อให้บรรลุเป้าหมายนี้:\n",
        "\n",
        "1. สกัดข้อความจากเอกสาร\n",
        "2. แบ่งข้อความออกเป็นส่วนย่อยที่จัดการได้\n",
        "3. แปลงข้อความเป็นตัวเลขที่คอมพิวเตอร์สามารถเข้าใจได้\n",
        "4. จัดเก็บตัวแทนตัวเลขของข้อความเหล่านี้ในที่ที่สามารถเรียกดูส่วนที่เกี่ยวข้องของเอกสารเพื่อตอบคำถามที่กำหนดได้อย่างง่ายดายและรวดเร็ว"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rtQLPhHXCmI"
      },
      "source": [
        "**กระบวนการนี้เรียกว่าการรับข้อมูล (Ingestion)** ซึ่งเป็นกระบวนการแปลงเอกสารของคุณเป็นตัวเลขที่คอมพิวเตอร์สามารถเข้าใจและวิเคราะห์ได้ และจัดเก็บไว้ในฐานข้อมูลประเภทพิเศษสำหรับการเรียกค้นข้อมูลอย่างมีประสิทธิภาพ ตัวเลขเหล่านี้เรียกว่า embeddings ในทางการ และฐานข้อมูลประเภทพิเศษนี้เรียกว่า vector store ลองมาดูกันอย่างใกล้ชิดมากขึ้นว่า embeddings คืออะไรและทำไมจึงสำคัญ เริ่มจากสิ่งที่ง่ายกว่า embeddings ที่ใช้พลังงานจาก LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mgkg45yXODI"
      },
      "source": [
        "## Embeddings: Converting Text to Numbers\n",
        "Embeddings หมายถึงการแทนข้อความเป็นลำดับของตัวเลข (ยาว) นี่คือการแสดงข้อมูลแบบสูญเสียข้อมูล (lossy representation) นั่นคือ คุณไม่สามารถกู้คืนข้อความต้นฉบับจากลำดับตัวเลขเหล่านี้ได้ ดังนั้น คุณมักจะจัดเก็บทั้งข้อความต้นฉบับและการแสดงข้อมูลเชิงตัวเลขนี้ไว้ด้วย\n",
        "ดังนั้น ทำไมต้องยุ่งยาก? เพราะคุณจะได้ประโยชน์จากความยืดหยุ่นและพลังที่เกิดจากการทำงานกับตัวเลข: คุณสามารถคำนวณทางคณิตศาสตร์กับคำศัพท์ได้! ลองมาดูกันว่าทำไมจึงน่าตื่นเต้น\n",
        "\n",
        "**Sentence1 :** What a sunny day\n",
        "\n",
        "**Sentence2 :** Such bright skies today\n",
        "\n",
        "**Sentence3 :** I haven’t seen a sunny day in weeks\n",
        "\n",
        "\n",
        "|Word|What|a|sunny|day|Such|bright|skies|today|I|haven't|seen|a|sunny|day|in|weeks|\n",
        "|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|\n",
        "|**Sentence 1**|1|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
        "|**Sentence 2**|0|0|0|0|1|1|1|1|0|0|0|0|0|0|0|0|0|\n",
        "|**Sentence 3**|1|0|1|1|0|0|0|0|1|1|1|1|1|1|1|1|1|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_HcssAOYNZ4"
      },
      "source": [
        "## LLM-based Embeddings\n",
        "**การฝังข้อมูล (Embeddings) ที่ใช้ LLM**\n",
        "\n",
        "เราจะข้ามการพัฒนาด้าน ML ทั้งหมดที่เกิดขึ้นระหว่างนี้ไป และกระโดดตรงไปยังการฝังข้อมูล (Embeddings) ที่ใช้ LLM ทันที เพียงแค่ทราบว่ามีวิวัฒนาการแบบค่อยเป็นค่อยไปจากวิธีการง่าย ๆ ที่ระบุไว้ในส่วนก่อนหน้าไปสู่วิธีการที่ซับซ้อนซึ่งอธิบายไว้ในส่วนนี้\n",
        "\n",
        "คุณสามารถคิดว่าโมเดลการฝังข้อมูลเป็นส่วนหนึ่งของกระบวนการฝึกอบรม LLM หากคุณจำได้จากคำนำ กระบวนการฝึกอบรม LLM (การเรียนรู้จากข้อความจำนวนมาก) ช่วยให้ LLM สามารถเติมเต็มคำสั่ง (หรืออินพุต) ด้วยการต่อท้ายที่เหมาะสมที่สุด (เอาต์พุต) ความสามารถนี้เกิดจากความเข้าใจความหมายของคำและประโยคในบริบทของข้อความโดยรอบ ซึ่งเรียนรู้มาจากวิธีการใช้คำร่วมกันในข้อความฝึกอบรม ความเข้าใจในความหมาย (หรือความหมายเชิงคำศัพท์) ของคำสั่งนี้สามารถสกัดเป็นการแสดงข้อมูลเชิงตัวเลข (หรือการฝังข้อมูล) ของข้อความอินพุต และสามารถนำไปใช้โดยตรงสำหรับกรณีการใช้งานที่น่าสนใจมากมายเช่นกัน\n",
        "\n",
        "ในทางปฏิบัติ โมเดลการฝังข้อมูลส่วนใหญ่ได้รับการฝึกอบรมเพื่อวัตถุประสงค์นั้นโดยเฉพาะ โดยใช้สถาปัตยกรรมและกระบวนการฝึกอบรมที่คล้ายคลึงกับ LLM เนื่องจากมีประสิทธิภาพและให้ผลลัพธ์การฝังข้อมูลที่มีคุณภาพสูงกว่า\n",
        "\n",
        "ดังนั้น โมเดลการฝังข้อมูลจึงเป็นอัลกอริทึมที่รับชิ้นส่วนของข้อความและส่งออกการแสดงข้อมูลเชิงตัวเลขของความหมาย - ในทางเทคนิคแล้ว เป็นรายการตัวเลขทศนิยม (floating point) ที่ยาวมาก โดยปกติจะอยู่ระหว่าง 100 ถึง 2,000 ตัวเลข หรือมิติข้อมูล เหล่านี้เรียกว่าการฝังข้อมูลแบบหนาแน่น (dense embeddings) ซึ่งตรงกันข้ามกับการฝังข้อมูลแบบเบาบาง (sparse embeddings) ในส่วนก่อนหน้า เนื่องจากที่นี่โดยปกติแล้วมิติข้อมูลทั้งหมดจะแตกต่างจาก 0\n",
        "\n",
        "**NOTE:** โมเดลที่แตกต่างกันจะสร้างตัวเลขที่แตกต่างกัน และขนาดของรายการที่แตกต่างกัน ทั้งหมดนี้เฉพาะเจาะจงสำหรับแต่ละโมเดล นั่นคือ แม้ว่าขนาดของรายการจะตรงกัน คุณก็ไม่สามารถเปรียบเทียบการฝังข้อมูลจากโมเดลต่าง ๆ ได้ ควรหลีกเลี่ยงการรวมการฝังข้อมูลจากโมเดลต่าง ๆ เสมอ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsfubMH2wziW"
      },
      "source": [
        "## Semantic Embeddings\n",
        "\n",
        "**การฝังข้อมูลเชิงความหมาย**\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure2-2.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure2-3.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "ลองพิจารณาคำศัพท์ทั้งสามคำนี้: สิงโต (lion), สัตว์เลี้ยง (pet), และ สุนัข (dog)\n",
        "\n",
        "โดยสัญชาตญาณ คู่คำใดในสามคำนี้มีลักษณะคล้ายคลึงกันมากที่สุดเมื่อมองแวบแรก? คำตอบที่ชัดเจนคือ สัตว์เลี้ยง (pet) และ สุนัข (dog) แต่คอมพิวเตอร์ไม่มีความสามารถในการเข้าถึงสัญชาตญาณหรือความเข้าใจที่ละเอียดอ่อนของภาษาอังกฤษ เพื่อให้คอมพิวเตอร์สามารถแยกแยะระหว่างสัตว์เลี้ยง สิงโต หรือสุนัขได้ คุณต้องสามารถแปลคำเหล่านั้นเป็นภาษาของคอมพิวเตอร์ ซึ่งก็คือตัวเลข\n",
        "\n",
        "วิธีการหนึ่งที่มีประสิทธิภาพในการคำนวณระดับความคล้ายคลึงกันระหว่างเวกเตอร์สองตัวในพื้นที่มิติหลายมิติเรียกว่า ค่าความคล้ายคลึงแบบโคไซน์ (cosine similarity) ค่าความคล้ายคลึงแบบโคไซน์คำนวณผลคูณจุดของเวกเตอร์และหารด้วยผลคูณของขนาดของเวกเตอร์เหล่านั้นเพื่อให้ได้ผลลัพธ์เป็นตัวเลขระหว่าง -1 ถึง 1 โดยที่ 0 หมายถึงเวกเตอร์ไม่มีความสัมพันธ์กัน -1 หมายถึงแตกต่างกันอย่างสิ้นเชิง และ 1 หมายถึงเหมือนกันอย่างสิ้นเชิง ดังนั้น ในกรณีของคำทั้งสามคำนี้ ค่าความคล้ายคลึงแบบโคไซน์ระหว่าง สัตว์เลี้ยง (pet) และ สุนัข (dog) อาจเป็น 0.75 แต่ระหว่าง สัตว์เลี้ยง (pet) และ สิงโต (lion) อาจเป็น 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb6pmBdlx5r1"
      },
      "source": [
        "## Other Embedding\n",
        "\n",
        "**การใช้งานอื่น ๆ ของ Embeddings**\n",
        "\n",
        "ลำดับของตัวเลขเหล่านี้ เวกเตอร์ มีคุณสมบัติที่น่าสนใจหลายประการ:\n",
        "\n",
        "- ดังที่คุณได้เรียนรู้ก่อนหน้านี้ หากคุณคิดว่าเวกเตอร์เป็นตัวอธิบายจุดในพื้นที่มิติสูง จุดที่อยู่ใกล้กันจะมีความหมายที่คล้ายคลึงกันมากขึ้น ดังนั้นจึงสามารถใช้ฟังก์ชันระยะทางเพื่อวัดความคล้ายคลึงกัน\n",
        "- กลุ่มของจุดที่อยู่ใกล้กันสามารถกล่าวได้ว่ามีความสัมพันธ์กัน ดังนั้นอัลกอริธึมการจัดกลุ่มสามารถใช้เพื่อระบุหัวข้อ (หรือกลุ่มของจุด) และจำแนกอินพุตใหม่ลงในหนึ่งในหัวข้อเหล่านั้น\n",
        "\n",
        "- หากคุณหาค่าเฉลี่ยของการฝังข้อมูลหลาย ๆ รายการ การฝังข้อมูลเฉลี่ยสามารถกล่าวได้ว่าแสดงถึงความหมายโดยรวมของกลุ่มนั้น เช่น คุณสามารถฝังเอกสารยาวได้โดย\n",
        "  - การฝังข้อมูลแต่ละหน้าแยกกัน และจากนั้น\n",
        "  - การหาค่าเฉลี่ยของการฝังข้อมูลของทุกหน้าเป็นการฝังข้อมูลของหนังสือ\n",
        "\n",
        "- คุณสามารถ \"เดินทาง\" ใน \"พื้นที่ความหมาย\" โดยใช้การดำเนินการทางคณิตศาสตร์เบื้องต้นของการบวกและการลบ:\n",
        "  - ตัวอย่างเช่น การดำเนินการ กษัตริย์ (king) - ชาย (man) + หญิง (woman) = ราชินี (queen)\n",
        "  - หากคุณใช้ความหมาย (หรือการฝังข้อมูลเชิงความหมาย) ของ กษัตริย์ ลบด้วยความหมายของ ชาย โดยสมมติฐานแล้ว คุณจะมาถึงความหมายที่เป็นนามธรรมมากขึ้นของ ผู้ปกครอง (monarch)\n",
        "  - ณ จุดนี้ หากคุณบวกความหมายของ หญิง คุณจะมาถึงใกล้เคียงกับความหมาย (หรือการฝังข้อมูล) ของคำว่า ราชินี\n",
        "\n",
        "- มีโมเดลที่สามารถสร้างการฝังข้อมูลสำหรับเนื้อหาที่ไม่ใช่ข้อความ เช่น รูปภาพ วิดีโอ เสียง นอกเหนือจากข้อความ ซึ่งจะช่วยให้สามารถค้นหารูปภาพที่คล้ายคลึงหรือเกี่ยวข้องมากที่สุดสำหรับประโยคที่กำหนดได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJujTYSQzS9O"
      },
      "source": [
        "# Converting Your Documents into Text\n",
        "\n",
        "**การแปลงเอกสารของคุณเป็นข้อความ**\n",
        "\n",
        "ดังที่กล่าวไว้ในตอนต้นของบทนี้ ขั้นตอนแรกในการเตรียมข้อมูลเอกสารของคุณคือการแปลงเป็นข้อความ เพื่อให้บรรลุเป้าหมายนี้ คุณจะต้องสร้างตรรกะเพื่อวิเคราะห์และสกัดเอกสารโดยมีความสูญเสียคุณภาพน้อยที่สุด โชคดีที่ LangChain มี document loaders ที่จัดการตรรกะการวิเคราะห์และช่วยให้คุณสามารถ \"โหลด\" ข้อมูลจากแหล่งต่างๆ เข้าสู่คลาส Document [ตัวอย่างคลาส](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)ซึ่งประกอบด้วยข้อความและเมตาดาต้าที่เกี่ยวข้อง\n",
        "\n",
        "ตัวอย่างเช่น พิจารณาไฟล์ .txt แบบง่าย คุณสามารถนำเข้าคลาส TextLoader ของ LangChain เพื่อสกัดข้อความได้ง่ายๆ เช่นนี้ เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czOB16zYH7NU"
      },
      "source": [
        "## Text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUOGTnuAUd4D",
        "outputId": "556df2d0-1283-4315-824c-6222516441cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './docs/test.txt'}, page_content='This is test file')]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "loader = TextLoader(\"./docs/test.txt\")\n",
        "loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYhrDwrjE-MW"
      },
      "source": [
        "## WebBase\n",
        "นอกเหนือจากไฟล์ .txt แล้ว LangChain ยังมี document loaders สำหรับไฟล์ประเภททั่วไปอื่นๆ รวมถึง .csv, .json และ Markdown พร้อมทั้งการผสานรวมกับแพลตฟอร์มยอดนิยม เช่น Slack และ Notion\n",
        "\n",
        "\n",
        "ตัวอย่างเช่น คุณสามารถใช้ WebBaseLoader เพื่อโหลด HTML จาก URL บนเว็บและวิเคราะห์เป็นข้อความ เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wmjgHb3mE7d8",
        "outputId": "e9e5eb02-9501-4b9a-92c1-8d9f22967c9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.langchain.com/', 'title': 'LangChain', 'description': 'LangChain’s suite of products supports developers along each step of their development journey.', 'language': 'en'}, page_content=\"LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nLangChainLangSmithLangGraphMethods\\n\\nRetrievalAgentsEvaluationResources\\n\\nBlogCustomer StoriesLangChain AcademyCommunityExpertsChangelogLLM Evaluations GuideState of AI AgentsBreakout Agent StoriesDocs\\n\\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\\n\\nAboutCareersPricing\\n\\nLangSmithLangGraph PlatformGet a demoSign up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nLangChainLangSmithLangGraphMethods\\n\\nRetrievalAgentsEvaluationResources\\n\\nBlogCustomer StoriesLangChain AcademyCommunityExpertsChangelogLLM Evaluations GuideState of AI AgentsBreakout Agent StoriesDocs\\n\\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\\n\\nAboutCareersPricing\\n\\nLangSmithLangGraph PlatformGet a demoSign upLangChain’s suite of products supports developers along each step of the LLM application lifecycle.Applications that can reason. Powered by LangChain.Get a demoSign up for free\\n\\nFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageDebug, collaborate, test, and monitor your LLM app in LangSmith - whether it's built with a LangChain framework or not.\\xa0\\n\\n\\n\\nBuild your app with LangChainBuild context-aware, reasoning applications with LangChain’s flexible framework that leverages your company’s data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChain\\n\\n\\n\\n\\nRun at scale with LangGraph\\xa0PlatformUse LangGraph Platform’s APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.\\n\\n\\nLearn more about LangGraph\\xa0Platform\\n\\nManage LLM performance with\\xa0LangSmithShip faster with LangSmith’s debug, test, deploy, and monitoring workflows. Don’t rely on “vibes” – add engineering rigor to your LLM-development workflow, whether you’re building with LangChain or not.Learn more about LangSmith\\n\\n\\nHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.“LangSmith helped us improve the accuracy and performance of Retool’s fine-tuned models. Not only did we deliver a better product by iterating with LangSmith, but we’re shipping new AI features to our users in a fraction of the time it would have taken without it.”Jamie CuffeHead of Self-Serve and New Products“By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches of using LLMs in an enterprise-setting faster.”Yusuke KajiGeneral Manager of AI“Working with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn’t have achieved \\xa0the product experience delivered to our customers without LangChain, and we couldn’t have done it at the same pace without LangSmith.”James SpiteriDirector of Security Products“As soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.”Jose PeñaSenior Manager\\n\\n\\n\\n\\n\\n\\n\\n\\nThe reference architecture enterprises adopt for success.LangChain’s suite of products can be used independently or stacked together for multiplicative impact – guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChain\\n\\n\\nGet started with the LangSmith platform todayGet a demoSign up for freeTeams building with LangChain are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.See customer stories\\n\\n\\nGet inspired by companies who have done it.Financial Services\\n\\n\\nFinTech\\n\\n\\nTechnology\\n\\n\\nLangSmith is the enterprise\\xa0developer platform\\xa0built for LLMs.Explore LangSmith\\n\\n\\n\\n\\nGain visibility to make trade offs between cost, latency, and quality.\\n\\nIncrease developer productivity.\\n\\nEliminate manual, error-prone testing.\\n\\nReduce hallucinations and improve reliability.\\n\\nEnterprise deployment options to keep data secure.Ready to start shipping \\u2028reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for freeProductsLangChainLangSmithLangGraphAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsChangelogCommunityLangSmith Trust PortalCompanyAboutCareersBlogTwitterLinkedInYouTubeMarketing AssetsSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\")]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure you connect to internet\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://www.langchain.com/\")\n",
        "loader.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWv7lqGcHxay"
      },
      "source": [
        "## PDF\n",
        "เราสามารถใช้ PDF Loader ของ LangChain เพื่อสกัดข้อความจากเอกสาร PDF ได้ เริ่มต้นใน Python:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jft72OJJFM_m",
        "outputId": "d597e495-b393-45e5-9686-939ae73b16f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './docs/test.pdf', 'page': 0, 'page_label': '1'}, page_content='This is test pdf file\\n')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"./docs/test.pdf\")\n",
        "pages = loader.load()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv4DV_5dI2zR"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "file_path = './docs/2021-tesla-impact-report.pdf'\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ztoTin_Kwnt",
        "outputId": "9f2b70f5-d2aa-4536-ec31-ddc1e8ea2be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "Foreword on Impact\n"
          ]
        }
      ],
      "source": [
        " # Print the content of the first page only first 20 words\n",
        "print(pages[1].page_content[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg_S9u3FIfmC"
      },
      "source": [
        "ข้อความถูกสกัดจากเอกสาร PDF แล้วและจัดเก็บไว้ในคลาส Document แต่มีปัญหาคือ เอกสารที่โหลดมามีความยาวมากกว่า 100,000 ตัวอักษร ดังนั้นจึงไม่พอดีกับหน้าต่างบริบท (context window) ของ LLM หรือโมเดลการฝังข้อมูลส่วนใหญ่ เพื่อเอาชนะข้อจำกัดนี้ เราจำเป็นต้องแบ่งเอกสารออกเป็นชิ้นส่วนย่อยที่จัดการได้ ซึ่งเราสามารถแปลงเป็นการฝังข้อมูลและค้นหาเชิงความหมายในภายหลัง นำเราไปสู่ขั้นตอนที่ 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_J6hBTCLY05"
      },
      "source": [
        "**NOTE :**โมเดล LLM และโมเดลการฝังข้อมูลได้รับการออกแบบโดยมีขีดจำกัดสูงสุดในการประมวลผลขนาดของข้อความอินพุตและเอาต์พุต ขีดจำกัดนี้มักเรียกว่าหน้าต่างบริบท (context window) และโดยปกติแล้วจะใช้กับการรวมกันของอินพุตและเอาต์พุต\n",
        "- กล่าวคือ หากหน้าต่างบริบทคือ 100 (เราจะพูดถึงหน่วยในภายหลัง) และอินพุตของคุณมีขนาด 90 เอาต์พุตสามารถมีความยาวได้มากที่สุด 10 หน้าต่างบริบทมักจะวัดเป็นจำนวนโทเค็น (tokens) เช่น 8,192 โทเค็น ดังที่กล่าวไว้ในคำนำ โทเค็นเป็นการแสดงข้อความเป็นตัวเลข โดยปกติแล้วโทเค็นแต่ละตัวจะครอบคลุมระหว่าง 3-4 ตัวอักษรของข้อความภาษาอังกฤษ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQcD97LLtSf"
      },
      "source": [
        "# Splitting Your Text Into Chunks\n",
        "\n",
        "**การแบ่งข้อความของคุณออกเป็นชิ้นส่วนย่อย**\n",
        "\n",
        "ในแวบแรก อาจดูเหมือนตรงไปตรงมาในการแบ่งข้อความขนาดใหญ่เป็นชิ้นส่วนย่อย แต่การรักษาชิ้นส่วนข้อความที่เกี่ยวข้องกันในเชิงความหมาย (เกี่ยวข้องกันตามความหมาย) ไว้ด้วยกันเป็นกระบวนการที่ซับซ้อน เพื่อให้ง่ายต่อการแบ่งเอกสารขนาดใหญ่เป็นชิ้นส่วนข้อความขนาดเล็ก แต่ยังคงมีความหมาย LangChain มี `RecursiveCharacterTextSplitter` ซึ่งดำเนินการดังต่อไปนี้:\n",
        "\n",
        "1. รวบรวมตัวคั่น (separators) เป็นรายการ โดยเรียงลำดับความสำคัญ โดยค่าเริ่มต้นคือ\n",
        "\n",
        "1. ตัวคั่นย่อหน้า (paragraph separator): \\n\\n\n",
        "2. ตัวคั่นบรรทัด (line separator): \\n\n",
        "3. ตัวคั่นคำ (word separator): อักขระช่องว่าง\n",
        "2. เพื่อให้เคารพขนาดชิ้นส่วนที่กำหนด เช่น 1,000 ตัวอักษร เริ่มต้นด้วยการแบ่งย่อหน้า\n",
        "\n",
        "3. สำหรับย่อหน้าใด ๆ ที่ยาวกว่าขนาดชิ้นส่วนที่ต้องการ ให้แบ่งตามตัวคั่นถัดไป นั่นคือ บรรทัด ดำเนินการต่อจนกระทั่งชิ้นส่วนทั้งหมดมีขนาดเล็กกว่าความยาวที่ต้องการ หรือไม่มีตัวคั่นเพิ่มเติมให้ลอง\n",
        "\n",
        "4. ส่งออกแต่ละชิ้นส่วนเป็น Document พร้อมกับเมตาดาต้าของเอกสารต้นฉบับที่ส่งผ่าน และข้อมูลเพิ่มเติมเกี่ยวกับตำแหน่งในเอกสารต้นฉบับ\n",
        "\n",
        "ตัวแบ่งข้อความของ LangChain เหล่านี้จัดการรูปแบบข้อมูลต่าง ๆ รวมถึง HTML, รหัส, JSON, Markdown, ข้อความ และอื่น ๆ มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrF--82HzQgn"
      },
      "source": [
        "## RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enUm0WOYH1mS",
        "outputId": "58b198ec-d623-49c0-8d57-792e410d8e91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './docs/test.txt'}, page_content='Th'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='hi'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='is'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='i'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='is'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='t'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='te'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='es'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='st'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='f'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='fi'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='il'),\n",
              " Document(metadata={'source': './docs/test.txt'}, page_content='le')]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "loader = TextLoader(\"./docs/test.txt\") # or any other loader\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2, # nomally 1000\n",
        "    chunk_overlap=1, # nomally 200\n",
        ")\n",
        "splitted_docs = splitter.split_documents(docs)\n",
        "splitted_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VfHYOaM2wW"
      },
      "source": [
        "ในโค้ดก่อนหน้านี้ เอกสารที่สร้างโดยตัวโหลดเอกสารจะถูกแบ่งออกเป็นชิ้นส่วนย่อยขนาด 2 ตัวอักษรต่อชิ้นส่วน โดยมีการทับซ้อนกันระหว่างชิ้นส่วนย่อย 1ตัวอักษรเพื่อรักษาบริบทบางส่วน ผลลัพธ์ยังเป็นรายการเอกสาร โดยที่แต่ละเอกสารมีความยาวสูงสุด 2 ตัวอักษร แบ่งตามการแบ่งส่วนตามธรรมชาติของข้อความที่เขียน เช่น ย่อหน้า บรรทัดใหม่ และสุดท้าย คำ นี้ใช้โครงสร้างของข้อความเพื่อรักษาแต่ละชิ้นส่วนให้เป็นส่วนย่อยของข้อความที่สอดคล้องกันและอ่านง่าย\n",
        "\n",
        "`RecursiveCharacterTextSplitter` ยังสามารถใช้เพื่อแบ่งภาษาการเขียนโปรแกรมและ Markdown ออกเป็นชิ้นส่วนเชิงความหมายได้ ซึ่งทำได้โดยใช้คำหลักเฉพาะของแต่ละภาษาเป็นตัวคั่น ซึ่งจะช่วยให้มั่นใจได้ว่า ตัวอย่างเช่น เนื้อหาของฟังก์ชันแต่ละฟังก์ชันจะถูกเก็บไว้ในชิ้นส่วนเดียวกัน แทนที่จะถูกแบ่งระหว่างหลายชิ้นส่วน\n",
        " - โดยปกติแล้ว เนื่องจากภาษาการเขียนโปรแกรมมีโครงสร้างมากกว่าข้อความที่เขียน จึงมีความจำเป็นน้อยกว่าที่จะใช้การทับซ้อนกันระหว่างชิ้นส่วน LangChain มีตัวคั่นสำหรับภาษาที่ได้รับความนิยมจำนวนมาก เช่น Python, JS, Markdown, HTML และอื่น ๆ อีกมากมาย มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0hOX2JyCCx"
      },
      "source": [
        "## Language.PYTHON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu9TqEgrMf0O",
        "outputId": "e310125c-889f-4cd2-9625-86f53df63575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
              " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ85qCmMOByL"
      },
      "source": [
        "สังเกตว่าเรายังคงใช้ `RecursiveCharacterTextSplitter` เหมือนข้างต้น แต่ตอนนี้เรากำลังสร้างอินสแตนซ์ของมันสำหรับภาษาเฉพาะ โดยใช้เมธอด `from_language` เมธอดนี้ยอมรับชื่อของภาษา และพารามิเตอร์ปกติสำหรับขนาดชิ้นส่วน ฯลฯ นอกจากนี้ โปรดสังเกตว่าตอนนี้เราใช้เมธอด `create_documents` ซึ่งยอมรับรายการสตริง แทนที่จะเป็นรายการเอกสารที่เรามีมาก่อนหน้านี้ **เมธอดนี้มีประโยชน์เมื่อข้อความที่คุณต้องการแบ่งไม่ได้มาจากตัวโหลดเอกสาร ดังนั้นคุณจึงมีเพียงสตริงข้อความดิบเท่านั้น**\n",
        "\n",
        "คุณยังสามารถใช้อาร์กิวเมนต์ที่สองซึ่งเป็นตัวเลือกของ `create_documents` เพื่อส่งรายการเมตาดาต้าเพื่อเชื่อมโยงกับสตริงข้อความแต่ละรายการ รายการเมตาดาต้านี้ควรมีความยาวเท่ากับรายการสตริง และจะถูกใช้เพื่อเติมข้อมูลในฟิลด์เมตาดาต้าของแต่ละเอกสารที่ส่งคืน\n",
        "\n",
        "มาดูตัวอย่างสำหรับข้อความ Markdown โดยใช้อาร์กิวเมนต์เมตาดาต้าด้วย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7wmbo3yWdd"
      },
      "source": [
        "## Language.MARKDOWN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD8lPMX_NiGj",
        "outputId": "273ee428-a16b-462c-860e-979addd8539a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.langchain.com'}, page_content='# LangChain'),\n",
              " Document(metadata={'source': 'https://www.langchain.com'}, page_content='Building applications with LLMs through composability'),\n",
              " Document(metadata={'source': 'https://www.langchain.com'}, page_content='## Quick Install\\n```bash\\npip install langchain'),\n",
              " Document(metadata={'source': 'https://www.langchain.com'}, page_content='```'),\n",
              " Document(metadata={'source': 'https://www.langchain.com'}, page_content='As an open-source project in a rapidly developing field, we'),\n",
              " Document(metadata={'source': 'https://www.langchain.com'}, page_content='are extremely open to contributions.')]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "markdown_text = \"\"\"\n",
        "# LangChain\n",
        "Building applications with LLMs through composability\n",
        "## Quick Install\n",
        "```bash\n",
        "pip install langchain\n",
        "```\n",
        "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
        "\"\"\"\n",
        "\n",
        "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
        ")\n",
        "md_docs = md_splitter.create_documents([markdown_text], [{\"source\": \"https://www.langchain.com\"}])\n",
        "md_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMHaWUF1PYdI"
      },
      "source": [
        "**สังเกตสองสิ่ง:**\n",
        "\n",
        "- ข้อความถูกแบ่งตามจุดหยุดตามธรรมชาติในเอกสาร Markdown ตัวอย่างเช่น หัวข้อจะอยู่ในชิ้นส่วนเดียว บรรทัดของข้อความที่อยู่ด้านล่างจะอยู่ในชิ้นส่วนที่แยกต่างหาก และอื่นๆ\n",
        "- เมตาดาต้าที่เราส่งผ่านในอาร์กิวเมนต์ที่สองจะถูกแนบกับเอกสารผลลัพธ์แต่ละฉบับ ซึ่งช่วยให้คุณสามารถติดตามได้ว่าเอกสารมาจากที่ใด และคุณสามารถไปที่ใดเพื่อดูต้นฉบับ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYzM041xb0ys"
      },
      "source": [
        "# Generating Text Embeddings\n",
        "\n",
        "**การสร้างการฝังข้อมูลข้อความ**\n",
        "\n",
        "LangChain ยังมีคลาส Embeddings ที่ออกแบบมาเพื่อเชื่อมต่อกับโมเดลการฝังข้อมูลข้อความ รวมถึง OpenAI, Cohere และ Hugging Face และสร้างการแสดงข้อมูลแบบเวกเตอร์ของข้อความ คลาสนี้มีสองเมธอด: หนึ่งสำหรับการฝังข้อมูลเอกสาร และอีกหนึ่งสำหรับการฝังข้อมูลแบบสอบถาม อดีตใช้ข้อความหลายรายการเป็นอินพุต ขณะที่หลังใช้ข้อความเดียวเป็นอินพุต"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGUVyGNjO4kw",
        "outputId": "2594371c-fac7-4d80-e74f-e89188776689"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "model = OpenAIEmbeddings()\n",
        "embeddings = model.embed_documents([\n",
        "    \"Hi there!\",\n",
        "    \"Oh, hello!\",\n",
        "    \"What's your name?\",\n",
        "    \"My friends call me World\",\n",
        "    \"Hello World!\"\n",
        "])\n",
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO2ufAr-b9w9",
        "outputId": "1487be0c-93ef-488b-872a-87e0743a2ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHWb71s_wzpW"
      },
      "source": [
        "สังเกตว่าคุณสามารถฝังข้อมูลเอกสารหลายฉบับในเวลาเดียวกันได้ คุณควรเลือกวิธีนี้มากกว่าการฝังข้อมูลทีละฉบับ เนื่องจากจะมีประสิทธิภาพมากกว่า (เนื่องจากวิธีการสร้างโมเดลเหล่านี้) คุณจะได้รับรายการของรายการตัวเลข โดยรายการภายในแต่ละรายการคือเวกเตอร์ หรือการฝังข้อมูล ตามที่ได้อธิบายไว้ในหัวข้อก่อนหน้า\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Aarqw0w9Gea"
      },
      "source": [
        "ตอนนี้มาดูตัวอย่างแบบ end-to-end ที่ใช้ความสามารถทั้งสามที่เราได้เห็นมาจนถึงตอนนี้\n",
        "\n",
        "- ตัวโหลดเอกสาร (Document loaders) เพื่อแปลงเอกสารใด ๆ เป็นข้อความธรรมดา\n",
        "ตัวแบ่งข้อความ (Text splitters) เพื่อแบ่งเอกสารขนาดใหญ่แต่ละฉบับออกเป็นหลายชิ้นส่วนที่เล็กกว่า\n",
        "โมเดลการฝังข้อมูล (Embeddings models) เพื่อสร้างการแสดงข้อมูลเชิงตัวเลขของความหมายของแต่ละชิ้นส่วน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SbVNgJbac4PN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "## Load the document\n",
        "file_path = './docs/2021-tesla-impact-report.pdf'\n",
        "loader = PyPDFLoader(file_path)\n",
        "doc = loader.load() # [Document(metadata={'source': './test.txt'}, page_content='This is test file')]\n",
        "\n",
        "## Split the document\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "chunks = text_splitter.split_documents(doc)\n",
        "\n",
        "## Generate embeddings\n",
        "model = OpenAIEmbeddings()\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "embeddings = model.embed_documents(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uw407tXy9rl"
      },
      "source": [
        "เมื่อคุณสร้างการฝังข้อมูลจากเอกสารของคุณแล้ว ขั้นตอนต่อไปคือการจัดเก็บไว้ในฐานข้อมูลพิเศษที่เรียกว่า Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ZjyHACzLJP"
      },
      "source": [
        "# Vector Store\n",
        "\n",
        "**การจัดเก็บการฝังข้อมูลใน Vector Store**\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure2-4.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "---\n",
        "ก่อนหน้านี้ในบทนี้ เราได้พูดคุยเกี่ยวกับการคำนวณค่าความคล้ายคลึงแบบโคไซน์เพื่อวัดความคล้ายคลึงกันระหว่างเวกเตอร์ในพื้นที่เวกเตอร์ Vector store เป็นฐานข้อมูลที่ออกแบบมาเพื่อจัดเก็บเวกเตอร์และดำเนินการคำนวณที่ซับซ้อน เช่น ค่าความคล้ายคลึงแบบโคไซน์ ได้อย่างมีประสิทธิภาพและรวดเร็ว\n",
        "\n",
        "ไม่เหมือนกับฐานข้อมูลแบบดั้งเดิมที่เชี่ยวชาญในการจัดเก็บข้อมูลที่มีโครงสร้าง (เช่น เอกสาร JSON หรือข้อมูลที่สอดคล้องกับรูปแบบแผนผังของฐานข้อมูลเชิงสัมพันธ์) Vector store จัดการข้อมูลที่ไม่มีโครงสร้าง รวมถึงข้อความและรูปภาพ เช่นเดียวกับฐานข้อมูลแบบดั้งเดิม Vector store สามารถดำเนินการสร้าง-อ่าน-อัปเดต-ลบ (CRUD) และการค้นหาได้\n",
        "\n",
        "Vector store เปิดใช้งานกรณีการใช้งานที่หลากหลาย รวมถึงแอปพลิเคชันที่ปรับขนาดได้ซึ่งใช้ AI ในการตอบคำถามเกี่ยวกับเอกสารขนาดใหญ่ ดังที่แสดงใน..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVBe9R6b1Y2y"
      },
      "source": [
        "ปัจจุบัน มีผู้ให้บริการ Vector Store ให้เลือกมากมาย โดยแต่ละรายการมีความเชี่ยวชาญในความสามารถที่แตกต่างกัน การเลือกของคุณควรขึ้นอยู่กับข้อกำหนดที่สำคัญของแอปพลิเคชันของคุณ รวมถึงการเช่าหลายผู้เช่า (multi-tenancy) ความสามารถในการกรองเมตาดาต้า ประสิทธิภาพต้นทุน และความสามารถในการปรับขนาด\n",
        "\n",
        "แม้ว่า Vector Store จะเป็นฐานข้อมูลเฉพาะที่สร้างขึ้นเพื่อจัดการข้อมูลเวกเตอร์ แต่ก็มีข้อเสียบางประการในการทำงานกับพวกมัน:\n",
        "\n",
        "- Vector Store ส่วนใหญ่ค่อนข้างใหม่และอาจไม่ผ่านการทดสอบตามเวลา\n",
        "- การจัดการและปรับแต่ง Vector Store อาจมีความซับซ้อนค่อนข้างมาก\n",
        "-   การจัดการฐานข้อมูลแยกต่างหากเพิ่มความซับซ้อนให้กับแอปพลิเคชันของคุณและอาจใช้ทรัพยากรที่มีค่า\n",
        "โชคดีที่ความสามารถของ Vector Store ได้รับการขยายไปยัง PostgreSQL (ฐานข้อมูลเชิงสัมพันธ์แบบโอเพนซอร์สยอดนิยม) ผ่านส่วนขยาย pgvector ซึ่งช่วยให้คุณสามารถใช้ฐานข้อมูลเดียวกันที่คุณคุ้นเคย เพื่อขับเคลื่อนทั้งตารางธุรกรรมของคุณ (เช่น ตารางผู้ใช้ของคุณ) รวมถึงตารางการค้นหาเวกเตอร์ของคุณ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEVO7sW8C0R"
      },
      "source": [
        "## Pgvector\n",
        "\n",
        "**การตั้งค่า Pgvector**\n",
        "\n",
        "ในการใช้ Postgres และ Pgvector คุณจะต้องทำตามขั้นตอนการตั้งค่าสองสามขั้นตอน:\n",
        "\n",
        "1. ตรวจสอบให้แน่ใจว่าคุณได้ติดตั้ง Docker บนคอมพิวเตอร์ของคุณแล้ว ดูคำแนะนำสำหรับระบบปฏิบัติการของคุณได้ที่ [link](https://docs.docker.com/get-started/get-docker/)\n",
        "2. เรียกใช้คำสั่งต่อไปนี้ในเทอร์มินัลของคุณ คำสั่งนี้จะเปิดใช้งานอินสแตนซ์ Postgres ในคอมพิวเตอร์ของคุณที่ทำงานบนพอร์ต 6024\n",
        "3. บันทึกสตริงการเชื่อมต่อเพื่อใช้ในโค้ดของคุณ เราจะต้องใช้ในภายหลัง\n",
        "`postgresql+psycopg://langchain:langchain@localhost:6024/langchain`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2z8gXIu8swI"
      },
      "source": [
        "## Vector Store\n",
        "การทำงานกับ Vector Store\n",
        "\n",
        "ต่อจากที่เราหยุดไว้ในหัวข้อก่อนหน้าเกี่ยวกับ Embeddings ตอนนี้มาดูตัวอย่างการโหลด แบ่งส่วน ฝังข้อมูล และจัดเก็บเอกสารใน Pgvector เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsY-X7CKaEL2",
        "outputId": "5cda3b7f-29b7-46d4-aca3-6f1dfee0e1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e7b364f8f71100562a2da33c85fdfa32e7c79bc4fe74fabd14f11505c1008ea2\n"
          ]
        }
      ],
      "source": [
        "!docker run --name pgvector-container -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-5TfSgtGFlD",
        "outputId": "ca964c6e-e1e1-4fd4-e3a9-c7e7734dc51c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './docs/test.txt'}, page_content='This is test file')]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_postgres.vectorstores import PGVector\n",
        "\n",
        "# Load the document, split it into chunks\n",
        "raw_documents = TextLoader('./docs/test.txt').load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mjIenxhA5vj"
      },
      "outputs": [],
      "source": [
        "# embed each chunk and insert it into the vector store\n",
        "model = OpenAIEmbeddings()\n",
        "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
        "db = PGVector.from_documents(documents, model, connection=connection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndtUKcvK9Y6_"
      },
      "source": [
        "สังเกตว่าเรานำรหัสจากส่วนก่อนหน้ากลับมาใช้ใหม่ เพื่อโหลดเอกสารด้วยตัวโหลดก่อน จากนั้นแบ่งออกเป็นชิ้นส่วนย่อย จากนั้นเราสร้างอินสแตนซ์ของโมเดลการฝังข้อมูลที่เราต้องการใช้ ในกรณีนี้คือของ OpenAI โปรดทราบว่าคุณสามารถใช้โมเดลการฝังข้อมูลอื่นใดก็ได้ที่ LangChain รองรับที่นี่\n",
        "\n",
        "จากนั้นเรามีโค้ดบรรทัดใหม่ ซึ่งสร้าง Vector Store ที่กำหนดเอกสาร โมเดลการฝังข้อมูล และสตริงการเชื่อมต่อ สิ่งนี้จะทำสิ่งต่างๆ ดังต่อไปนี้:\n",
        "\n",
        "- สร้างการเชื่อมต่อกับอินสแตนซ์ Postgres ที่ทำงานในคอมพิวเตอร์ของคุณ (ดูส่วนการตั้งค่าก่อนหน้านี้)\n",
        "- เรียกใช้การตั้งค่าที่จำเป็น เช่น การสร้างตารางเพื่อเก็บเอกสารและเวกเตอร์ของคุณ หากนี่เป็นครั้งแรกที่คุณเรียกใช้\n",
        "- สร้างการฝังข้อมูลสำหรับเอกสารแต่ละฉบับที่คุณส่งผ่าน โดยใช้โมเดลที่คุณเลือก\n",
        "- จัดเก็บการฝังข้อมูล เมตาดาต้าของเอกสาร และเนื้อหาข้อความของเอกสารใน Postgres พร้อมสำหรับการค้นหา\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWgk2hRaMnq9"
      },
      "source": [
        "มาดูกันว่าการค้นหาเอกสารเป็นอย่างไร เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayFTuQrl9IT3",
        "outputId": "bffb29c3-2baa-4908-b794-0e686c7f051f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='83261a76-69ff-476d-bfff-2c8040dc58f7', metadata={'source': './docs/test.txt'}, page_content='This is test file'),\n",
              " Document(id='d8223846-5361-458c-bb7f-2206a6de476b', metadata={'source': './docs/test.txt'}, page_content='This is test file')]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.similarity_search(\"query\", k=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1tkwI0tO7KT"
      },
      "source": [
        "วิธีนี้จะค้นหาเอกสารที่เกี่ยวข้องมากที่สุด (ซึ่งคุณได้จัดทำดัชนีไว้ก่อนหน้านี้ตามที่กล่าวไว้ข้างต้น) โดยทำตามขั้นตอนต่อไปนี้:\n",
        "\n",
        "คำค้นหา ในกรณีนี้คือคำว่า`query`จะถูกส่งไปยังโมเดลการฝังข้อมูลเพื่อดึงข้อมูลการฝังข้อมูล\n",
        "จากนั้นจะเรียกใช้คิวรีบน Postgres เพื่อค้นหาการฝังข้อมูล N รายการก่อนหน้านี้ (ในกรณีนี้ 4 รายการ) ที่คล้ายคลึงกับคำค้นหาของคุณมากที่สุด\n",
        "สุดท้าย จะดึงเนื้อหาข้อความและเมตาดาต้าที่เกี่ยวข้องกับการฝังข้อมูลแต่ละรายการ\n",
        "และส่งคืนรายการ`Document`ที่เรียงลำดับตามความคล้ายคลึงกันกับคำค้นหา โดยที่คล้ายคลึงกันมากที่สุดก่อน รองลงมาเป็นอันดับที่สอง และอื่นๆ\n",
        "คุณยังสามารถเพิ่มเอกสารเพิ่มเติมลงในฐานข้อมูลที่มีอยู่ได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGX4Te8sPFPy"
      },
      "source": [
        "มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je_u6kQ4M_wr",
        "outputId": "a9d5dfab-e16d-43f3-94cd-d5f804930f0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "db.add_documents([\n",
        "  Document(\n",
        "      page_content=\"there are cats in the pond\",\n",
        "      metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
        "  ),\n",
        "  Document(\n",
        "      page_content=\"ducks are also found in the pond\",\n",
        "      metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
        "  ),\n",
        "], ids=[1, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn75WhgqSERy",
        "outputId": "9f59972f-b948-4a90-acf5-4c954a8d9c0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2', metadata={'topic': 'animals', 'location': 'pond'}, page_content='ducks are also found in the pond'),\n",
              " Document(id='1', metadata={'topic': 'animals', 'location': 'pond'}, page_content='there are cats in the pond'),\n",
              " Document(id='83261a76-69ff-476d-bfff-2c8040dc58f7', metadata={'source': './docs/test.txt'}, page_content='This is test file'),\n",
              " Document(id='d8223846-5361-458c-bb7f-2206a6de476b', metadata={'source': './docs/test.txt'}, page_content='This is test file')]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.similarity_search(\"ducks\", k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtCyRcOVQEl1"
      },
      "source": [
        "เมธอด`add_documents`ที่เรากำลังใช้อยู่ที่นี่จะปฏิบัติตามกระบวนการที่คล้ายคลึงกับ`fromDocuments`:\n",
        "\n",
        "1. สร้างการฝังข้อมูลสำหรับเอกสารแต่ละฉบับที่คุณส่งผ่าน โดยใช้โมเดลที่คุณเลือก\n",
        "2. จัดเก็บการฝังข้อมูล เมตาดาต้าของเอกสาร และเนื้อหาข้อความของเอกสารใน Postgres พร้อมสำหรับการค้นหา\n",
        "\n",
        "ในตัวอย่างนี้ เราใช้อาร์กิวเมนต์`ids`แบบตัวเลือกเพื่อกำหนดตัวระบุให้กับเอกสารแต่ละฉบับ ซึ่งช่วยให้เราสามารถอัปเดตหรือลบเอกสารเหล่านั้นในภายหลังได้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5wObvxFPWSN"
      },
      "outputs": [],
      "source": [
        "db.delete(ids=[str(id) for id in [2]]) # Convert the ids to strings using a list comprehension.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbAy3DrbQ4DG"
      },
      "source": [
        "นี่คือการลบเอกสารที่เราแทรกไว้ก่อนหน้านี้โดยใช้ id 2 ตอนนี้มาดูวิธีการทำสิ่งนี้ในเชิงระบบมากขึ้น\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lljWKOxOREH-"
      },
      "source": [
        "# Tracking Changes to your Documents\n",
        "\n",
        "**การติดตามการเปลี่ยนแปลงเอกสารของคุณ**\n",
        "\n",
        "หนึ่งในความท้าทายหลักในการทำงานกับ Vector Store คือการทำงานกับข้อมูลที่เปลี่ยนแปลงเป็นประจำ เนื่องจากการเปลี่ยนแปลงหมายถึงการจัดทำดัชนีใหม่ และการจัดทำดัชนีใหม่สามารถนำไปสู่การคำนวณการฝังข้อมูลซ้ำซ้อนที่มีค่าใช้จ่ายสูงและการทำซ้ำของเนื้อหาที่มีอยู่ก่อนแล้ว\n",
        "\n",
        "โชคดีที่ LangChain มี API การจัดทำดัชนีเพื่อให้ง่ายต่อการทำให้เอกสารของคุณตรงกับ Vector Store API นี้ใช้คลาส (`RecordManager`) เพื่อติดตามการเขียนเอกสารลงใน Vector Store เมื่อจัดทำดัชนีเนื้อหา แฮชจะถูกคำนวณสำหรับแต่ละเอกสาร และข้อมูลต่อไปนี้จะถูกจัดเก็บใน`RecordManager`:\n",
        "\n",
        "- แฮชเอกสาร (แฮชของทั้งเนื้อหาหน้าและเมตาดาต้า)\n",
        "- เวลาเขียน\n",
        "- รหัสแหล่งที่มา (เอกสารแต่ละฉบับควรมีข้อมูลในเมตาดาต้าเพื่อกำหนดแหล่งที่มาขั้นสูงสุดของเอกสารนี้)\n",
        "\n",
        "นอกจากนี้ API การจัดทำดัชนียังมีโหมดการล้างข้อมูลเพื่อช่วยคุณตัดสินใจว่าจะลบเอกสารที่มีอยู่ก่อนหน้านี้ใน Vector Store อย่างไร ตัวอย่างเช่น หากคุณได้ทำการเปลี่ยนแปลงวิธีการประมวลผลเอกสารก่อนการแทรกหรือแหล่งที่มาของเอกสารได้เปลี่ยนแปลง คุณอาจต้องการลบเอกสารที่มีอยู่ก่อนหน้านี้ทั้งหมดที่ มาจากแหล่งที่มาเดียวกับเอกสารใหม่ที่กำลังถูกจัดทำดัชนี หากมีการลบแหล่งที่มาของเอกสารบางส่วน คุณจะต้องลบเอกสารที่มีอยู่ทั้งหมดใน Vector Store และแทนที่ด้วยเอกสารที่จัดทำดัชนีใหม่\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-qolD_jRVoL"
      },
      "source": [
        "โหมดต่างๆ ดังนี้:\n",
        "\n",
        "- โหมด`None`ไม่ทำการล้างข้อมูลอัตโนมัติใดๆ อนุญาตให้ผู้ใช้ทำการล้างข้อมูลเนื้อหาเก่าด้วยตนเอง\n",
        "\n",
        "- โหมด`Incremental`และ`full`จะลบเวอร์ชันก่อนหน้าของเนื้อหาหากเนื้อหาของแหล่งที่มาของเอกสารหรือเอกสารที่ได้มาเปลี่ยนแปลง\n",
        "\n",
        "- โหมด`full`จะลบเอกสารใดๆ ที่ไม่รวมอยู่ในเอกสารที่กำลังถูกจัดทำดัชนีเพิ่มเติม\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FkS_PN7Rhqa"
      },
      "source": [
        "นี่คือตัวอย่างการใช้ API การจัดทำดัชนีกับฐานข้อมูล Postgres ที่ตั้งค่าเป็นตัวจัดการบันทึก เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ith1bDEiQZK4",
        "outputId": "31b5b710-740a-4ef0-bee7-0558cd8c9671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.indexes import SQLRecordManager, index\n",
        "\n",
        "namespace = \"my_documents\"  # Choose a descriptive name for your documents\n",
        "\n",
        "record_manager = SQLRecordManager(\n",
        "    namespace,\n",
        "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
        ")\n",
        "\n",
        "# Create the schema if it doesn't exist\n",
        "record_manager.create_schema()\n",
        "\n",
        "# Replace with your actual document content and metadata\n",
        "doc1Updated = Document(page_content=\"Content of doc1Updated\", metadata={\"source\": \"source1\"})\n",
        "doc2 = Document(page_content=\"Content of doc2\", metadata={\"source\": \"source2\"})\n",
        "\n",
        "#'db' is your PGVector instance from previous cells\n",
        "vectorstore = db\n",
        "\n",
        "index(\n",
        "    [doc1Updated, doc2],\n",
        "    record_manager,\n",
        "    vectorstore,\n",
        "    cleanup='incremental',\n",
        "    source_id_key= 'source'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4uriELiUCwC",
        "outputId": "3b0bedaa-fcfa-448e-87d2-e8380e23d660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='7e9e8002-0bf4-5eb1-a694-ca72de669df3', metadata={'source': 'source1'}, page_content='Content of doc1Updated'),\n",
              " Document(id='dc35e004-495f-58d4-8c5f-32d73a512c66', metadata={'source': 'source2'}, page_content='Content of doc2'),\n",
              " Document(id='83261a76-69ff-476d-bfff-2c8040dc58f7', metadata={'source': './docs/test.txt'}, page_content='This is test file'),\n",
              " Document(id='d8223846-5361-458c-bb7f-2206a6de476b', metadata={'source': './docs/test.txt'}, page_content='This is test file')]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.similarity_search(\"doc1Updated\", k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v03QtWy7TeSR"
      },
      "source": [
        "อันดับแรก คุณสร้างตัวจัดการบันทึก ซึ่งติดตามว่ามีการจัดทำดัชนีเอกสารใดไว้ก่อนหน้านี้ จากนั้นคุณใช้ฟังก์ชัน index เพื่อซิงโครไนซ์ Vector Store ของคุณกับรายการเอกสารใหม่ ในตัวอย่างนี้ เราใช้โหมด incremental ดังนั้น เอกสารใด ๆ ที่มี id เดียวกันกับเอกสารก่อนหน้านี้จะถูกแทนที่ด้วยเวอร์ชันใหม่\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awhY5VJyVOOz"
      },
      "source": [
        "# Summary\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้\n",
        "คุณได้เรียนรู้วิธีเตรียมและเตรียมข้อมูลเอกสารของคุณสำหรับแอปพลิเคชัน LLM โดยใช้โมดูลต่างๆ ของ LangChain ตัวโหลดเอกสารช่วยให้คุณสามารถแยกข้อความจากแหล่งข้อมูลของคุณ ตัวแบ่งข้อความช่วยให้คุณแบ่งเอกสารของคุณออกเป็นชิ้นส่วนที่มีความหมายคล้ายคลึงกัน และโมเดลการฝังข้อมูลแปลงข้อความของคุณเป็นการแสดงข้อมูลแบบเวกเตอร์ของความหมาย สุดท้าย Vector Store ช่วยให้คุณดำเนินการ CRUD กับการฝังข้อมูลเหล่านี้ควบคู่ไปกับการคำนวณที่ซับซ้อนเพื่อคำนวณชิ้นส่วนข้อความที่คล้ายคลึงกันทางความหมาย\n",
        "\n",
        "**ในบทต่อไป คุณจะได้เรียนรู้วิธีดึงข้อมูลชิ้นส่วนเอกสารที่คล้ายคลึงกันมากที่สุดอย่างมีประสิทธิภาพจาก Vector Store ของคุณโดยยึดตามคำถามของคุณ จัดเตรียมเป็นบริบทที่โมเดลสามารถเห็นได้ และจากนั้นสร้างผลลัพธ์ที่ถูกต้อง**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
