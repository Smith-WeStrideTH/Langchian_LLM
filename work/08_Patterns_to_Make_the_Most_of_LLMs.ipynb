{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OhSm1wNcgtS3"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ks3N66EnS8wl"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pprint import pprint\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJTAyZR3S9-z",
        "outputId": "cba1e510-9255-4220-fda2-909645c4dc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')\n",
        "\n",
        "# Get the API key from the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was found\n",
        "if api_key:\n",
        "  print(\"API Key loaded successfully.\")\n",
        "else:\n",
        "  print(\"API Key not found in the environment variables.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDzrGIAKzXL"
      },
      "source": [
        "# Chapter 8. Patterns to Make the Most of LLMs\n",
        "\n",
        "LLM ในปัจจุบันมีข้อจำกัดที่สำคัญ แต่ไม่ได้หมายความว่าแอปพลิเคชัน LLM ในฝันของคุณเป็นไปไม่ได้ที่จะสร้าง ประสบการณ์ที่คุณออกแบบสำหรับผู้ใช้ของแอปพลิเคชันของคุณจำเป็นต้องทำงานรอบๆ และในอุดมคติจะทำงานร่วมกับข้อจำกัดเหล่านั้น\n",
        "\n",
        "**บทที่ 5 ได้กล่าวถึงการแลกเปลี่ยนที่สำคัญที่เราเผชิญเมื่อสร้างแอปพลิเคชัน LLM**:\n",
        "\n",
        "- การแลกเปลี่ยนระหว่างตัวแทน (ความสามารถในการดำเนินการโดยอิสระ) และความน่าเชื่อถือ (ระดับที่เราสามารถไว้วางใจผลลัพธ์ของมัน) โดยสัญชาตญาณ แอปพลิเคชัน LLM ใดๆ ก็ตามจะมีประโยชน์กับเรามากกว่า หากดำเนินการเพิ่มเติมโดยไม่ต้องมีส่วนร่วมจากเรา แต่ถ้าเราปล่อยให้ตัวแทนไปไกลเกินไป แอปพลิเคชันจะหลีกเลี่ยงไม่ได้ที่จะทำสิ่งที่เราไม่ต้องการให้ทำ\n",
        "\n",
        "รูปนี้ แสดงให้เห็นถึงการแลกเปลี่ยนนี้\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure8-1.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbSelUY9MFiy"
      },
      "source": [
        "เพื่อยืมแนวคิดจากสาขาอื่นๆ *(In finance, the efficient frontier in portfolio optimization; in economics, a production-possibility frontier; in engineering, the Pareto front.)* เราสามารถมองเห็นการแลกเปลี่ยนเป็นแนวเขตแดน - จุดทั้งหมดบนเส้นโค้งนี้คือสถาปัตยกรรม LLM ที่เหมาะสมที่สุดสำหรับแอปพลิเคชันบางอย่าง ทำเครื่องหมายตัวเลือกที่แตกต่างกันระหว่างตัวแทนและความน่าเชื่อถือ (ดูบทที่ 5 สำหรับภาพรวมของสถาปัตยกรรมแอปพลิเคชัน LLM ที่แตกต่างกัน) ตัวอย่างเช่น สังเกตว่าสถาปัตยกรรมเชนมีตัวแทนค่อนข้างต่ำ แต่มีความน่าเชื่อถือสูงกว่า ในขณะที่สถาปัตยกรรมตัวแทนมีตัวแทนสูงกว่าแต่มีความน่าเชื่อถือต่ำกว่า\n",
        "\n",
        "มาสัมผัสกันโดยย่อเกี่ยวกับวัตถุประสงค์เพิ่มเติม (แต่ยังคงสำคัญ) อีกหลายประการที่คุณอาจต้องการให้แอปพลิเคชัน LLM ของคุณมี แอปพลิเคชัน LLM แต่ละตัวจะถูกออกแบบมาสำหรับการผสมผสานวัตถุประสงค์อย่างน้อยหนึ่งอย่างที่แตกต่างกัน:\n",
        "\n",
        "- **เวลาแฝง (Latency)**\n",
        "ลดเวลาในการรับคำตอบขั้นสุดท้ายให้น้อยที่สุด\n",
        "\n",
        "- **ภูมิภาคปกครองตนเอง (Autonomy)**\n",
        "ลดการหยุดชะงักสำหรับอินพุตของมนุษย์\n",
        "\n",
        "- **ความแปรปรวน (Variance)**\n",
        "ลดความแปรปรวนระหว่างการเรียกใช้งาน\n",
        "\n",
        "นี่ไม่ได้มีจุดประสงค์เพื่อเป็นรายการที่ครอบคลุมของวัตถุประสงค์ที่เป็นไปได้ทั้งหมด แต่เป็นเพียงตัวอย่างของการแลกเปลี่ยนที่คุณเผชิญเมื่อสร้างแอปพลิเคชันของคุณ แต่ละวัตถุประสงค์จะทำให้วัตถุประสงค์อื่นๆ เป็นโมฆะหากได้รับน้ำหนักเต็มที่ (ตัวอย่างเช่น แอปพลิเคชันที่มีเวลาแฝงขั้นต่ำคือแอปพลิเคชันที่ไม่ได้ทำอะไรเลย) ดังรูปที่แสดงให้เห็นนี้\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure8-2.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7rxn6vKNjHF"
      },
      "source": [
        "สิ่งที่เราต้องการจริงๆ ในฐานะนักพัฒนาแอปพลิเคชันคือการขยายขอบเขตของความสามารถ\n",
        "\n",
        "\"สำหรับระดับความน่าเชื่อถือที่เท่ากัน เราต้องการให้ระบบมีความสามารถในการทำงานที่หลากหลายมากขึ้น\"\n",
        "และ \"สำหรับระดับความสามารถในการทำงานที่เท่ากัน เราต้องการให้ระบบมีความน่าเชื่อถือมากขึ้น\"\n",
        "**บทนี้กล่าวถึงเทคนิคต่างๆ ที่คุณสามารถใช้เพื่อให้บรรลุเป้าหมายนี้:**\n",
        "\n",
        "- **เอาต์พุตแบบสตรีม/กลางทาง (Streaming/intermediate output):**\n",
        "การรับรู้ถึงความคืบหน้าหรือได้รับผลลัพธ์บางส่วนระหว่างการประมวลผลจะช่วยให้ผู้ใช้รู้สึกสบายใจมากขึ้นแม้ว่าระบบจะใช้เวลานานในการตอบสนอง\n",
        "\n",
        "- **เอาต์พุตที่มีโครงสร้าง (Structured output):**\n",
        "การกำหนดรูปแบบเอาต์พุตที่ชัดเจน เช่น JSON หรือ XML จะช่วยให้ LLM ผลิตผลลัพธ์ที่สอดคล้องกับความคาดหวังของเราได้มากขึ้น\n",
        "\n",
        "- **การมีส่วนร่วมของมนุษย์ (Human in the loop):**\n",
        "การอนุญาตให้มนุษย์เข้ามามีส่วนร่วมในกระบวนการทำงานของระบบ เช่น การหยุดชั่วคราว การอนุมัติผลลัพธ์ การแก้ไข หรือการยกเลิกคำสั่ง จะช่วยเพิ่มความยืดหยุ่นและความน่าเชื่อถือของระบบ\n",
        "\n",
        "- **โหมดข้อความทับซ้อน (Double texting modes):**\n",
        "หากแอปพลิเคชัน LLM ใช้เวลานานในการตอบสนอง ผู้ใช้มักจะส่งคำสั่งใหม่เข้ามาโดยไม่รอคำตอบจากคำสั่งก่อนหน้า ดังนั้น ระบบควรออกแบบให้สามารถรับมือกับสถานการณ์นี้ได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD0_0E8iSTPM"
      },
      "source": [
        "# Structured Output\n",
        "\n",
        "\n",
        "บ่อยครั้งที่จำเป็นอย่างยิ่งที่จะต้องให้ LLM ส่งคืนเอาต์พุตที่มีโครงสร้าง อาจเป็นเพราะการใช้งานเอาต์พุตนั้นต่อไปต้องการข้อมูลในรูปแบบเฉพาะ (เช่น การกำหนดชื่อและชนิดข้อมูลของฟิลด์ต่างๆ ในเอาต์พุต) หรือเพื่อลดความแปรปรวนของผลลัพธ์ที่อาจเกิดขึ้นหากปล่อยให้ LLM สร้างเอาต์พุตในรูปแบบข้อความอิสระ\n",
        "\n",
        "มีกลยุทธ์ที่แตกต่างกันหลายอย่างที่คุณสามารถใช้ได้กับ LLM ที่แตกต่างกัน:\n",
        "\n",
        "- **การเขียนพรอมต์:**\n",
        "วิธีนี้เกี่ยวข้องกับการขอให้ LLM (อย่างสุภาพ) ส่งคืนเอาต์พุตในรูปแบบที่ต้องการ (เช่น JSON, XML หรือ CSV)\n",
        "ข้อดีของการเขียนพรอมต์คือสามารถใช้ได้กับ LLM เกือบทุกตัว\n",
        "ข้อเสียคือ การเขียนพรอมต์เพียงอย่างเดียวอาจไม่เพียงพอที่จะรับประกันว่า LLM จะส่งคืนเอาต์พุตในรูปแบบที่ต้องการเสมอไป\n",
        "\n",
        "- **การเรียกใช้งานเครื่องมือ:**\n",
        "วิธีนี้ใช้ได้กับ LLM ที่ได้รับการปรับแต่งให้เลือกจากรายการรูปแบบเอาต์พุตที่เป็นไปได้ และสร้างผลลัพธ์ที่สอดคล้องกับรูปแบบที่เลือก\n",
        "โดยทั่วไปจะเกี่ยวข้องกับการเขียน ชื่อ เพื่อระบุรูปแบบเอาต์พุต คำอธิบาย เพื่อช่วยให้ LLM ตัดสินใจว่าควรเลือกใช้รูปแบบใด และรูปแบบของเอาต์พุตที่ต้องการ (โดยทั่วไปจะอยู่ในรูปแบบ JSONSchema) สำหรับแต่ละรูปแบบเอาต์พุตที่เป็นไปได้\n",
        "\n",
        "- **โหมด JSON:**\n",
        "นี่คือโหมดที่มีอยู่ใน LLM บางตัว (เช่น โมเดล OpenAI รุ่นล่าสุด) ที่บังคับให้เอาต์พุตเป็นเอกสาร JSON ที่ถูกต้อง\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRL1QCPhSvkq"
      },
      "source": [
        "โมเดลต่างๆอาจรองรับรูปแบบที่แตกต่างกันเหล่านี้ด้วยพารามิเตอร์ที่แตกต่างกันเล็กน้อย เพื่อให้ง่ายต่อการรับเอาต์พุตที่มีโครงสร้างจาก LLM โมเดล LangChain ได้ใช้ส่วนต่อประสานทั่วไป ซึ่งเป็นเมธอดที่เรียกว่า .`with_structured_output` โดยการเรียกใช้เมธอดนี้ และส่งผ่าน JSON schema หรือโมเดล Pydantic (ใน Python) หรือ Zod (ใน JS) โมเดลจะเพิ่มพารามิเตอร์ของโมเดลและตัวแยกวิเคราะห์เอาต์พุตที่จำเป็นเพื่อสร้างและส่งคืนเอาต์พุตที่มีโครงสร้าง เมื่อโมเดลใดโมเดลหนึ่งใช้กลยุทธ์ข้างต้นมากกว่าหนึ่งวิธี คุณสามารถกำหนดค่าว่าจะใช้วิธีใด\n",
        "\n",
        "มาสร้างรูปแบบที่จะใช้กัน:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XYXsgjunG1bU"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-F53IlDTBeO"
      },
      "source": [
        "**สังเกตว่าเราใส่ใจในการเพิ่มคำอธิบายสำหรับแต่ละฟิลด์**\n",
        "\n",
        "นี่เป็นสิ่งสำคัญ เพราะร่วมกับชื่อของฟิลด์ ข้อมูลนี้จะเป็นสิ่งที่ LLM ใช้ในการตัดสินใจว่าส่วนใดของเอาต์พุตควรอยู่ในแต่ละฟิลด์\n",
        "เราสามารถกำหนดรูปแบบด้วยสัญกรณ์ JSONSchema ดิบได้ ซึ่งจะมีลักษณะดังนี้:\n",
        "```\n",
        "{'properties': {'setup': {'description': 'The setup of the joke',\n",
        "   'title': 'Setup',\n",
        "   'type': 'string'},\n",
        "  'punchline': {'description': 'The punchline to the joke',\n",
        "   'title': 'Punchline',\n",
        "   'type': 'string'}},\n",
        " 'required': ['setup', 'punchline'],\n",
        " 'title': 'Joke',\n",
        " 'type': 'object'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4htIdTgcTTPZ"
      },
      "source": [
        "และตอนนี้มาดูการสร้างเอาต์พุตที่สอดคล้องกับ schema นี้โดย LLM\n",
        "\n",
        "| Feature        | Structured Outputs | JSON mode | Adheres to schema | Compatible models                                    | Enabling                                        |\n",
        "|----------------|--------------------|-----------|--------------------|----------------------------------------------------|-------------------------------------------------|\n",
        "| JSON mode      | Yes               | Yes        | Yes (see supported schemas) | • gpt-4o-mini<br>• gpt-4o-2024-08-06 and later<br>• gpt-3.5-turbo<br>• gpt-4-*<br>• gpt-4o-* | `response_format: { type: \"json_schema\", json_schema: {\"strict\": true, \"schema\": ...} }` |\n",
        "| Structured Output | Yes               | Yes        | Yes (see supported schemas) | • gpt-4o-mini<br>• gpt-4o-2024-08-06 and later<br>• gpt-3.5-turbo<br>• gpt-4-*<br>• gpt-4o-* | `response_format: { type: \"json_object\" }` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lztv_ASWW8K",
        "outputId": "7fe1e326-38ca-4b28-f645-51605423a91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!'\n",
            "Why was the cat sitting on the computer?\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "model = model.with_structured_output(Joke)\n",
        "\n",
        "result = model.invoke(\"Tell me a joke about cats\")\n",
        "print(result)\n",
        "print(result.setup)\n",
        "print(result.punchline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TR5ije5XQZP"
      },
      "source": [
        "| Feature | ChatOpenAI | OpenAI |\n",
        "|---|---|---|\n",
        "| **Purpose** | Designed for conversational models (e.g., gpt-3.5-turbo, gpt-4) | Designed for older models (e.g., text-davinci-003) |\n",
        "| **API Endpoint** | chat/completions | completions |\n",
        "| **Input** | List of messages (conversation history) | Single string |\n",
        "| **Output** | Single message | Single string |\n",
        "| **Key Use Cases** | Conversations, chatbots, maintaining context | Simple input/output interactions |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1dLcNOTS1w",
        "outputId": "ad21f9e1-deea-4f23-87cd-7daa1ced6e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!'\n",
            "Why was the cat sitting on the computer?\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Define your data structure\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "\n",
        "# Generate a response and parse it into your structure\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and harmless AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke about cats\"}\n",
        "    ],\n",
        "    response_format=Joke,  # Pass your Pydantic model\n",
        ")\n",
        "\n",
        "# Extract the parsed joke\n",
        "joke = completion.choices[0].message.parsed\n",
        "\n",
        "# Print the joke\n",
        "print(joke)\n",
        "print(joke.setup)\n",
        "print(joke.punchline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NGKHGGNZiXg"
      },
      "source": [
        "```python\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "model = model.with_structured_output(Joke)\n",
        "result = model.invoke(\"Tell me a joke about cats\")\n",
        "```\n",
        "**มีสองสามสิ่งที่ควรสังเกต:**\n",
        "\n",
        "- เราสร้างอินสแตนซ์ของโมเดลตามปกติ โดยระบุชื่อโมเดลที่จะใช้และพารามิเตอร์อื่นๆ\n",
        "- (Low Temperature)อุณหภูมิต่ำมักจะเหมาะกับการสร้างเอาต์พุตที่มีโครงสร้าง เนื่องจากจะช่วยลดโอกาสที่ LLM จะสร้างเอาต์พุตที่ไม่ถูกต้องซึ่งไม่เป็นไปตาม Schema\n",
        "- หลังจากนั้น เราจะแนบ Schema เข้ากับโมเดล ซึ่งจะส่งคืนอ็อบเจ็กต์ใหม่ ซึ่งจะสร้างเอาต์พุตที่ตรงกับ Schema ที่ให้ไว้ เมื่อคุณส่งอ็อบเจ็กต์ Pydantic หรือ Zod สำหรับ Schema สิ่งนี้จะถูกใช้สำหรับการตรวจสอบความถูกต้องด้วย นั่นคือ หาก LLM สร้างเอาต์พุตที่ไม่เป็นไปตามนั้น ข้อผิดพลาดในการตรวจสอบความถูกต้องจะถูกส่งคืนให้คุณแทนที่จะเป็นเอาต์พุตที่ล้มเหลว\n",
        "- สุดท้าย เราเรียกใช้โมเดลด้วยอินพุต (แบบอิสระ) ของเรา และรับเอาต์พุตที่ตรงกับโครงสร้างที่เราต้องการ\n",
        "\n",
        "รูปแบบของการใช้อินพุตที่มีโครงสร้างนี้มีประโยชน์มากทั้งในฐานะเครื่องมือแบบสแตนด์อโลน ดังที่คุณเห็นด้านบน และเป็นส่วนหนึ่งของแอปพลิเคชันขนาดใหญ่ ตัวอย่างเช่น กลับไปดูบทที่ 5 ซึ่งเราใช้ความสามารถนี้เพื่อใช้ขั้นตอนการกำหนดเส้นทางของสถาปัตยกรรมเราเตอร์"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpN6QxDza0Lx"
      },
      "source": [
        "# Intermediate Output\n",
        "**เอาต์พุตระดับกลาง**\n",
        "\n",
        "ยิ่งสถาปัตยกรรม LLM ของคุณซับซ้อนมากขึ้นเท่าไหร่ ก็มีแนวโน้มที่จะใช้เวลาในการประมวลผลนานขึ้นเท่านั้น หากคุณลองนึกถึงไดอะแกรมสถาปัตยกรรมในบทที่ 5 และ 6 ทุกครั้งที่คุณเห็นหลายขั้นตอน (หรือโหนด) ที่เชื่อมต่อกันเป็นลำดับหรือเป็นวง นั่นเป็นตัวบ่งชี้ว่าเวลาที่ใช้สำหรับการเรียกใช้งานทั้งหมดกำลังเพิ่มขึ้น\n",
        "\n",
        "ความหน่วงที่เพิ่มขึ้นนี้ หากไม่ได้รับการแก้ไข อาจเป็นอุปสรรคต่อการนำแอปพลิเคชัน LLM ไปใช้โดยผู้ใช้ เนื่องจากผู้ใช้ส่วนใหญ่คาดหวังว่าแอปพลิเคชันคอมพิวเตอร์จะสร้างเอาต์พุตภายในไม่กี่วินาที มีกลยุทธ์หลายประการที่จะทำให้ความหน่วงที่สูงขึ้นนั้นยอมรับได้มากขึ้น **แต่ทั้งหมดอยู่ภายใต้แนวคิดของเอาต์พุตแบบสตรีมมิ่ง**\n",
        "- นั่นคือการรับเอาต์พุตจากแอปพลิเคชันในขณะที่แอปพลิเคชันยังคงทำงานอยู่\n",
        "\n",
        "สำหรับส่วนนี้ เราจะใช้สถาปัตยกรรมสุดท้ายที่อธิบายไว้ในหัวข้อ\n",
        "**“การจัดการกับเครื่องมือจำนวนมาก”** ในบทที่ 6 โปรดกลับไปดูบทนั้นสำหรับโค้ดแบบเต็ม\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6XpxLH0eCnq"
      },
      "source": [
        "ในการสร้างเอาต์พุตระดับกลางด้วย LangGraph สิ่งที่คุณต้องทำคือเรียกกราฟด้วยเมธอด `stream` ซึ่งจะให้ผลลัพธ์ของแต่ละโหนดทันทีที่แต่ละโหนดเสร็จสิ้น ลองมาดูว่ามันมีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vZ7QhdQnenX1"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from typing import Annotated, TypedDict\n",
        "\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "@tool\n",
        "def calculator(query: str) -> str:\n",
        "    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\n",
        "    return ast.literal_eval(query)\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "tools = [search, calculator]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "model = ChatOpenAI(temperature=0.1)\n",
        "\n",
        "tools_retriever = InMemoryVectorStore.from_documents(\n",
        "    [Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
        "    embeddings,\n",
        ").as_retriever()\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    selected_tools: list[str]\n",
        "\n",
        "def model_node(state: State) -> State:\n",
        "    selected_tools = [\n",
        "        tool for tool in tools if tool.name in state[\"selected_tools\"]\n",
        "    ]\n",
        "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
        "    return {\"messages\": res}\n",
        "\n",
        "def select_tools(state: State) -> State:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    tool_docs = tools_retriever.invoke(query)\n",
        "    return {\"selected_tools\": [doc.metadata[\"name\"] for doc in tool_docs]}\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"select_tools\", select_tools)\n",
        "builder.add_node(\"model\", model_node)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "builder.add_edge(START, \"select_tools\")\n",
        "builder.add_edge(\"select_tools\", \"model\")\n",
        "builder.add_conditional_edges(\"model\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"model\")\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "TKog4nV4esSe",
        "outputId": "b2c94f4f-ef85-42b3-87a4-0a8d4cf03314"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "image_bytes = graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
        "\n",
        "display(Image(data=image_bytes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSICHQ0ZUCO1",
        "outputId": "d56d61b3-e656-49dc-e56f-62606797b121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'select_tools': {'selected_tools': ['duckduckgo_search', 'calculator']}}\n",
            "{'model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZZiivf4yyBJZlvjpXLNBZgup', 'function': {'arguments': '{\"query\":\"30th president of the United States age at death\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 118, 'total_tokens': 145, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-91ad6941-5dab-4ba6-8bd0-542cb1527498-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': '30th president of the United States age at death'}, 'id': 'call_ZZiivf4yyBJZlvjpXLNBZgup', 'type': 'tool_call'}], usage_metadata={'input_tokens': 118, 'output_tokens': 27, 'total_tokens': 145, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
            "{'tools': {'messages': [ToolMessage(content=\"Calvin Coolidge (born John Calvin Coolidge Jr. [1] / ˈ k uː l ɪ dʒ / KOOL-ij; July 4, 1872 - January 5, 1933) was the 30th president of the United States, serving from 1923 to 1929.A Republican lawyer from Massachusetts, he previously served as the 29th vice president from 1921 to 1923 under President Warren G. Harding, and as the 48th governor of Massachusetts from 1919 to 1921. The White House, official residence of the president of the United States, in July 2008. The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of ... Calvin Coolidge was the 30th president of the United States (1923-29). Coolidge acceded to the presidency after the death in office of Warren G. Harding, just as the Harding scandals were coming to light. He restored integrity to the executive branch while continuing Harding's conservative pro-business policies. The president who died at the youngest age was John F. Kennedy, ... PresidentsUSA.net, Length of life and cause of death of United States Presidents from 1799 to 2024 Statista, https://www ... The first table below charts the age of each president of the United States at the time of their presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated through January 24, 2025.\", name='duckduckgo_search', id='0c50cbdf-cbbb-4ba5-81a7-68b0cbdcc319', tool_call_id='call_ZZiivf4yyBJZlvjpXLNBZgup')]}}\n",
            "{'model': {'messages': AIMessage(content=\"Calvin Coolidge, the 30th president of the United States, was born on July 4, 1872, and died on January 5, 1933. To calculate his age at the time of his death, we need to determine the difference in years between these two dates. Let's calculate that.\", additional_kwargs={'tool_calls': [{'id': 'call_jp3h7SFV9OjPBzRq5J1hPEKJ', 'function': {'arguments': '{\"query\": \"1933-1872\"}', 'name': 'calculator'}, 'type': 'function'}, {'id': 'call_JMt7dVQYetvLbXDJQNVPFLqX', 'function': {'arguments': '{\"query\": \"1933-1872-1\"}', 'name': 'calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 536, 'total_tokens': 658, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-437d59c7-4876-4fb1-b597-7d76f6f047f5-0', tool_calls=[{'name': 'calculator', 'args': {'query': '1933-1872'}, 'id': 'call_jp3h7SFV9OjPBzRq5J1hPEKJ', 'type': 'tool_call'}, {'name': 'calculator', 'args': {'query': '1933-1872-1'}, 'id': 'call_JMt7dVQYetvLbXDJQNVPFLqX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 536, 'output_tokens': 122, 'total_tokens': 658, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
            "{'tools': {'messages': [ToolMessage(content=\"Error: ValueError('malformed node or string on line 1: <ast.BinOp object at 0x000001D94470B4F0>')\\n Please fix your mistakes.\", name='calculator', id='756e824d-f2c2-457e-963c-cb10006f69d1', tool_call_id='call_jp3h7SFV9OjPBzRq5J1hPEKJ', status='error'), ToolMessage(content=\"Error: ValueError('malformed node or string on line 1: <ast.BinOp object at 0x000001D94470B7C0>')\\n Please fix your mistakes.\", name='calculator', id='3dcaebd8-5ab2-41db-ac40-dec36618c48a', tool_call_id='call_JMt7dVQYetvLbXDJQNVPFLqX', status='error')]}}\n",
            "{'model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kI2oRazaWvvta33j1axOZHnb', 'function': {'arguments': '{\"query\":\"1933-1872\"}', 'name': 'calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 753, 'total_tokens': 772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b4f86b29-e0a5-441e-b7a4-a310b7e02540-0', tool_calls=[{'name': 'calculator', 'args': {'query': '1933-1872'}, 'id': 'call_kI2oRazaWvvta33j1axOZHnb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 753, 'output_tokens': 19, 'total_tokens': 772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
            "{'tools': {'messages': [ToolMessage(content=\"Error: ValueError('malformed node or string on line 1: <ast.BinOp object at 0x000001D94472BB50>')\\n Please fix your mistakes.\", name='calculator', id='8f8fc254-19e2-4aab-b386-d467b96fc4f6', tool_call_id='call_kI2oRazaWvvta33j1axOZHnb', status='error')]}}\n",
            "{'model': {'messages': AIMessage(content='It seems there was an error in calculating the age of Calvin Coolidge at the time of his death. Let me try to calculate it again.', additional_kwargs={'tool_calls': [{'id': 'call_KOOBo7zrtZcu57oVW2DPEFk4', 'function': {'arguments': '{\"query\":\"1933 - 1872\"}', 'name': 'calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 815, 'total_tokens': 866, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c0206963-a86b-43fd-a57d-0f8cefa5d555-0', tool_calls=[{'name': 'calculator', 'args': {'query': '1933 - 1872'}, 'id': 'call_KOOBo7zrtZcu57oVW2DPEFk4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 815, 'output_tokens': 51, 'total_tokens': 866, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
            "{'tools': {'messages': [ToolMessage(content=\"Error: ValueError('malformed node or string on line 1: <ast.BinOp object at 0x000001D947B95810>')\\n Please fix your mistakes.\", name='calculator', id='1c73d487-49c6-4d9e-aae1-19d22b00adeb', tool_call_id='call_KOOBo7zrtZcu57oVW2DPEFk4', status='error')]}}\n",
            "{'model': {'messages': AIMessage(content='I apologize for the technical difficulties. Let me calculate the age of Calvin Coolidge at the time of his death manually.\\n\\nCalvin Coolidge was born on July 4, 1872, and he died on January 5, 1933. To calculate his age at the time of his death, we need to consider the years between these two dates.\\n\\n1872 to 1932 - 60 years\\n1932 to 1933 (up to January 5) - 1 year\\n\\nTherefore, Calvin Coolidge was 61 years old when he died.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 911, 'total_tokens': 1029, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-60cf3e56-8cb2-4924-b15b-a3cfcae56753-0', usage_metadata={'input_tokens': 911, 'output_tokens': 118, 'total_tokens': 1029, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "input = {\n",
        "  \"messages\": [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
        "  ]\n",
        "}\n",
        "for c in graph.stream(input, stream_mode='updates'):\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc8zUwxCe5a_"
      },
      "source": [
        "สังเกตว่ารายการเอาต์พุตแต่ละรายการเป็นพจนานุกรม โดยมีชื่อของโหนดที่ส่งออกมาเป็นคีย์ และเอาต์พุตของโหนดนั้นเป็นค่า สิ่งนี้ให้ข้อมูลสำคัญสองส่วนแก่คุณ:\n",
        "\n",
        "- แอปพลิเคชันอยู่ที่ใดในปัจจุบัน นั่นคือ หากคุณนึกถึงไดอะแกรมสถาปัตยกรรมที่แสดงในบทก่อนๆ เราอยู่ในส่วนใดของไดอะแกรมนั้นในปัจจุบัน\n",
        "- การอัปเดตแต่ละครั้งไปยังสถานะที่ใช้ร่วมกันของแอปพลิเคชัน ซึ่งรวมกันเป็นเอาต์พุตสุดท้ายของกราฟ\n",
        "\n",
        "นอกจากนี้ LangGraph ยังรองรับโหมดสตรีมเพิ่มเติม:\n",
        "\n",
        "- `updates` (การอัปเดต) นี่คือโหมดเริ่มต้น อธิบายไว้ข้างต้น\n",
        "- `values` (ค่า) โหมดนี้จะให้สถานะปัจจุบันของกราฟทุกครั้งที่สถานะเปลี่ยนแปลง นั่นคือหลังจากที่โหนดแต่ละชุดประมวลผลเสร็จสิ้น โหมดนี้มีประโยชน์เมื่อวิธีการแสดงเอาต์พุตของคุณต่อผู้ใช้ติดตามรูปร่างของสถานะกราฟอย่างใกล้ชิด\n",
        "- `debug` (การแก้ไขข้อบกพร่อง) โหมดนี้จะให้เหตุการณ์โดยละเอียดทุกครั้งที่มีบางอย่างเกิดขึ้นในกราฟของคุณ รวมถึง:\n",
        "\n",
        "  - เหตุการณ์ `checkpoint` (จุดตรวจสอบ) ทุกครั้งที่มีการบันทึกจุดตรวจสอบใหม่ของสถานะปัจจุบันลงในฐานข้อมูล\n",
        "  - เหตุการณ์ `task` (งาน) ที่ส่งออกมาเมื่อโหนดกำลังจะเริ่มทำงาน\n",
        "  - เหตุการณ์ `task_result` (ผลลัพธ์ของงาน) ที่ส่งออกมาเมื่อโหนดทำงานเสร็จสิ้น\n",
        "\n",
        "- สุดท้าย คุณสามารถรวมโหมดเหล่านี้ได้ ตัวอย่างเช่น การขอทั้ง updates และ values โดยการส่งรายการ\n",
        "\n",
        "คุณควบคุมโหมดสตรีมด้วยอาร์กิวเมนต์ `stream_mode` ไปยัง `stream()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hrONNT3gJ8T"
      },
      "source": [
        "# Streaming LLM Output Token-by-Token\n",
        "\n",
        "**การสตรีมเอาต์พุต LLM แบบโทเค็นต่อโทเค็น**\n",
        "\n",
        "บางครั้งคุณอาจต้องการรับเอาต์พุตแบบสตรีมมิ่งจากการเรียก LLM แต่ละครั้งภายในแอปพลิเคชัน LLM ขนาดใหญ่ของคุณ นี่อาจมีประโยชน์ ตัวอย่างเช่น เมื่อสร้างแชทบอทแบบโต้ตอบ ซึ่งคุณต้องการให้แสดงแต่ละคำทันทีที่ LLM สร้างขึ้น"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS3myX3dgjOv",
        "outputId": "1412aebf-c016-4d64-ad21-a20ff1ee745e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 30th president of the United States was Calvin Coolidge. He was born on July 4, 1872, and died on January 5, 1933. This means he was 60 years old at the time of his death."
          ]
        }
      ],
      "source": [
        "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
        "\n",
        "app = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Or any other OpenAI model you prefer\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
        "]\n",
        "\n",
        "# Initialize the callback handler\n",
        "handler = AsyncIteratorCallbackHandler()\n",
        "\n",
        "# Call the model with the handler, using messages directly\n",
        "output = app.astream_events(messages, version=\"v2\", callbacks=[handler])\n",
        "\n",
        "async def process_stream():\n",
        "    async for token in app.astream(messages):\n",
        "        print(token.content, end=\"\", flush=True)\n",
        "await process_stream()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkGRXU2jgIHD"
      },
      "source": [
        "โค้ดนี้แสดงวิธีการสตรีมเอาต์พุตจากโมเดลภาษาโดยใช้ฟังก์ชัน asynchronous ใน Python\n",
        "\n",
        "*   `async def process_stream():` บรรทัดนี้กำหนดฟังก์ชัน asynchronous ชื่อ `process_stream` ฟังก์ชัน asynchronous ช่วยให้คุณทำงานต่างๆ พร้อมกันได้โดยไม่บล็อกเธรดหลักของการประมวลผล ซึ่งมีประโยชน์สำหรับงานต่างๆ เช่น การสตรีมข้อมูล ซึ่งอาจใช้เวลา\n",
        "\n",
        "*   `async for token in app.astream(messages):` นี่คือหัวใจสำคัญของตรรกะการสตรีม\n",
        "\n",
        "    *   `app.astream(messages):` ส่วนนี้น่าจะเรียกใช้เมธอด asynchronous (`.astream()`) บนอ็อบเจ็กต์โมเดลภาษาของคุณ (`app`) เมธอดนี้เริ่มต้นสตรีมของโทเค็นที่สร้างโดยโมเดลเพื่อตอบสนองต่อข้อความที่ให้ไว้\n",
        "    *   `async for token in ...:` นี่คือลูป `for` แบบ asynchronous ที่วนซ้ำโทเค็นที่มาจากสตรีม โทเค็นแต่ละโทเค็นแสดงถึงข้อความส่วนหนึ่ง (คำหรือส่วนของคำ) ที่สร้างโดยโมเดล\n",
        "    *   `print(token.content, end=\"\", flush=True):` บรรทัดนี้พิมพ์เนื้อหาของแต่ละโทเค็นเมื่อมาถึง\n",
        "\n",
        "        *   `token.content:` ส่วนนี้เข้าถึงเนื้อหาข้อความจริงของโทเค็น\n",
        "        *   `end=\"\":` ส่วนนี้ป้องกันการพิมพ์อักขระขึ้นบรรทัดใหม่หลังโทเค็นแต่ละตัว เพื่อให้เอาต์พุตปรากฏเป็นสตรีมต่อเนื่อง\n",
        "        *   `flush=True:` ส่วนนี้ช่วยให้แน่ใจว่าเอาต์พุตจะแสดงบนคอนโซลทันที แม้ว่าบัฟเฟอร์เอาต์พุตจะไม่เต็ม สิ่งนี้สำคัญสำหรับการดูสตรีมแบบเรียลไทม์\n",
        "\n",
        "*   `await process_stream():` บรรทัดนี้เรียกใช้ฟังก์ชัน `process_stream` และรอให้เสร็จสมบูรณ์ เนื่องจาก `process_stream` เป็นฟังก์ชัน asynchronous การใช้ `await` ช่วยให้โปรแกรมทำงานอื่นๆ ต่อไปได้ในขณะที่กำลังประมวลผลสตรีม สิ่งสำคัญคือต้องใช้ `await` ที่นี่ มิฉะนั้นโปรแกรมอาจออกจากโปรแกรมก่อนที่สตรีมจะถูกใช้และพิมพ์จนหมด\n",
        "\n",
        "**สรุป:**\n",
        "\n",
        "โค้ดนี้ตั้งค่าสตรีมของเอาต์พุตข้อความจากโมเดลภาษาของคุณ (`app`) วนซ้ำโทเค็นแต่ละตัวในสตรีมขณะที่สร้างขึ้น และพิมพ์ไปยังคอนโซลทันที สร้างการแสดงผลแบบเรียลไทม์ของเอาต์พุตของโมเดล\n",
        "\n",
        "การใช้ `async` และ `await` ทำให้กระบวนการนี้ไม่บล็อก ช่วยให้โปรแกรมของคุณจัดการงานอื่นๆ ได้ในขณะที่รอให้สตรีมเสร็จสมบูรณ์ สิ่งนี้สำคัญสำหรับแอปพลิเคชันแบบโต้ตอบที่การตอบสนองมีความสำคัญ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdNzkbvhOwL"
      },
      "source": [
        "สิ่งนี้จะส่งออกแต่ละคำ (ในทางเทคนิคคือแต่ละโทเค็น) ทันทีที่ได้รับจาก LLM คุณสามารถดูรายละเอียดเพิ่มเติมเกี่ยวกับรูปแบบนี้ได้จาก [LangChain](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-nodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZOrTnzRhtAe"
      },
      "source": [
        "# Human-in-the-Loop Modalities\n",
        "**รูปแบบการมีส่วนร่วมของมนุษย์**\n",
        "\n",
        "ขณะที่เราก้าวขึ้นบันไดแห่งความเป็นอิสระ (หรือความสามารถในการดำเนินการด้วยตนเอง) เราพบว่าตัวเองต้องสูญเสียการควบคุม (หรือการกำกับดูแล) มากขึ้นเรื่อยๆ เพื่อแลกกับความสามารถ (หรือความเป็นอิสระ) รูปแบบสถานะที่ใช้ร่วมกันที่ใช้ใน LangGraph **(ดูบทที่ 5 สำหรับบทนำ)** ทำให้ง่ายต่อการสังเกต ขัดจังหวะ และแก้ไขแอปพลิเคชัน ทำให้สามารถใช้โหมด \"มนุษย์ในวงจร\" (human-in-the-loop) ที่หลากหลาย หรือวิธีการที่นักพัฒนา/ผู้ใช้ปลายทางของแอปพลิเคชันสามารถมีอิทธิพลต่อสิ่งที่ LLM กำลังทำอยู่\n",
        "\n",
        "สำหรับส่วนนี้ เราจะใช้อีกครั้งกับสถาปัตยกรรมสุดท้ายที่อธิบายไว้ใน **\"การจัดการกับเครื่องมือจำนวนมาก\" ในบทที่ 6** โปรดกลับไปดูบทนั้นสำหรับโค้ดแบบเต็ม สำหรับโหมดมนุษย์ในวงจรทั้งหมด เราต้องแนบตัวตรวจสอบจุด (checkpointer) เข้ากับกราฟก่อน โปรดดู **\"Adding Memory to StateGraph\" ในบทที่ 4**\n",
        "\n",
        "สำหรับรายละเอียดเพิ่มเติมเกี่ยวกับเรื่องนี้:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sFrnmON_gUMO"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = builder.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxr4o8fKii4o"
      },
      "source": [
        "**สิ่งนี้จะส่งคืนอินสแตนซ์ของกราฟที่จัดเก็บสถานะเมื่อสิ้นสุดแต่ละขั้นตอน**\n",
        "- ดังนั้นการเรียกใช้ทุกครั้งหลังจากครั้งแรกจะไม่เริ่มต้นจากศูนย์\n",
        "- ทุกครั้งที่กราฟถูกเรียกใช้ กราฟจะเริ่มต้นโดยใช้ตัวตรวจสอบจุด (checkpointer) เพื่อดึงสถานะที่บันทึกล่าสุด หากมีและรวมอินพุตใหม่กับสถานะก่อนหน้า\n",
        "- จากนั้นจึงดำเนินการโหนดแรก\n",
        "\n",
        "**นี่เป็นกุญแจสำคัญในการเปิดใช้งานรูปแบบการมีส่วนร่วมของมนุษย์ (human-in-the-loop modalities) ซึ่งทั้งหมดขึ้นอยู่กับการที่กราฟจดจำสถานะก่อนหน้า**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure8-3.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4CUoEnxjL6q"
      },
      "source": [
        "โหมดแรกคือ \"การขัดจังหวะ\" `(interrupt)` เป็นรูปแบบการควบคุมที่ง่ายที่สุด ผู้ใช้กำลังดูเอาต์พุตแบบสตรีมมิ่งของแอปพลิเคชันขณะที่กำลังสร้าง และขัดจังหวะด้วยตนเองเมื่อเห็นสมควร (ดูรูป) สถานะจะถูกบันทึก ณ ขั้นตอนสุดท้ายก่อนที่ผู้ใช้จะกดปุ่มขัดจังหวะ จากตรงนั้น ผู้ใช้สามารถเลือกที่จะ:\n",
        "\n",
        "- ดำเนินการต่อจากจุดนั้นเป็นต้นไป และการคำนวณจะดำเนินต่อไปราวกับว่าไม่มีการขัดจังหวะ `(ดู \"Resume\")`\n",
        "- ส่งอินพุตใหม่ไปยังแอปพลิเคชัน (เช่น ข้อความใหม่ในแชทบอท) ซึ่งจะยกเลิกขั้นตอนในอนาคตที่ค้างอยู่และเริ่มจัดการกับอินพุตใหม่ `(ดู \"Restart\")`\n",
        "- ไม่ทำอะไรเลย และไม่มีอะไรอื่นจะทำงาน\n",
        "\n",
        "มาดูวิธีการทำเช่นนี้ใน LangGraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AnszMt0dicZj"
      },
      "outputs": [],
      "source": [
        "import asyncio # asyncio library for asynchronous programming.\n",
        "from contextlib import aclosing # for ensuring resources are properly closed when an asynchronous context manager exits\n",
        "\n",
        "event = asyncio.Event()\n",
        "\n",
        "input = {\n",
        "  \"messages\": [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
        "  ]\n",
        "}\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "async with aclosing(graph.astream(input, config)) as stream:\n",
        "    async for chunk in stream:\n",
        "        if event.is_set():\n",
        "            break\n",
        "        else:\n",
        "            ... # do something with the output\n",
        "\n",
        "# Somewhere else in your application\n",
        "\n",
        "event.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4p4Am6MhWG6"
      },
      "source": [
        "**คำอธิบายโค้ดการขัดจังหวะการทำงานของ LangGraph**\n",
        "\n",
        "โค้ดนี้แสดงวิธีการตั้งค่า LangGraph ให้ทำงานแบบ asynchronous และมีกลไกในการขัดจังหวะการทำงานโดยการตั้งค่า event จากส่วนอื่นของแอปพลิเคชัน\n",
        "\n",
        "*   **สร้าง event:**\n",
        "\n",
        "    *   `event = asyncio.Event():` สร้างอ็อบเจ็กต์ `asyncio.Event` ซึ่งทำหน้าที่เป็นสัญญาณเพื่อควบคุมการไหลของการประมวลผล ในตอนแรก event จะยังไม่ถูกตั้งค่า\n",
        "\n",
        "*   **กำหนดอินพุตและการกำหนดค่า:**\n",
        "\n",
        "    *   `input:` พจนานุกรมที่ประกอบด้วยข้อความที่จะประมวลผลโดย LangGraph\n",
        "    *   `config:` พจนานุกรมที่ระบุ ID เธรดสำหรับการโต้ตอบนี้กับกราฟ ซึ่งอาจใช้สำหรับการติดตามหรือจัดการการโต้ตอบพร้อมกัน\n",
        "\n",
        "*   **รัน LangGraph แบบ Asynchronously:**\n",
        "\n",
        "    *   `async with aclosing(graph.astream(input, config)) as stream::` บรรทัดนี้เริ่มต้น context manager แบบ asynchronous เพื่อรัน LangGraph\n",
        "        *   `graph.astream:` เริ่มต้นการประมวลผลของ LangGraph ด้วยอินพุตและการกำหนดค่าที่กำหนด ซึ่งน่าจะส่งคืนสตรีมแบบ asynchronous ของส่วนเอาต์พุต\n",
        "        *   `aclosing:` ช่วยให้แน่ใจว่า `stream` ถูกปิดอย่างถูกต้องเมื่อบล็อก `async with` สิ้นสุดลง แม้ว่าจะเกิดข้อยกเว้นก็ตาม\n",
        "    *   `async for chunk in stream::` ลูปนี้วนซ้ำส่วนเอาต์พุตแต่ละส่วนที่มาจาก LangGraph\n",
        "        *   `if event.is_set(): break:` ตรวจสอบว่า `event` ถูกตั้งค่าหรือไม่ ถ้าตั้งค่าแล้ว ลูปจะสิ้นสุดลง ซึ่งเป็นการขัดจังหวะการประมวลผลของ LangGraph\n",
        "        *   `else: ...:` หาก event ยังไม่ถูกตั้งค่า โค้ดภายในบล็อกนี้จะถูกดำเนินการเพื่อประมวลผลส่วนที่ได้รับ\n",
        "\n",
        "*   **ตั้งค่า event:**\n",
        "\n",
        "    *   `# Somewhere else in your application:` ความคิดเห็นนี้บ่งบอกว่าบรรทัดต่อไปนี้มีไว้สำหรับวางในส่วนอื่นของโปรแกรมของคุณ\n",
        "    *   `event.set():` ตั้งค่าอ็อบเจ็กต์ `event` นี่เป็นสัญญาณให้ลูป asynchronous หยุดประมวลผลสตรีม ซึ่งเป็นการขัดจังหวะ LangGraph\n",
        "\n",
        "โดยสรุป โค้ดนี้ตั้งค่า LangGraph ให้ทำงานแบบ asynchronous และมีกลไกในการขัดจังหวะการทำงานโดยการตั้งค่า event จากส่วนอื่นของแอปพลิเคชัน สิ่งนี้มีประโยชน์ในสถานการณ์ที่คุณอาจต้องหยุดชั่วคราวหรือหยุดการประมวลผลของกราฟเพื่อตอบสนองต่อเหตุการณ์ภายนอก ตัวห่อ `aclosing` ช่วยให้แน่ใจว่าทรัพยากรถูกล้างอย่างถูกต้องเมื่อเกิดการขัดจังหวะ\n",
        "\n",
        "**บริบท Human-in-the-Loop:**\n",
        "\n",
        "ในบริบทของ \"รูปแบบ Human-in-the-Loop\" โค้ดนี้น่าจะแสดงถึงรูปแบบ \"การขัดจังหวะ\" ผู้ใช้อาจกำลังสังเกตสตรีมของเอาต์พุตจาก LangGraph และตัดสินใจที่จะหยุดในบางจุด การตั้งค่า event จะกระตุ้นให้เกิดการขัดจังหวะ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhkxy6iIj1aT"
      },
      "source": [
        "สิ่งนี้ใช้เหตุการณ์หรือสัญญาณ เพื่อให้คุณสามารถควบคุมการขัดจังหวะจากภายนอกแอปพลิเคชันที่กำลังทำงานอยู่ สังเกตการใช้ `aclosing` ในบล็อกโค้ด Python สิ่งนี้ช่วยให้แน่ใจว่าสตรีมถูกปิดอย่างถูกต้องเมื่อถูกขัดจังหวะ สังเกตการใช้คำสั่ง `try-catch` ใน JS เนื่องจากการขัดจังหวะการรันจะส่งผลให้เกิดข้อยกเว้นการยกเลิก สุดท้าย สังเกตว่าการใช้งานตัวตรวจสอบจุด `(checkpointer)` จำเป็นต้องส่งตัวระบุสำหรับเธรดนี้ เพื่อแยกความแตกต่างของการโต้ตอบนี้กับกราฟจากการโต้ตอบอื่นๆ ทั้งหมด"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmWM7ahif58"
      },
      "source": [
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure8-4.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRil62WCkZOB"
      },
      "source": [
        "โหมดการควบคุมที่สองคือ \"การอนุญาต\" `(authorize)` ซึ่งผู้ใช้กำหนดล่วงหน้าว่าต้องการให้แอปพลิเคชันส่งมอบการควบคุมให้พวกเขา ทุกครั้งที่โหนดเฉพาะกำลังจะถูกเรียก (ดูรูปนี้) โดยทั่วไปจะใช้สำหรับการยืนยันเครื่องมือ ก่อนที่จะเรียกเครื่องมือใดๆ (หรือเครื่องมือเฉพาะ) แอปพลิเคชันจะหยุดชั่วคราวและขอการยืนยัน ซึ่ง ณ จุดนั้น ผู้ใช้สามารถทำได้อีกครั้ง:\n",
        "\n",
        "- `Resume`ดำเนินการคำนวณต่อ โดยยอมรับการเรียกเครื่องมือ\n",
        "- ส่งข้อความใหม่เพื่อนำบอทไปในทิศทางที่แตกต่างกัน ซึ่งในกรณีนี้เครื่องมือจะไม่ถูกเรียก\n",
        "- ไม่ทำอะไรเลย"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8gnO9EaKjj6M"
      },
      "outputs": [],
      "source": [
        "input = {\n",
        "  \"messages\": [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
        "  ]\n",
        "}\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "output = graph.astream(input, config, interrupt_before=['tools'])\n",
        "\n",
        "async for c in output:\n",
        "    ... # do something with the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6FIWIgk5Pg"
      },
      "source": [
        "สิ่งนี้จะรันกราฟจนกว่าจะถึงจุดที่กำลังจะเข้าสู่โหนดที่เรียกว่า `tools` ซึ่งจะให้โอกาสคุณในการตรวจสอบสถานะปัจจุบัน และตัดสินใจว่าจะดำเนินการต่อหรือไม่ สังเกตว่า `interrupt_before` เป็นรายการที่ลำดับไม่สำคัญ หากคุณส่งชื่อโหนดหลายชื่อ มันจะขัดจังหวะก่อนที่จะเข้าสู่แต่ละชื่อ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epBBBs8xe5gf"
      },
      "source": [
        "## Resume\n",
        "**การดำเนินการต่อ**\n",
        "\n",
        "ในการดำเนินการต่อจากกราฟที่ถูกขัดจังหวะ เช่น เมื่อใช้หนึ่งในสองรูปแบบก่อนหน้านี้ คุณเพียงแค่ต้องเรียกกราฟอีกครั้งด้วยอินพุต null (หรือ None ใน Python) สิ่งนี้ถูกใช้เป็นสัญญาณเพื่อดำเนินการประมวลผลอินพุตที่ไม่ใช่ null ก่อนหน้าต่อไป:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ASSapgUpkqZQ"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "output = graph.astream(None, config, interrupt_before=['tools'])\n",
        "\n",
        "async for c in output:\n",
        "    ... # do something with the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-obFVK96jAQm"
      },
      "source": [
        "## Restart\n",
        "**การเริ่มต้นใหม่**\n",
        "\n",
        "หากคุณต้องการให้กราฟที่ถูกขัดจังหวะเริ่มต้นใหม่จากโหนดแรก โดยมีอินพุตใหม่เพิ่มเติม คุณเพียงแค่ต้องเรียกใช้กราฟด้วยอินพุตใหม่:\n",
        "\n",
        "**คำอธิบายเพิ่มเติม:**\n",
        "\n",
        "*   **การเริ่มต้นใหม่ (Restart):** การเริ่มกระบวนการทำงานของกราฟใหม่ตั้งแต่ต้น โดยไม่ใช้สถานะที่บันทึกไว้ก่อนหน้านี้\n",
        "\n",
        "กล่าวคือ หากกราฟถูกขัดจังหวะและคุณต้องการให้มันเริ่มทำงานใหม่โดยใช้อินพุตใหม่ คุณไม่จำเป็นต้องทำอะไรเป็นพิเศษนอกจากการเรียกใช้ฟังก์ชันกราฟอีกครั้งด้วยอินพุตใหม่นั้นเอง กราฟจะเริ่มต้นการประมวลผลใหม่ตั้งแต่ต้นโดยใช้อินพุตใหม่นี้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sqeqCI1pi7U4"
      },
      "outputs": [],
      "source": [
        "input = {\n",
        "  \"messages\": [\n",
        "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
        "  ]\n",
        "}\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "output = graph.astream(input, config)\n",
        "\n",
        "async for c in output:\n",
        "    ... # do something with the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CpKURrwjdMS"
      },
      "source": [
        "สิ่งนี้จะรักษาสถานะปัจจุบันของกราฟ รวมเข้ากับอินพุตใหม่ และเริ่มต้นใหม่อีกครั้งจากโหนดแรก\n",
        "\n",
        "หากคุณต้องการลบสถานะปัจจุบัน เพียงแค่เปลี่ยน `thread_id` ซึ่งจะเริ่มต้นการโต้ตอบใหม่จากศูนย์ ค่าสตริงใดๆ ก็เป็น `thread_id` ที่ถูกต้อง เราขอแนะนำให้ใช้ UUID (หรือตัวระบุเฉพาะอื่นๆ) เป็น thread ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdF50FTIjldK"
      },
      "source": [
        "## Edit state\n",
        "\n",
        "**การแก้ไขสถานะ**\n",
        "\n",
        "บางครั้งคุณอาจต้องการอัปเดตสถานะของกราฟก่อนที่จะดำเนินการต่อ ซึ่งสามารถทำได้ด้วยเมธอด `update_state` โดยปกติแล้วคุณจะต้องตรวจสอบสถานะปัจจุบันก่อนด้วย `get_state`\n",
        "\n",
        "ลองมาดูว่ามีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2MtvIdPjTEk",
        "outputId": "735f2f1a-79b1-4540-8272-841673ba5dee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efdbdff-c596-6c29-8011-7b7917876121'}}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "state = graph.get_state(config)\n",
        "\n",
        "# something you want to add or replace\n",
        "update = { }\n",
        "\n",
        "graph.update_state(config, update)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjtsSduSj1XE"
      },
      "source": [
        "สิ่งนี้จะสร้างจุดตรวจสอบใหม่ที่มีการอัปเดตของคุณ ดังนั้นหลังจากนี้ คุณก็พร้อมที่จะดำเนินการกราฟต่อจากจุดใหม่นี้ ดูที่ `Resume` เพื่อดูวิธีการ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orstCS2bkCW2"
      },
      "source": [
        "## Fork\n",
        "**การแตกแขนง**\n",
        "\n",
        "คุณยังสามารถเรียกดูประวัติของสถานะทั้งหมดที่กราฟเคยผ่านมา และสามารถเยี่ยมชมสถานะใดก็ได้อีกครั้ง เช่น เพื่อรับคำตอบอื่น ซึ่งมีประโยชน์มากในแอปพลิเคชันที่สร้างสรรค์มากขึ้น ซึ่งคาดว่าการรันกราฟแต่ละครั้งจะสร้างเอาต์พุตที่แตกต่างกัน\n",
        "\n",
        "มาดูกันว่ามันมีลักษณะอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eS5O1_BJjsO3",
        "outputId": "554a338a-a060-4f1a-f8d8-e3a9376c8472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='How old was the 30th president of the United States when he died?', additional_kwargs={}, response_metadata={}, id='96959783-aee3-40ee-84aa-878f7f2c2c59'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vtIWfiM5y8CqKyEtviVwLN6A', 'function': {'arguments': '{\"query\":\"30th president of the United States\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 118, 'total_tokens': 142, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-85952b57-476c-4153-bd51-8ac0dbe8ded4-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': '30th president of the United States'}, 'id': 'call_vtIWfiM5y8CqKyEtviVwLN6A', 'type': 'tool_call'}], usage_metadata={'input_tokens': 118, 'output_tokens': 24, 'total_tokens': 142, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content=\"Learn about the life and presidency of Calvin Coolidge, the 30th president of the United States from 1923 to 1929. Find out his achievements, controversies, and legacy as a conservative Republican leader. The White House, official residence of the president of the United States, in July 2008. The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of ... Calvin Coolidge was the 30th president of the United States (1923-29). Coolidge acceded to the presidency after the death in office of Warren G. Harding, just as the Harding scandals were coming to light. He restored integrity to the executive branch while continuing Harding's conservative pro-business policies. As the head of the government of the United States, the president is arguably the most powerful government official in the world. The president is elected to a four-year term via an electoral college system. Since the Twenty-second Amendment was adopted in 1951, the American presidency has been limited to a maximum of two terms.. Click on a president below to learn more about each presidency ... Here is a list of the presidents and vice presidents of the United States along with their parties and dates in office. ... Chester A Arthur: Twenty-First President of the United States. 10 Interesting Facts About James Buchanan. Martin Van Buren - Eighth President of the United States. Quotes From Harry S. Truman. ThoughtCo.\", name='duckduckgo_search', id='76183289-aae5-4415-a366-82f829bf7cc8', tool_call_id='call_vtIWfiM5y8CqKyEtviVwLN6A'),\n",
              "  AIMessage(content='The 30th president of the United States was Calvin Coolidge. Let me find out his age at the time of his death.', additional_kwargs={'tool_calls': [{'id': 'call_wnU0SUzYQzYJExM2UvBT4NMx', 'function': {'arguments': '{\"query\":\"Calvin Coolidge age at death\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 485, 'total_tokens': 538, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c4dc6c90-6d66-4a89-ae66-7bb691b7c5ce-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Calvin Coolidge age at death'}, 'id': 'call_wnU0SUzYQzYJExM2UvBT4NMx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 485, 'output_tokens': 53, 'total_tokens': 538, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='His younger sister, Abigail Grace Coolidge (1875-1890), died at the age of 15, probably of appendicitis, when Coolidge was 18. Coolidge\\'s father married a Plymouth schoolteacher in 1891, and lived to the age of 80. ... After the conventions and the death of his younger son Calvin, Coolidge became withdrawn; he later said that \"when he [the ... Calvin Coolidge was the 30th president of the United States (1923-29). Coolidge acceded to the presidency after the death in office of Warren G. Harding, just as the Harding scandals were coming to light. ... Calvin Coolidge, Calvin Coolidge, age seven, c. 1879-80. (more) 2 of 2. ... Acceding to the presidency upon Harding\\'s unexpected ... Five years later, he\\'d suffer great loss once again when his younger sister and only sibling, Abigail or \"Abbie,\" died at age 12, likely from an attack of appendicitis. Through their grief, Calvin and his father persisted in maintaining the family store and homestead, and Calvin continued his studies. ... Calvin Coolidge declined to run ... Age: Dec. at 78 (1767-1845) Birthplace: Waxhaws ... Calvin Coolidge, Dec. 1932 (Died From Coronary Thrombosis On Jan. 5, 1933) Photo: British Movietone; YouTube; Age: Dec. at 60 (1872-1933) Birthplace: Plymouth Notch, Vermont; Also ranks #4 on Every President\\'s Most Controversial Pardon, Ranked; Calvin Coolidge died of a heart attack at age 60, four years after leaving the Oval Office. Lyndon Johnson had a heart attack before he ascended to the White House and died of one at age 64 after he left. At 66, George H.W. Bush was treated for atrial fibrillation, a type of irregular heartbeat, while in office. After leaving office, Bill ...', name='duckduckgo_search', id='ad54fc37-1324-4a9e-be45-7fa4e6953f5f', tool_call_id='call_wnU0SUzYQzYJExM2UvBT4NMx'),\n",
              "  AIMessage(content='Calvin Coolidge, the 30th president of the United States, died at the age of 60.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 977, 'total_tokens': 1002, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-94f3db1b-c271-40ae-9654-87794cbbe876-0', usage_metadata={'input_tokens': 977, 'output_tokens': 25, 'total_tokens': 1002, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  HumanMessage(content='How old was the 30th president of the United States when he died?', additional_kwargs={}, response_metadata={}, id='61c8e994-dbf6-4498-b309-9d1a1d101eff'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UfqSg3xzTpqPqfTVWU4zFOAP', 'function': {'arguments': '{\"query\":\"Calvin Coolidge age at death\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 1024, 'total_tokens': 1048, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-97d677a1-947f-4ce6-a27d-26f3ee0f5d36-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Calvin Coolidge age at death'}, 'id': 'call_UfqSg3xzTpqPqfTVWU4zFOAP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1024, 'output_tokens': 24, 'total_tokens': 1048, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='His younger sister, Abigail Grace Coolidge (1875-1890), died at the age of 15, probably of appendicitis, when Coolidge was 18. Coolidge\\'s father married a Plymouth schoolteacher in 1891, and lived to the age of 80. ... After the conventions and the death of his younger son Calvin, Coolidge became withdrawn; he later said that \"when he [the ... Calvin Coolidge was the 30th president of the United States (1923-29). Coolidge acceded to the presidency after the death in office of Warren G. Harding, just as the Harding scandals were coming to light. ... Calvin Coolidge, Calvin Coolidge, age seven, c. 1879-80. (more) 2 of 2. ... Acceding to the presidency upon Harding\\'s unexpected ... Calvin Coolidge died of a heart attack at age 60, four years after leaving the Oval Office. Lyndon Johnson had a heart attack before he ascended to the White House and died of one at age 64 after he left. At 66, George H.W. Bush was treated for atrial fibrillation, a type of irregular heartbeat, while in office. After leaving office, Bill ... Five years later, he\\'d suffer great loss once again when his younger sister and only sibling, Abigail or \"Abbie,\" died at age 12, likely from an attack of appendicitis. Through their grief, Calvin and his father persisted in maintaining the family store and homestead, and Calvin continued his studies. ... Calvin Coolidge declined to run ... A Rare Recording of President Calvin Coolidge (Audio Recording) by Calvin Coolidge and Listen &Live Audio Calvin Coolidge (July 4, 1872-January 5, 1933) was the 30th president of the United States from 1923 to 1929. Born in Vermont, Coolidge was a Republican lawyer from New England who climbed up the ladder of Massachusetts state politics, becoming the state\\'s 48th governor.', name='duckduckgo_search', id='0ea66c65-a731-4e4a-82bf-667ed8c16bc1', tool_call_id='call_UfqSg3xzTpqPqfTVWU4zFOAP'),\n",
              "  AIMessage(content='Calvin Coolidge, the 30th president of the United States, died at the age of 60.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1478, 'total_tokens': 1503, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-900e23c2-af72-473e-8559-0c862a62fc97-0', usage_metadata={'input_tokens': 1478, 'output_tokens': 25, 'total_tokens': 1503, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  HumanMessage(content='How old was the 30th president of the United States when he died?', additional_kwargs={}, response_metadata={}, id='662a8679-ce51-464d-b439-1d534b1de973'),\n",
              "  AIMessage(content='Calvin Coolidge, the 30th president of the United States, died at the age of 60.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1525, 'total_tokens': 1550, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-90bd82be-9b06-47e3-afcd-94b9e81d3678-0', usage_metadata={'input_tokens': 1525, 'output_tokens': 25, 'total_tokens': 1550, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
              " 'selected_tools': ['duckduckgo_search', 'calculator']}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "history = [\n",
        "  state for state in\n",
        "  graph.get_state_history(config)\n",
        "]\n",
        "\n",
        "# replay a past state\n",
        "graph.invoke(None, history[2].config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "32Wr2cGnkn71"
      },
      "outputs": [],
      "source": [
        "# for state in graph.get_state_history(config):\n",
        "#   print(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3cD2iaukUM_"
      },
      "source": [
        "สังเกตว่าเราเก็บประวัติไว้ในรายการ/อาร์เรย์ในทั้งสองภาษาเพราะ `get_state_history` ส่งคืน iterator ของสถานะ (เพื่อให้สามารถใช้งานแบบ lazy  หรือ `การประมวลผลหรือดึงข้อมูลเฉพาะเมื่อจำเป็นเท่านั้น` ได้) สถานะที่ส่งคืนจากเมธอด history จะถูกเรียงลำดับโดยสถานะล่าสุดอยู่ก่อนและสถานะที่เก่าที่สุดอยู่หลังสุด\n",
        "\n",
        "พลังที่แท้จริงของการควบคุมแบบ human-in-the-loop มาจากการผสมผสานพวกมันเข้าด้วยกันในแบบที่เหมาะสมกับแอปพลิเคชันของคุณ\n",
        "\n",
        "**คำอธิบายเพิ่มเติมเพื่อความเข้าใจ:**\n",
        "\n",
        "`get_state_history` คืนค่า iterator แทนที่จะเป็น list โดยตรง เพื่อประสิทธิภาพ หากประวัติมีขนาดใหญ่มาก การสร้าง list ทั้งหมดในหน่วยความจำอาจใช้ทรัพยากรมาก การใช้ iterator ช่วยให้เราเข้าถึงสถานะทีละรายการเมื่อเราต้องการเท่านั้น\n",
        "\n",
        "การเรียงลำดับสถานะจากล่าสุดไปเก่าสุดช่วยให้เข้าถึงสถานะปัจจุบันได้อย่างรวดเร็วโดยไม่ต้องวนซ้ำทั้งหมด\n",
        "\n",
        "ข้อความสุดท้ายเน้นย้ำถึงความยืดหยุ่นของการควบคุมแบบ human-in-the-loop โดยอนุญาตให้นักพัฒนาผสมผสานวิธีการควบคุมต่างๆ (เช่น การขัดจังหวะและการอนุญาต) เพื่อให้เหมาะสมกับความต้องการเฉพาะของแอปพลิเคชัน\n",
        "\n",
        "**สรุป:**\n",
        "\n",
        "โค้ดนี้จัดการประวัติสถานะโดยใช้ iterator เพื่อประสิทธิภาพ และเน้นว่าการควบคุมแบบ human-in-the-loop สามารถปรับแต่งได้โดยการรวมวิธีการควบคุมต่างๆ เข้าด้วยกัน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF1DXHDqlHPa"
      },
      "source": [
        "# Multitasking LLMs\n",
        "\n",
        "**การจัดการ LLM แบบ Multitasking**\n",
        "\n",
        "ส่วนนี้ครอบคลุมปัญหาการจัดการอินพุตพร้อมกันสำหรับแอปพลิเคชัน LLM นี่เป็นปัญหาที่เกี่ยวข้องอย่างยิ่ง เนื่องจาก LLM ค่อนข้างช้า โดยเฉพาะอย่างยิ่งเมื่อสร้างเอาต์พุตยาวหรือเมื่อเชื่อมโยงในสถาปัตยกรรมหลายขั้นตอน (เช่นที่คุณทำได้กับ LangGraph) แม้ว่า LLM จะเร็วขึ้น แต่สิ่งนี้จะยังคงเป็นความท้าทาย เนื่องจากจะเปิดประตูสำหรับกรณีการใช้งานที่ซับซ้อนมากขึ้นเรื่อยๆ ในลักษณะเดียวกับที่แม้แต่คนที่ทำงานได้มีประสิทธิภาพมากที่สุดก็ยังคงต้องเผชิญกับความจำเป็นในการจัดลำดับความสำคัญของความต้องการที่แข่งขันกันในเวลาของพวกเขา\n",
        "\n",
        "มาดูตัวเลือกต่างๆ กัน\n",
        "\n",
        "- **Refuse concurrent inputs(ปฏิเสธอินพุตพร้อมกัน)**\n",
        "\n",
        "อินพุตใดๆ ที่ได้รับขณะประมวลผลอินพุตก่อนหน้าจะถูกปฏิเสธ นี่เป็นกลยุทธ์ที่ง่ายที่สุด แต่ไม่น่าจะครอบคลุมทุกความต้องการ เนื่องจากหมายถึงการส่งมอบการจัดการ concurrency ให้กับผู้เรียก\n",
        "\n",
        "- **Handle independently (จัดการอย่างอิสระ)**\n",
        "\n",
        "อีกทางเลือกหนึ่งที่ง่ายคือการจัดการอินพุตใหม่ใดๆ เป็นการเรียกใช้แบบอิสระ สร้างเธรดใหม่ (เตือนความจำ: นี่คือคอนเทนเนอร์สำหรับจดจำสถานะ) และสร้างเอาต์พุตในบริบทนั้น สิ่งนี้มีข้อเสียที่ชัดเจนคือต้องแสดงให้ผู้ใช้เห็นเป็นการเรียกใช้สองครั้งที่แยกจากกันและไม่สามารถประนีประนอมได้ ซึ่งไม่สามารถทำได้หรือเป็นที่ต้องการเสมอไป ในทางกลับกัน มันมีข้อดีคือการปรับขนาดให้มีขนาดใหญ่เท่าใดก็ได้ และเป็นสิ่งที่คุณจะใช้ในแอปพลิเคชันของคุณในระดับหนึ่งอย่างแน่นอน ตัวอย่างเช่น นี่คือวิธีที่คุณจะคิดเกี่ยวกับปัญหาการทำให้แชทบอท \"แชท\" กับผู้ใช้สองคนพร้อมกัน\n",
        "\n",
        "- **Queue concurrent inputs (จัดคิวอินพุตพร้อมกัน)**\n",
        "\n",
        "อินพุตใดๆ ที่ได้รับขณะประมวลผลอินพุตก่อนหน้าจะถูกจัดคิวและจัดการเมื่ออินพุตปัจจุบันเสร็จสิ้น กลยุทธ์นี้มีข้อดีบางประการ:\n",
        "\n",
        "*   รองรับการรับคำขอพร้อมกันจำนวนเท่าใดก็ได้\n",
        "*   เนื่องจากเรารอให้อินพุตปัจจุบันประมวลผลเสร็จ จึงไม่สำคัญว่าอินพุตใหม่จะมาถึงทันทีหลังจากที่เราเริ่มจัดการอินพุตปัจจุบันหรือทันทีก่อนที่เราจะประมวลผลเสร็จ ผลลัพธ์สุดท้ายจะเหมือนกัน เนื่องจากเราจะประมวลผลอินพุตปัจจุบันให้เสร็จก่อนที่จะไปยังอินพุตถัดไป\n",
        "\n",
        "กลยุทธ์นี้มีข้อเสียบางประการเช่นกัน:\n",
        "\n",
        "*   อาจใช้เวลาสักครู่ในการประมวลผลอินพุตที่อยู่ในคิวทั้งหมด ในความเป็นจริง คิวอาจเติบโตอย่างไม่สิ้นสุดหากอินพุตถูกสร้างขึ้นในอัตราที่เร็วกว่าการประมวลผล\n",
        "*   อินพุตอาจล้าสมัยเมื่อถึงเวลาที่ได้รับการประมวลผล เนื่องจากอยู่ในคิวก่อนที่จะเห็นการตอบสนองต่ออินพุตก่อนหน้า และไม่เปลี่ยนแปลงหลังจากนั้น กลยุทธ์นี้ไม่เหมาะสมเมื่ออินพุตใหม่ขึ้นอยู่กับการตอบกลับก่อนหน้า\n",
        "\n",
        "- **Interrupt (ขัดจังหวะ)**\n",
        "\n",
        "เมื่อได้รับอินพุตใหม่ในขณะที่กำลังประมวลผลอินพุตอื่น ให้ละทิ้งการประมวลผลอินพุตปัจจุบันและเริ่มต้นเชนใหม่ด้วยอินพุตใหม่ กลยุทธ์นี้อาจแตกต่างกันไปตามสิ่งที่เก็บไว้ของการรันที่ถูกขัดจังหวะ นี่คือตัวเลือกบางส่วน:\n",
        "\n",
        "*   ไม่เก็บอะไรเลย อินพุตก่อนหน้าจะถูกลืมไปโดยสมบูรณ์ ราวกับว่าไม่เคยถูกส่งหรือประมวลผล\n",
        "*   เก็บขั้นตอนสุดท้ายที่เสร็จสมบูรณ์ ในแอปที่ใช้ checkpointing (ซึ่งจัดเก็บความคืบหน้าขณะที่เคลื่อนผ่านการคำนวณ) ให้เก็บสถานะที่สร้างโดยขั้นตอนสุดท้ายที่เสร็จสมบูรณ์ ละทิ้งการอัปเดตสถานะที่ค้างอยู่จากขั้นตอนที่กำลังดำเนินการ และเริ่มจัดการอินพุตใหม่ในบริบทนั้น\n",
        "*   เก็บขั้นตอนที่กำลังดำเนินการไว้ด้วย พยายามขัดจังหวะขั้นตอนปัจจุบันในขณะที่ดูแลเพื่อบันทึกการอัปเดตสถานะที่ไม่สมบูรณ์ที่กำลังสร้างขึ้นในขณะนั้น สิ่งนี้ไม่น่าจะใช้ได้ทั่วไปนอกเหนือจากสถาปัตยกรรมที่ง่ายที่สุด\n",
        "*   รอให้โหนดปัจจุบัน (แต่ไม่ใช่โหนดถัดไป) ประมวลผลเสร็จ จากนั้นบันทึกและขัดจังหวะ\n",
        "\n",
        "ตัวเลือกนี้มีข้อดีบางประการเมื่อเทียบกับการจัดคิวอินพุตพร้อมกัน:\n",
        "\n",
        "*   อินพุตใหม่จะได้รับการจัดการโดยเร็วที่สุด ลดโอกาสในการสร้างเอาต์พุตที่ล้าสมัยและความหน่วงในการสร้าง\n",
        "*   สำหรับตัวแปร \"ไม่เก็บอะไรเลย\" เอาต์พุตสุดท้ายจะไม่ขึ้นอยู่กับเวลาที่ได้รับอินพุตใหม่\n",
        "\n",
        "แต่ก็มีข้อเสียเช่นกัน:\n",
        "\n",
        "*   ในทางปฏิบัติ กลยุทธ์นี้ยังคงจำกัดอยู่เพียงการประมวลผลอินพุตทีละรายการ เนื่องจากอินพุตเก่าใดๆ จะถูกละทิ้งเมื่อได้รับอินพุตใหม่\n",
        "*   การเก็บการอัปเดตสถานะบางส่วนสำหรับการรันครั้งต่อไปต้องออกแบบสถานะโดยคำนึงถึงสิ่งนั้น หากไม่ใช่เช่นนั้น แอปพลิเคชันของคุณอาจจบลงในสถานะที่ไม่ถูกต้อง ตัวอย่างเช่น โมเดลแชท OpenAI กำหนดให้ข้อความ AI ที่ขอการเรียกเครื่องมือต้องตามด้วยข้อความเครื่องมือพร้อมเอาต์พุตของเครื่องมือทันที หากการรันของคุณถูกขัดจังหวะระหว่างนั้น คุณต้องล้างสถานะกลางอย่างระมัดระวังหรือเสี่ยงต่อการไม่สามารถดำเนินการต่อไปได้\n",
        "*   เอาต์พุตสุดท้ายที่สร้างขึ้นมีความไวต่อเวลาที่ได้รับอินพุตใหม่ เนื่องจากอินพุตใหม่จะได้รับการจัดการในบริบทของความคืบหน้า (ที่ไม่สมบูรณ์) ที่ทำไว้ก่อนหน้านี้ในการจัดการอินพุตก่อนหน้า สิ่งนี้อาจส่งผลให้เกิดผลลัพธ์ที่เปราะบางหรือไม่สามารถคาดเดาได้หากคุณไม่ออกแบบให้สอดคล้องกัน\n",
        "\n",
        "- **Fork and merge**\n",
        "\n",
        "อีกทางเลือกหนึ่งคือการจัดการอินพุตใหม่แบบขนาน โดย fork สถานะของเธรดเมื่อได้รับอินพุตใหม่และรวมสถานะสุดท้ายเมื่อการจัดการอินพุตเสร็จสิ้น ตัวเลือกนี้กำหนดให้ต้องออกแบบสถานะของคุณให้สามารถรวมได้โดยไม่มีข้อขัดแย้ง (เช่น การใช้ conflict-free replicated data types [CRDTs] หรืออัลกอริธึมการแก้ไขข้อขัดแย้งอื่นๆ) หรือให้ผู้ใช้แก้ไขข้อขัดแย้งด้วยตนเองก่อนที่คุณจะสามารถเข้าใจเอาต์พุตหรือส่งอินพุตใหม่ในเธรดนี้ หากตรงตามข้อกำหนดใดข้อกำหนดหนึ่งเหล่านี้ นี่น่าจะเป็นตัวเลือกที่ดีที่สุดโดยรวม เนื่องจากอินพุตใหม่ได้รับการจัดการอย่างทันท่วงที เอาต์พุตเป็นอิสระจากเวลาที่ได้รับ และรองรับการรันพร้อมกันจำนวนเท่าใดก็ได้\n",
        "\n",
        "กลยุทธ์เหล่านี้บางส่วนถูกนำมาใช้ใน LangGraph Cloud ซึ่งจะกล่าวถึงในบทที่ 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ZZd8z2m9q1"
      },
      "source": [
        "# Summary\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้ เราได้กลับมาพิจารณาถึงข้อแลกเปลี่ยนหลักที่คุณต้องเผชิญเมื่อสร้างแอปพลิเคชัน LLM นั่นคือ ความสามารถในการดำเนินการด้วยตนเอง (agency) กับความน่าเชื่อถือ (reliability) เราได้เรียนรู้ว่ามีกลยุทธ์ที่จะเอาชนะอุปสรรคบางส่วนและได้รับความน่าเชื่อถือมากขึ้นโดยไม่สูญเสียความสามารถในการดำเนินการด้วยตนเอง และในทางกลับกัน\n",
        "\n",
        "เราเริ่มต้นด้วยการกล่าวถึงเอาต์พุตที่มีโครงสร้าง ซึ่งสามารถปรับปรุงความสามารถในการคาดการณ์ของข้อความที่สร้างโดย LLM ต่อไป เราได้พูดคุยเกี่ยวกับการส่งออกเอาต์พุตแบบสตรีมมิ่ง/ระดับกลางจากแอปพลิเคชันของคุณ ซึ่งสามารถทำให้แอปพลิเคชันที่มีความหน่วงสูง (ผลข้างเคียงที่หลีกเลี่ยงไม่ได้ของความสามารถในการดำเนินการด้วยตนเองในปัจจุบัน) ใช้งานได้ง่าย\n",
        "\n",
        "เรายังได้กล่าวถึงการควบคุมแบบ human-in-the-loop ที่หลากหลาย นั่นคือ เทคนิคในการคืนการกำกับดูแลบางส่วนให้กับผู้ใช้ปลายทางของแอปพลิเคชัน LLM ของคุณ ซึ่งมักจะสร้างความแตกต่างในการทำให้สถาปัตยกรรมที่มีความสามารถในการดำเนินการด้วยตนเองสูงมีความน่าเชื่อถือ สุดท้าย เราได้พูดคุยเกี่ยวกับปัญหาของการจัดการอินพุตพร้อมกันไปยังแอปพลิเคชันของคุณ ซึ่งเป็นปัญหาที่สำคัญอย่างยิ่งสำหรับแอป LLM เนื่องจากมีความหน่วงสูง"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
