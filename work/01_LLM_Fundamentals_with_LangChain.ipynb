{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0qrYIf4f2n"
      },
      "source": [
        "# Chapter 1. LLM Fundamentals with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iduom2IArR_o",
        "outputId": "b684fee2-a34c-45ec-a7d0-208d88e143f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Langchian_LLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Smith-WeStrideTH/Langchian_LLM.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aV1cHaeEPpJr",
        "outputId": "776e474d-0489-4281-9fd1-bf8fce5069dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Using cached openai-1.59.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai) (4.8.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: sniffio in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai) (1.3.1)\n",
            "Collecting tqdm>4 (from openai)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Using cached openai-1.59.9-py3-none-any.whl (455 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.8.2-cp311-cp311-win_amd64.whl (206 kB)\n",
            "Using cached pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
            "Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.5/2.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 0.8/2.0 MB 1.3 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.0/2.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.6/2.0 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 1.7 MB/s eta 0:00:00\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: tqdm, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
            "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.8.2 openai-1.59.9 pydantic-2.10.5 pydantic-core-2.27.2 tqdm-4.67.1\n",
            "Collecting python-dotenv\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_openai\n",
            "  Using cached langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-postgres\n",
            "  Using cached langchain_postgres-0.0.12-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.37-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.31 (from langchain)\n",
            "  Using cached langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Using cached langsmith-0.3.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1.22.4 (from langchain)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
            "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain_openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pgvector<0.3.0,>=0.2.5 (from langchain-postgres)\n",
            "  Using cached pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting psycopg<4,>=3 (from langchain-postgres)\n",
            "  Using cached psycopg-3.2.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting psycopg-pool<4.0.0,>=3.2.1 (from langchain-postgres)\n",
            "  Using cached psycopg_pool-3.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.31->langchain)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.31->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.31->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Collecting tzdata (from psycopg<4,>=3->langchain-postgres)\n",
            "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain) (3.0.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\smith\\anaconda3\\envs\\langenv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain_openai) (0.4.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Using cached langchain-0.3.15-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_openai-0.3.1-py3-none-any.whl (54 kB)\n",
            "Using cached langchain_community-0.3.15-py3-none-any.whl (2.5 MB)\n",
            "Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
            "Using cached langchain_postgres-0.0.12-py3-none-any.whl (21 kB)\n",
            "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Using cached langchain_core-0.3.31-py3-none-any.whl (412 kB)\n",
            "Using cached langsmith-0.3.0-py3-none-any.whl (332 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
            "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/15.8 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.8/15.8 MB 2.0 MB/s eta 0:00:08\n",
            "   -- ------------------------------------- 1.0/15.8 MB 1.8 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 1.3/15.8 MB 1.9 MB/s eta 0:00:08\n",
            "   ---- ----------------------------------- 1.8/15.8 MB 1.9 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 2.4/15.8 MB 1.9 MB/s eta 0:00:08\n",
            "   ------ --------------------------------- 2.6/15.8 MB 1.9 MB/s eta 0:00:08\n",
            "   ------- -------------------------------- 3.1/15.8 MB 1.8 MB/s eta 0:00:08\n",
            "   -------- ------------------------------- 3.4/15.8 MB 1.8 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.7/15.8 MB 1.8 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.9/15.8 MB 1.7 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 4.2/15.8 MB 1.6 MB/s eta 0:00:08\n",
            "   ---------- ----------------------------- 4.2/15.8 MB 1.6 MB/s eta 0:00:08\n",
            "   ----------- ---------------------------- 4.5/15.8 MB 1.5 MB/s eta 0:00:08\n",
            "   ----------- ---------------------------- 4.7/15.8 MB 1.5 MB/s eta 0:00:08\n",
            "   ------------ --------------------------- 5.0/15.8 MB 1.4 MB/s eta 0:00:08\n",
            "   ------------- -------------------------- 5.2/15.8 MB 1.4 MB/s eta 0:00:08\n",
            "   ------------- -------------------------- 5.2/15.8 MB 1.4 MB/s eta 0:00:08\n",
            "   ------------- -------------------------- 5.5/15.8 MB 1.4 MB/s eta 0:00:08\n",
            "   ------------- -------------------------- 5.5/15.8 MB 1.4 MB/s eta 0:00:08\n",
            "   -------------- ------------------------- 5.8/15.8 MB 1.3 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 6.0/15.8 MB 1.3 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 6.3/15.8 MB 1.3 MB/s eta 0:00:08\n",
            "   ---------------- ----------------------- 6.6/15.8 MB 1.3 MB/s eta 0:00:08\n",
            "   ----------------- ---------------------- 6.8/15.8 MB 1.3 MB/s eta 0:00:08\n",
            "   ----------------- ---------------------- 7.1/15.8 MB 1.3 MB/s eta 0:00:07\n",
            "   ------------------ --------------------- 7.3/15.8 MB 1.3 MB/s eta 0:00:07\n",
            "   ------------------- -------------------- 7.6/15.8 MB 1.3 MB/s eta 0:00:07\n",
            "   ------------------- -------------------- 7.9/15.8 MB 1.3 MB/s eta 0:00:07\n",
            "   -------------------- ------------------- 8.1/15.8 MB 1.3 MB/s eta 0:00:07\n",
            "   --------------------- ------------------ 8.4/15.8 MB 1.3 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 8.7/15.8 MB 1.3 MB/s eta 0:00:06\n",
            "   ---------------------- ----------------- 8.9/15.8 MB 1.3 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 9.2/15.8 MB 1.3 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 9.4/15.8 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------------ --------------- 9.7/15.8 MB 1.3 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 10.0/15.8 MB 1.3 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 10.2/15.8 MB 1.3 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 10.5/15.8 MB 1.3 MB/s eta 0:00:05\n",
            "   --------------------------- ------------ 11.0/15.8 MB 1.3 MB/s eta 0:00:04\n",
            "   ---------------------------- ----------- 11.3/15.8 MB 1.3 MB/s eta 0:00:04\n",
            "   ----------------------------- ---------- 11.5/15.8 MB 1.3 MB/s eta 0:00:04\n",
            "   ----------------------------- ---------- 11.8/15.8 MB 1.3 MB/s eta 0:00:04\n",
            "   ------------------------------ --------- 12.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------------------------- -------- 12.6/15.8 MB 1.3 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 12.8/15.8 MB 1.3 MB/s eta 0:00:03\n",
            "   --------------------------------- ------ 13.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
            "   --------------------------------- ------ 13.4/15.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 13.9/15.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 14.2/15.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 14.4/15.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------------------- -- 14.9/15.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 14.9/15.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 15.2/15.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  15.5/15.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  15.7/15.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 15.8/15.8 MB 1.3 MB/s eta 0:00:00\n",
            "Using cached pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached psycopg-3.2.4-py3-none-any.whl (198 kB)\n",
            "Using cached psycopg_pool-3.2.4-py3-none-any.whl (38 kB)\n",
            "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading SQLAlchemy-2.0.37-cp311-cp311-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 0.5/2.1 MB 441.3 kB/s eta 0:00:04\n",
            "   --------- ------------------------------ 0.5/2.1 MB 441.3 kB/s eta 0:00:04\n",
            "   -------------- ------------------------- 0.8/2.1 MB 541.1 kB/s eta 0:00:03\n",
            "   ------------------- -------------------- 1.0/2.1 MB 578.3 kB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.0/2.1 MB 578.3 kB/s eta 0:00:02\n",
            "   ------------------------ --------------- 1.3/2.1 MB 621.2 kB/s eta 0:00:02\n",
            "   ------------------------ --------------- 1.3/2.1 MB 621.2 kB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 635.3 kB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 675.6 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 691.7 kB/s eta 0:00:00\n",
            "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-win_amd64.whl (884 kB)\n",
            "   ---------------------------------------- 0.0/884.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/884.5 kB ? eta -:--:--\n",
            "   ----------- ---------------------------- 262.1/884.5 kB ? eta -:--:--\n",
            "   ----------------------- ---------------- 524.3/884.5 kB 1.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 786.4/884.5 kB 1.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 884.5/884.5 kB 1.0 MB/s eta 0:00:00\n",
            "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
            "Downloading greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
            "Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl (133 kB)\n",
            "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
            "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: zstandard, tzdata, tenacity, regex, psycopg-pool, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, psycopg, pgvector, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-postgres, langchain_openai, langchain, langchain_community\n",
            "Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.15 langchain-core-0.3.31 langchain-postgres-0.0.12 langchain-text-splitters-0.3.5 langchain_community-0.3.15 langchain_openai-0.3.1 langsmith-0.3.0 marshmallow-3.25.1 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.15 pgvector-0.2.5 propcache-0.2.1 psycopg-3.2.4 psycopg-pool-3.2.4 pydantic-settings-2.7.1 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-9.0.0 tiktoken-0.8.0 typing-inspect-0.9.0 tzdata-2025.1 yarl-1.18.3 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain langchain_openai langchain_community langchain-text-splitters langchain-postgres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYOd_z945pA"
      },
      "source": [
        "# Introduction Setup\n",
        "<img align=\"top\" src=\"https://github.com/Smith-WeStrideTH/Langchian_LLM/blob/main/work/pics/Figure1-1.png?raw=1\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "**การใช้งานตลอด Notebookนี้**\n",
        "\n",
        "- **โมเดลแชท**:  หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Anthropic เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **เวกเตอร์เอมเบดดิ้ง**: หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Cohere เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **ที่เก็บเวกเตอร์**: หากคุณไม่ต้องการใช้ Pgvector (ส่วนขยายโอเพนซอร์สของฐานข้อมูล SQL ที่เป็นที่นิยมอย่าง Postgres) เราขอแนะนำให้ใช้ Weaviate (ที่เก็บเวกเตอร์เฉพาะ) หรือ OpenSearch (คุณสมบัติการค้นหาเวกเตอร์ที่เป็นส่วนหนึ่งของฐานข้อมูลการค้นหาที่เป็นที่นิยม)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YuxZmLB57Re"
      },
      "source": [
        "ความพยายามนี้ไปไกลกว่าเพียงแค่ให้ LLM ทั้งหมดมีวิธีการเดียวกัน โดยมีอาร์กิวเมนต์และค่าส่งคืนที่คล้ายคลึงกัน มาดูตัวอย่างของโมเดลแชทและผู้ให้บริการ LLM ที่ได้รับความนิยมสองรายอย่าง OpenAI และ Anthropic ทั้งคู่มี API แชทที่รับ ข้อความแชท (กำหนดแบบหลวม ๆ ว่าเป็นอ็อบเจ็กต์ที่มีสตริงชนิดและสตริงเนื้อหา) และส่งคืนข้อความใหม่ที่สร้างโดยโมเดล แต่หากคุณพยายามใช้ทั้งสองโมเดลในบทสนทนาเดียวกัน คุณจะพบปัญหาทันที เนื่องจากรูปแบบข้อความแชทของพวกเขาไม่เข้ากันเล็กน้อย LangChain สร้างความเป็นนามธรรมของความแตกต่างเหล่านี้เพื่อเปิดใช้งานการสร้างแอปพลิเคชันที่เป็นอิสระจากผู้ให้บริการโดยแท้จริง ตัวอย่างเช่น ด้วย LangChain บทสนทนาแชทบอทที่คุณใช้ทั้งโมเดล OpenAI และ Anthropic ก็ใช้งานได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWq72Aa6hkT"
      },
      "source": [
        "ทางเลือกอื่น ๆ อินเทอร์เฟซ Chat Model ช่วยให้เกิดการสนทนาแบบโต้ตอบระหว่างผู้ใช้กับโมเดล เหตุผลที่เป็นอินเทอร์เฟซที่แยกต่างหากนั้นเป็นเพราะผู้ให้บริการ LLM ที่ได้รับความนิยม เช่น OpenAI แยกแยะข้อความที่ส่งไปและมาจากโมเดลออกเป็นบทบาทของผู้ใช้ ผู้ช่วย และระบบ (ที่นี่ บทบาทหมายถึงประเภทของเนื้อหาที่ข้อความมี):\n",
        "\n",
        "- **บทบาทระบบ (System role)**\n",
        "\n",
        "ช่วยให้ผู้พัฒนาสามารถระบุคำแนะนำที่โมเดลควรใช้เพื่อตอบคำถามของผู้ใช้\n",
        "\n",
        "- **บทบาทผู้ใช้(User role)**\n",
        "\n",
        "บุคคลที่ถามคำถามและสร้างคำถามที่ส่งไปยังโมเดล\n",
        "\n",
        "- **บทบาทผู้ช่วย(Assistant role)**\n",
        "\n",
        "การตอบสนองของโมเดลต่อคำถามของผู้ใช้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFLbsSz9M0c"
      },
      "source": [
        "## ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EjcPWdh7LxkL"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check api from '/credential/cred.env'\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')  # Replace with the actual path if needed\n",
        "\n",
        "# Access the API key\n",
        "api_key = os.getenv(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was loaded successfully\n",
        "if api_key:\n",
        "    print(\"API key loaded successfully.\")\n",
        "    # You can now use the api_key with the OpenAI API\n",
        "else:\n",
        "    print(\"Error: API_KEY not found in .env file.\")"
      ],
      "metadata": {
        "id": "4WTWAYJOXTY4",
        "outputId": "f46eda46-4ef2-46cd-e869-469491386ee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "qWi7xtUqXsZn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4QVuB_RPNb",
        "outputId": "d9350fa9-6795-44dc-c82f-281ce6469a70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='เมืองหลวงของประเทศฝรั่งเศสคือกรุงปารีส (Paris) ค่ะ ซึ่งเป็นเมืองที่มีชื่อเสียงและมีสถานีท่องเที่ยวที่ดังๆอย่างเช่น Eiffel Tower, Louvre Museum, และ Notre-Dame Cathedral ค่ะ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 42, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0417bdb8-6348-4e59-a7fc-42f509b40010-0', usage_metadata={'input_tokens': 42, 'output_tokens': 126, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "model = ChatOpenAI()  # The API key will be automatically fetched from the OPENAI_API_KEY environment variable\n",
        "\n",
        "prompt = [HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')]\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgTxKRc4Qkt"
      },
      "source": [
        "แทนที่จะเป็นสตริงพร้อมท์เดียว โมเดลแชทใช้ส่วนต่อประสานข้อความแชทประเภทต่างๆ ที่เกี่ยวข้องกับบทบาทที่กล่าวถึงก่อนหน้านี้ ซึ่งรวมถึงสิ่งต่อไปนี้:\n",
        "\n",
        "- HumanMessage ข้อความที่ส่งจากมุมมองของมนุษย์ โดยมีบทบาทเป็น ผู้ใช้\n",
        "- AIMessage ข้อความที่ส่งจากมุมมองของ AI ที่มนุษย์โต้ตอบด้วย โดยมีบทบาทเป็น ผู้ช่วย\n",
        "- SystemMessage ข้อความที่ตั้งค่าคำแนะนำที่ AI ควรปฏิบัติตาม โดยมีบทบาทเป็น ระบบ\n",
        "- ChatMessage ข้อความที่อนุญาตให้ตั้งค่าบทบาทโดยพลการ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E0IIh1KPx8P",
        "outputId": "47248845-c205-43f8-ed37-b7a7e291dbac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ปารีส!!!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 108, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-85828e7c-afdc-4386-b735-4b54e23049dc-0', usage_metadata={'input_tokens': 108, 'output_tokens': 6, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()\n",
        "system_msg = SystemMessage('คุณเป็นผู้ช่วยที่เป็นประโยชน์ที่ตอบคำถามด้วยเครื่องหมาย exclamation marks 3 ตัว')\n",
        "human_msg = HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')\n",
        "completion = model.invoke([system_msg, human_msg])\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBULIqmU74cU"
      },
      "source": [
        "## PromptTemplate\n",
        "โชคดีที่ LangChain มีอินเทอร์เฟซเทมเพลตพร้อมท์ที่ทำให้การสร้างพร้อมท์ที่มีอินพุตแบบไดนามิกเป็นเรื่องง่าย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXC2QSYk64GG",
        "outputId": "28abebd2-16dd-478e-d0fd-db71f6550913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\\nContext: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\\nQuestion: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\\nAnswer: ')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisV9Vq-8jaW",
        "outputId": "5b18a7aa-ff7d-4c29-bc84-d42e62aea6e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'โมเดลรายใดก็ได้ที่มีความสามารถด้าน NLP และเพิ่มประสิทธิภาพของการพัฒนาแอปพลิเคชัน ซึ่งสามารถเข้าถึงผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ '"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# both `template` and `model` can be reused many times\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "\n",
        "model = OpenAI()\n",
        "# `prompt` and `completion` are the results of using template and model once\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDPjs4z9-wi"
      },
      "source": [
        "## ChatPromptTemplate\n",
        "หากคุณกำลังมองหาการสร้างแอปพลิเคชันแชท AI เทมเพลตพร้อมท์แชทสามารถใช้แทนเพื่อให้ข้อมูลอินพุตแบบไดนามิกโดยยึดตามบทบาทของข้อความแชท เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYjexTbO9GVY",
        "outputId": "fdbcf4ca-1218-4757-dbc3-1fa6bc2928b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x67TYDjP-rQ_"
      },
      "source": [
        "สังเกตว่าพร้อมท์ประกอบด้วยคำแนะนำใน SystemMessage และ HumanMessages สองข้อความที่มีตัวแปร context และ question แบบไดนามิก คุณยังคงสามารถจัดรูปแบบเทมเพลตในลักษณะเดียวกัน และรับพร้อมท์แบบคงที่ที่คุณสามารถส่งผ่านไปยังโมเดลภาษาขนาดใหญ่สำหรับผลลัพธ์การทำนาย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2CASUrW91bE",
        "outputId": "5b818dad-ddfc-43f6-c5c2-09aa749dc2b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBot: Hugging Face, OpenAI, and Cohere are all providers of LLM models.'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "model = OpenAI()\n",
        "\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uws0QjeG_KP5"
      },
      "source": [
        "## JSON Output\n",
        "\n",
        "รูปแบบที่พบมากที่สุดในการสร้างด้วย LLM คือ JSON ซึ่งสามารถนำไปใช้เพื่อ เช่น:\n",
        "\n",
        "- ส่งไปยังโค้ดฝั่งไคลเอนต์ของคุณผ่านเครือข่าย\n",
        "\n",
        "- บันทึกไปยังฐานข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieDQgsHd_k6V"
      },
      "source": [
        "เมื่อสร้าง JSON งานแรกคือการกำหนดรูปแบบข้อมูล (schema) ที่คุณต้องการให้ LLM เคารพเมื่อสร้างผลลัพธ์ จากนั้น คุณควรใส่รูปแบบข้อมูลนั้นไว้ในพร้อมท์ พร้อมกับข้อความที่คุณต้องการใช้เป็นแหล่งที่มา มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OHoJqkp-2p6",
        "outputId": "eeec4817-0594-42e0-af91-ed74d9b0e8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Smith\\anaconda3\\envs\\langenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "C:\\Users\\Smith\\anaconda3\\envs\\langenv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1363: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='อิฐหนึ่งปอนด์หนักกว่าขนนกหนึ่งปอนด์', justification='อิฐมีความหนาและหนักกว่าขนนก')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้พร้อมเหตุผลสนับสนุนคำตอบ'''\n",
        "    answer: str\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้'''\n",
        "    justification: str\n",
        "    '''เหตุผลสนับสนุนคำตอบ'''\n",
        "\n",
        "llm = ChatOpenAI(temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"อะไรหนักกว่า ระหว่างอิฐหนึ่งปอนด์กับขนนกหนึ่งปอนด์\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltuwEJE__10",
        "outputId": "96a9eabd-eb8e-4753-ac11-be27377685c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1363: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='They weigh the same.', justification='A pound of bricks and a pound of feathers both weigh one pound. The weight is the same, but the volume or density of the two substances is different.')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''An answer to the user question along with justification for the answer.'''\n",
        "    answer: str\n",
        "    '''The answer to the user's question'''\n",
        "    justification: str\n",
        "    '''Justification for the answer'''\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'), temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxK8v2a_A37q"
      },
      "source": [
        "ดังนั้น ขั้นแรกให้กำหนดรูปแบบข้อมูล (schema) ใน Python การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Pydantic (ไลบรารีที่ใช้สำหรับตรวจสอบความถูกต้องของข้อมูลตามรูปแบบข้อมูล) ใน JS การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Zod (ไลบรารีที่เทียบเท่ากัน)\n",
        "\n",
        "**วิธีการ with_structured_output จะใช้รูปแบบข้อมูลนั้นสำหรับสองสิ่ง:**\n",
        "\n",
        "- รูปแบบข้อมูลจะถูกแปลงเป็นอ็อบเจ็กต์ JSONSchema (รูปแบบ JSON ที่ใช้เพื่ออธิบายรูปร่าง ประเภท ชื่อ คำอธิบาย ของข้อมูล JSON) ซึ่งจะถูกส่งไปยัง LLM LangChain เลือกวิธีที่ดีที่สุดในการทำสิ่งนี้โดยปกติจะเป็นการเรียกใช้ฟังก์ชันหรือการส่งพร้อมท์\n",
        "- รูปแบบข้อมูลจะถูกใช้เพื่อตรวจสอบความถูกต้องของเอาต์พุตที่ส่งคืนโดย LLM ก่อนที่จะส่งคืน ซึ่งจะช่วยให้มั่นใจได้ว่าเอาต์พุตที่สร้างขึ้นเป็นไปตามรูปแบบข้อมูลที่คุณส่งผ่านมาอย่างแน่นอน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICP-do9_Bn3C"
      },
      "source": [
        "## Other Output\n",
        "**รูปแบบที่อ่านได้ด้วยเครื่องจักรอื่น ๆ ที่มีตัวแยกวิเคราะห์ผลลัพธ์**\n",
        "\n",
        "คุณยังสามารถใช้อัลกอริทึม LLM หรือ Chat Model เพื่อสร้างผลลัพธ์ในรูปแบบอื่น เช่น CSV หรือ XML ซึ่งเป็นที่ที่ตัวแยกวิเคราะห์ผลลัพธ์มีประโยชน์ ตัวแยกวิเคราะห์ผลลัพธ์เป็นคลาสที่ช่วยคุณจัดโครงสร้างการตอบกลับของโมเดลภาษาขนาดใหญ่ พวกมันมีสองฟังก์ชัน:\n",
        "\n",
        "- **การให้คำแนะนำในการจัดรูปแบบ** : ตัวแยกวิเคราะห์ผลลัพธ์สามารถใช้เพื่อแทรกคำแนะนำเพิ่มเติมบางอย่างในพร้อมท์ ซึ่งจะช่วยแนะนำ LLM ให้ส่งออกข้อความในรูปแบบที่ทราบวิธีการแยกวิเคราะห์\n",
        "\n",
        "- **การตรวจสอบความถูกต้องและการแยกวิเคราะห์ผลลัพธ์** : ฟังก์ชันหลักคือการนำเอาต์พุตข้อความของ LLM หรือ Chat Model มาแสดงผลในรูปแบบที่มีโครงสร้างมากขึ้น เช่น รายการ XML เป็นต้น ซึ่งอาจรวมถึงการลบข้อมูลที่ไม่จำเป็น การแก้ไขเอาต์พุตที่ไม่สมบูรณ์ และการตรวจสอบความถูกต้องของค่าที่ถูกแยกวิเคราะห์"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0R0k_s5A3gG",
        "outputId": "6dd9a8d2-abb5-480e-8a31-23ed90c0c101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['apple', 'banana', 'cherry']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "items = parser.invoke(\"apple, banana, cherry\")\n",
        "items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTzl67MPEaaa"
      },
      "source": [
        "# LLM Components\n",
        "**การประกอบส่วนประกอบต่าง ๆ ของแอปพลิเคชัน LLM**\n",
        "\n",
        "ส่วนประกอบสำคัญที่คุณได้เรียนรู้มาจนถึงตอนนี้เป็นส่วนประกอบพื้นฐานที่สำคัญของเฟรมเวิร์ก LangChain ซึ่งนำเราไปสู่คำถามที่สำคัญ: คุณจะรวมส่วนประกอบเหล่านี้เข้าด้วยกันอย่างมีประสิทธิภาพเพื่อสร้างแอปพลิเคชัน LLM ของคุณได้อย่างไร?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C0nyIqOEt2v"
      },
      "source": [
        "**อินเทอร์เฟซทั่วไปที่มีเมธอดเหล่านี้:**\n",
        "\n",
        "- **invoke** : แปลงอินพุตเดียวเป็นเอาต์พุต\n",
        "- **batch**: แปลงอินพุตหลายรายการเป็นเอาต์พุตหลายรายการอย่างมีประสิทธิภาพ\n",
        "- **stream** : สตรีมเอาต์พุตจากอินพุตเดียวขณะที่กำลังผลิต\n",
        "ยูทิลิตี้ในตัวสำหรับการลองอีกครั้ง การสำรอง การจัดรูปแบบข้อมูล และการกำหนดค่าเวลาทำงาน (รับอินพุตเดียวและส่งคืนตัววนซ้ำของส่วนต่างๆ ของเอาต์พุตเมื่อมีให้ใช้งาน)\n",
        "\n",
        "**ใน Python แต่ละเมธอดทั้ง 3 ข้างต้นมีคู่เทียบ asyncio**\n",
        "ดังนั้น ส่วนประกอบทั้งหมดจึงทำงานในลักษณะเดียวกัน และอินเทอร์เฟซที่เรียนรู้สำหรับส่วนประกอบใดส่วนประกอบหนึ่งจะใช้ได้กับทั้งหมด เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPALzmnLEsip",
        "outputId": "103a0481-16c8-4179-bce0-b1b182f3e1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\\n\\n1. Hello\\n2. Greetings\\n3. Hey\\n4. Salutations\\n5. What's up\"\n",
            "['\\n1. Hello\\n2. Hey\\n3. Greetings\\n4. Salutations\\n5. Howdy',\n",
            " '\\n\\n1. Farewell\\n2. Adieu\\n3. See you later\\n4. Goodbye\\n5. So long']\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            ".\n",
            " Hello\n",
            "\n",
            "\n",
            "2\n",
            ".\n",
            " G\n",
            "reetings\n",
            "\n",
            "\n",
            "3\n",
            ".\n",
            " Hey\n",
            "\n",
            "\n",
            "4\n",
            ".\n",
            " Sal\n",
            "utations\n",
            "\n",
            "\n",
            "5\n",
            ".\n",
            " How\n",
            "dy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "model = OpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "completion = model.invoke('Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi')\n",
        "# Hi!\n",
        "pprint(completion)\n",
        "completions = model.batch(['Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi',\n",
        "                           'Please answer maximize only 5 keywords on answer. Question: Synonyme of Bye'])\n",
        "# ['Hi!', 'See you!']\n",
        "pprint(completions)\n",
        "for token in model.stream('Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi'):\n",
        "    print(token)\n",
        "    # Good\n",
        "    # bye\n",
        "    # !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWV_ycbiFbBr"
      },
      "source": [
        "ในบางกรณี หากส่วนประกอบพื้นฐานไม่รองรับเอาต์พุตแบบวนซ้ำ จะมีส่วนเดียวที่มีเอาต์พุตทั้งหมด\n",
        "\n",
        "คุณสามารถรวมส่วนประกอบเหล่านี้ได้สองวิธี:\n",
        "\n",
        "- **แบบ Imperative** : เรียกใช้โดยตรง ตัวอย่างเช่น ด้วย model.invoke(...)\n",
        "- **แบบ Declarative** : ด้วย LangChain Expression Language (LCEL) ซึ่งจะกล่าวถึงในหัวข้อถัดไป\n",
        "\n",
        "| Feature        | Imperative        | Declarative        |\n",
        "|----------------|--------------------|--------------------|\n",
        "| Syntax         | All of Python or JavaScript | LCEL              |\n",
        "| Parallel execution | Python: With threads or coroutines <br> JavaScript: With Promise.all | Automatic          |\n",
        "| Streaming      | With yield keyword | Automatic          |\n",
        "| Async execution | With async functions | Automatic          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WtWsS0kKxX7"
      },
      "source": [
        "## Imperative Composition\n",
        "การประมวลผลแบบ Imperative เป็นเพียงชื่อที่หรูหราสำหรับการเขียนโค้ดที่คุณคุ้นเคย การประกอบส่วนประกอบเหล่านี้เข้าด้วยกันในฟังก์ชันและคลาส ต่อไปนี้เป็นตัวอย่างการรวมพร้อมท์ โมเดล และตัวแยกวิเคราะห์ผลลัพธ์ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKfc_oz3Ajro"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# combine them in a function\n",
        "## @chain decorator adds the same Runnable interface for any function you write\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    return model.invoke(prompt)\n",
        "\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZz0m_HLdH-",
        "outputId": "f16c07c9-7a60-4cc8-ea5e-9486c791c0d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, Hey, Greetings, Salutations, Sup'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf9h6HtNC7H"
      },
      "source": [
        "สิ่งที่นำเสนอไปก่อนหน้านี้เป็นตัวอย่างที่สมบูรณ์ของแชทบอท โดยใช้พร้อมท์และโมเดลแชท ดังที่คุณเห็น มันใช้ไวยากรณ์ Python ที่คุ้นเคยและรองรับตรรกะแบบกำหนดเองใดๆ ที่คุณอาจต้องการเพิ่มในฟังก์ชันนั้น\n",
        "\n",
        "\n",
        "**ในทางกลับกัน** หากคุณต้องการเปิดใช้งานการสตรีมหรือการรองรับแบบอะซิงโครนัส คุณจะต้องปรับเปลี่ยนฟังก์ชันของคุณเพื่อรองรับ ตัวอย่างเช่น การรองรับการสตรีมสามารถเพิ่มได้ดังนี้ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg07hIBqLqPl",
        "outputId": "965dbecf-fb24-46a0-e356-ceae45eddad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('content', '1. Hello\\n2. Hey\\n3. Greetings\\n4. Salutations\\n5. Howdy')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 23, 'prompt_tokens': 27, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-eb4bd0d8-e861-4607-a6ab-23a51c86207f-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 27, 'output_tokens': 23, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
          ]
        }
      ],
      "source": [
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.invoke(prompt):\n",
        "        yield token\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yPtepbOaBm"
      },
      "source": [
        "## Declarative Composition\n",
        "\n",
        "**การประมวลผลแบบ Declarative**\n",
        "\n",
        "LangChain Expression Language (LCEL) เป็น ภาษาประกาศ สำหรับการประกอบส่วนประกอบของ LangChain LangChain คอมไพล์การประมวลผล LCEL เป็น แผนการดำเนินการที่ปรับให้เหมาะสม พร้อมการประมวลผลแบบขนาน การสตรีม การติดตาม และการรองรับแบบอะซิงโครนัสโดยอัตโนมัติ\n",
        "\n",
        "มาดูตัวอย่างเดียวกันโดยใช้ LCEL เริ่มต้นใน Python:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCmiUM9gMT-p"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "# combine them with the | operator\n",
        "\n",
        "chatbot = template | model\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHq3AvMOl1z",
        "outputId": "f3e991ca-a319-4386-db91-22606d69fa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'1. Hello\\n2. Hey\\n3. Greetings\\n4. Howdy\\n5. Hi there'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxpCkw5YO_Uy"
      },
      "source": [
        "ที่สำคัญคือ บรรทัดสุดท้ายเหมือนกันในทั้งสองตัวอย่าง นั่นคือ คุณใช้ฟังก์ชันและลำดับ LCEL ในลักษณะเดียวกัน โดยใช้ `invoke/stream/batch` และในเวอร์ชันนี้ คุณไม่จำเป็นต้องทำอะไรเพิ่มเติมเพื่อใช้การสตรีม เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrnHdAwWOwhU",
        "outputId": "9d3cb207-c36f-49f5-a0f1-801fb0e6382b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='Hello' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hey' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' greetings' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hi' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='ya' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hi' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yccWx9AWPJCg",
        "outputId": "c67405fe-3a46-4d85-c246-5fbd9489c175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, Greetings, Hey, Howdy, Hiya'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "result = await chatbot.ainvoke({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "})\n",
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbMjQSKnPfKB"
      },
      "source": [
        "# Summary\n",
        "\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้ คุณได้เรียนรู้เกี่ยวกับส่วนประกอบพื้นฐานและองค์ประกอบสำคัญที่จำเป็นสำหรับการสร้างแอปพลิเคชัน LLM โดยใช้ LangChain แอปพลิเคชัน LLM โดยพื้นฐานแล้วเป็นห่วงโซ่ที่ประกอบด้วยโมเดลภาษาขนาดใหญ่เพื่อทำการคาดการณ์ คำแนะนำพร้อมท์เพื่อนำทางโมเดลไปสู่เอาต์พุตที่ต้องการ และตัวแยกวิเคราะห์ผลลัพธ์ (optional) เพื่อแปลงรูปแบบของเอาต์พุตของโมเดล\n",
        "\n",
        "ส่วนประกอบทั้งหมดของ LangChain ใช้ร่วมกันอินเทอร์เฟซเดียวกันกับเมธอด invoke, stream และ batch เพื่อจัดการอินพุตและเอาต์พุตต่างๆ พวกเขาสามารถรวมและดำเนินการแบบ Imperative ได้โดยการเรียกใช้โดยตรงหรือแบบ Declarative โดยใช้ LangChain Expression Language (LCEL)\n",
        "\n",
        "วิธีการแบบ Imperative มีประโยชน์หากคุณตั้งใจที่จะเขียนตรรกะแบบกำหนดเองจำนวนมาก ในขณะที่วิธีการแบบ Declarative มีประโยชน์สำหรับการประกอบส่วนประกอบที่มีอยู่โดยมีการปรับแต่งจำกัด\n",
        "\n",
        "ในบทต่อไป คุณจะได้เรียนรู้วิธีการจัดเตรียมข้อมูลภายนอกให้กับแชทบอท AI ของคุณในฐานะ บริบท เพื่อให้คุณสามารถสร้างแอปพลิเคชัน LLM ที่ช่วยให้คุณ \"แชท\" กับข้อมูลของคุณได้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Trh4GrPRqw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}