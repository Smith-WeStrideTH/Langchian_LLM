{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0qrYIf4f2n"
      },
      "source": [
        "# Chapter 1. LLM Fundamentals with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iduom2IArR_o",
        "outputId": "b684fee2-a34c-45ec-a7d0-208d88e143f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Langchian_LLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Smith-WeStrideTH/Langchian_LLM.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aV1cHaeEPpJr",
        "outputId": "a7e90f15-5e5a-43f3-bed3-eef4e59b435e"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain langchain_openai langchain_community langchain-text-splitters langchain-postgres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYOd_z945pA"
      },
      "source": [
        "# Introduction Setup\n",
        "<img align=\"left\" src=\"./pics/Figure1-1.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "**การใช้งานตลอด Notebookนี้**\n",
        "\n",
        "- **โมเดลแชท**:  หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Anthropic เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **เวกเตอร์เอมเบดดิ้ง**: หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Cohere เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **ที่เก็บเวกเตอร์**: หากคุณไม่ต้องการใช้ Pgvector (ส่วนขยายโอเพนซอร์สของฐานข้อมูล SQL ที่เป็นที่นิยมอย่าง Postgres) เราขอแนะนำให้ใช้ Weaviate (ที่เก็บเวกเตอร์เฉพาะ) หรือ OpenSearch (คุณสมบัติการค้นหาเวกเตอร์ที่เป็นส่วนหนึ่งของฐานข้อมูลการค้นหาที่เป็นที่นิยม)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YuxZmLB57Re"
      },
      "source": [
        "ความพยายามนี้ไปไกลกว่าเพียงแค่ให้ LLM ทั้งหมดมีวิธีการเดียวกัน โดยมีอาร์กิวเมนต์และค่าส่งคืนที่คล้ายคลึงกัน มาดูตัวอย่างของโมเดลแชทและผู้ให้บริการ LLM ที่ได้รับความนิยมสองรายอย่าง OpenAI และ Anthropic ทั้งคู่มี API แชทที่รับ ข้อความแชท (กำหนดแบบหลวม ๆ ว่าเป็นอ็อบเจ็กต์ที่มีสตริงชนิดและสตริงเนื้อหา) และส่งคืนข้อความใหม่ที่สร้างโดยโมเดล แต่หากคุณพยายามใช้ทั้งสองโมเดลในบทสนทนาเดียวกัน คุณจะพบปัญหาทันที เนื่องจากรูปแบบข้อความแชทของพวกเขาไม่เข้ากันเล็กน้อย LangChain สร้างความเป็นนามธรรมของความแตกต่างเหล่านี้เพื่อเปิดใช้งานการสร้างแอปพลิเคชันที่เป็นอิสระจากผู้ให้บริการโดยแท้จริง ตัวอย่างเช่น ด้วย LangChain บทสนทนาแชทบอทที่คุณใช้ทั้งโมเดล OpenAI และ Anthropic ก็ใช้งานได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWq72Aa6hkT"
      },
      "source": [
        "ทางเลือกอื่น ๆ อินเทอร์เฟซ Chat Model ช่วยให้เกิดการสนทนาแบบโต้ตอบระหว่างผู้ใช้กับโมเดล เหตุผลที่เป็นอินเทอร์เฟซที่แยกต่างหากนั้นเป็นเพราะผู้ให้บริการ LLM ที่ได้รับความนิยม เช่น OpenAI แยกแยะข้อความที่ส่งไปและมาจากโมเดลออกเป็นบทบาทของผู้ใช้ ผู้ช่วย และระบบ (ที่นี่ บทบาทหมายถึงประเภทของเนื้อหาที่ข้อความมี):\n",
        "\n",
        "- **บทบาทระบบ (System role)**\n",
        "\n",
        "ช่วยให้ผู้พัฒนาสามารถระบุคำแนะนำที่โมเดลควรใช้เพื่อตอบคำถามของผู้ใช้\n",
        "\n",
        "- **บทบาทผู้ใช้(User role)**\n",
        "\n",
        "บุคคลที่ถามคำถามและสร้างคำถามที่ส่งไปยังโมเดล\n",
        "\n",
        "- **บทบาทผู้ช่วย(Assistant role)**\n",
        "\n",
        "การตอบสนองของโมเดลต่อคำถามของผู้ใช้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFLbsSz9M0c"
      },
      "source": [
        "## ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EjcPWdh7LxkL"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4QVuB_RPNb",
        "outputId": "fa6c3a6c-152e-4910-a137-d358fd95bbb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='เมืองหลวงของประเทศฝรั่งเศสคือกรุงปารีส (Paris) ครับ/ค่ะ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 42, 'total_tokens': 91, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9b40bfe9-cf85-4880-a0d2-6813e6fdb56b-0', usage_metadata={'input_tokens': 42, 'output_tokens': 49, 'total_tokens': 91, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))  # The API key will be automatically fetched from the OPENAI_API_KEY environment variable\n",
        "\n",
        "prompt = [HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')]\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgTxKRc4Qkt"
      },
      "source": [
        "แทนที่จะเป็นสตริงพร้อมท์เดียว โมเดลแชทใช้ส่วนต่อประสานข้อความแชทประเภทต่างๆ ที่เกี่ยวข้องกับบทบาทที่กล่าวถึงก่อนหน้านี้ ซึ่งรวมถึงสิ่งต่อไปนี้:\n",
        "\n",
        "- HumanMessage ข้อความที่ส่งจากมุมมองของมนุษย์ โดยมีบทบาทเป็น ผู้ใช้\n",
        "- AIMessage ข้อความที่ส่งจากมุมมองของ AI ที่มนุษย์โต้ตอบด้วย โดยมีบทบาทเป็น ผู้ช่วย\n",
        "- SystemMessage ข้อความที่ตั้งค่าคำแนะนำที่ AI ควรปฏิบัติตาม โดยมีบทบาทเป็น ระบบ\n",
        "- ChatMessage ข้อความที่อนุญาตให้ตั้งค่าบทบาทโดยพลการ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E0IIh1KPx8P",
        "outputId": "59bd9735-8e81-47d3-ddbf-68cf5ee62dd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ปารีส!!!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 108, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fd5b2175-075f-4cd6-b4f5-1efacd9ec668-0', usage_metadata={'input_tokens': 108, 'output_tokens': 6, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "system_msg = SystemMessage('คุณเป็นผู้ช่วยที่เป็นประโยชน์ที่ตอบคำถามด้วยเครื่องหมาย exclamation marks 3 ตัว')\n",
        "human_msg = HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')\n",
        "completion = model.invoke([system_msg, human_msg])\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBULIqmU74cU"
      },
      "source": [
        "## PromptTemplate\n",
        "โชคดีที่ LangChain มีอินเทอร์เฟซเทมเพลตพร้อมท์ที่ทำให้การสร้างพร้อมท์ที่มีอินพุตแบบไดนามิกเป็นเรื่องง่าย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXC2QSYk64GG",
        "outputId": "08bc74e1-f7d0-4766-fd77-f73cfaf3ec3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\\nContext: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\\nQuestion: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\\nAnswer: ')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nisV9Vq-8jaW",
        "outputId": "b6c75902-6a8b-4c96-b776-15abab6546a4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM ได้แก่ Hugging Face, OpenAI และ Cohere ผ่านไลบรารี transformers และ openai และ cohere ตามลำดับ'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# both `template` and `model` can be reused many times\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "\n",
        "model = OpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "# `prompt` and `completion` are the results of using template and model once\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDPjs4z9-wi"
      },
      "source": [
        "## ChatPromptTemplate\n",
        "หากคุณกำลังมองหาการสร้างแอปพลิเคชันแชท AI เทมเพลตพร้อมท์แชทสามารถใช้แทนเพื่อให้ข้อมูลอินพุตแบบไดนามิกโดยยึดตามบทบาทของข้อความแชท เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYjexTbO9GVY",
        "outputId": "9db37fd5-952b-4bd5-a011-846517b88390"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x67TYDjP-rQ_"
      },
      "source": [
        "สังเกตว่าพร้อมท์ประกอบด้วยคำแนะนำใน SystemMessage และ HumanMessages สองข้อความที่มีตัวแปร context และ question แบบไดนามิก คุณยังคงสามารถจัดรูปแบบเทมเพลตในลักษณะเดียวกัน และรับพร้อมท์แบบคงที่ที่คุณสามารถส่งผ่านไปยังโมเดลภาษาขนาดใหญ่สำหรับผลลัพธ์การทำนาย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A2CASUrW91bE",
        "outputId": "c7620b5d-82a1-4a71-d253-ce554d822c1a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSystem: ผู้ให้บริการ LLM ปัจจุบันได้แก่ Hugging Face, OpenAI, และ Cohere ผ่านไลบรารี transformers, openai, และ cohere ตามลำดับ '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "model = OpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uws0QjeG_KP5"
      },
      "source": [
        "## JSON Output\n",
        "\n",
        "รูปแบบที่พบมากที่สุดในการสร้างด้วย LLM คือ JSON ซึ่งสามารถนำไปใช้เพื่อ เช่น:\n",
        "\n",
        "- ส่งไปยังโค้ดฝั่งไคลเอนต์ของคุณผ่านเครือข่าย\n",
        "\n",
        "- บันทึกไปยังฐานข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieDQgsHd_k6V"
      },
      "source": [
        "เมื่อสร้าง JSON งานแรกคือการกำหนดรูปแบบข้อมูล (schema) ที่คุณต้องการให้ LLM เคารพเมื่อสร้างผลลัพธ์ จากนั้น คุณควรใส่รูปแบบข้อมูลนั้นไว้ในพร้อมท์ พร้อมกับข้อความที่คุณต้องการใช้เป็นแหล่งที่มา มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OHoJqkp-2p6",
        "outputId": "e53c52fb-60cd-42a7-b47a-2059dd80e4f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1363: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='อิฐหนึ่งปอนด์หนักกว่าขนนกหนึ่งปอนด์', justification='อิฐมีความหนาและหนักกว่าขนนกที่มีโครงสร้างเป็นขนนก')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้พร้อมเหตุผลสนับสนุนคำตอบ'''\n",
        "    answer: str\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้'''\n",
        "    justification: str\n",
        "    '''เหตุผลสนับสนุนคำตอบ'''\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'), temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"อะไรหนักกว่า ระหว่างอิฐหนึ่งปอนด์กับขนนกหนึ่งปอนด์\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltuwEJE__10",
        "outputId": "96a9eabd-eb8e-4753-ac11-be27377685c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1363: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='They weigh the same.', justification='A pound of bricks and a pound of feathers both weigh one pound. The weight is the same, but the volume or density of the two substances is different.')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''An answer to the user question along with justification for the answer.'''\n",
        "    answer: str\n",
        "    '''The answer to the user's question'''\n",
        "    justification: str\n",
        "    '''Justification for the answer'''\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'), temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxK8v2a_A37q"
      },
      "source": [
        "ดังนั้น ขั้นแรกให้กำหนดรูปแบบข้อมูล (schema) ใน Python การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Pydantic (ไลบรารีที่ใช้สำหรับตรวจสอบความถูกต้องของข้อมูลตามรูปแบบข้อมูล) ใน JS การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Zod (ไลบรารีที่เทียบเท่ากัน)\n",
        "\n",
        "**วิธีการ with_structured_output จะใช้รูปแบบข้อมูลนั้นสำหรับสองสิ่ง:**\n",
        "\n",
        "- รูปแบบข้อมูลจะถูกแปลงเป็นอ็อบเจ็กต์ JSONSchema (รูปแบบ JSON ที่ใช้เพื่ออธิบายรูปร่าง ประเภท ชื่อ คำอธิบาย ของข้อมูล JSON) ซึ่งจะถูกส่งไปยัง LLM LangChain เลือกวิธีที่ดีที่สุดในการทำสิ่งนี้โดยปกติจะเป็นการเรียกใช้ฟังก์ชันหรือการส่งพร้อมท์\n",
        "- รูปแบบข้อมูลจะถูกใช้เพื่อตรวจสอบความถูกต้องของเอาต์พุตที่ส่งคืนโดย LLM ก่อนที่จะส่งคืน ซึ่งจะช่วยให้มั่นใจได้ว่าเอาต์พุตที่สร้างขึ้นเป็นไปตามรูปแบบข้อมูลที่คุณส่งผ่านมาอย่างแน่นอน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICP-do9_Bn3C"
      },
      "source": [
        "## Other Output\n",
        "**รูปแบบที่อ่านได้ด้วยเครื่องจักรอื่น ๆ ที่มีตัวแยกวิเคราะห์ผลลัพธ์**\n",
        "\n",
        "คุณยังสามารถใช้อัลกอริทึม LLM หรือ Chat Model เพื่อสร้างผลลัพธ์ในรูปแบบอื่น เช่น CSV หรือ XML ซึ่งเป็นที่ที่ตัวแยกวิเคราะห์ผลลัพธ์มีประโยชน์ ตัวแยกวิเคราะห์ผลลัพธ์เป็นคลาสที่ช่วยคุณจัดโครงสร้างการตอบกลับของโมเดลภาษาขนาดใหญ่ พวกมันมีสองฟังก์ชัน:\n",
        "\n",
        "- **การให้คำแนะนำในการจัดรูปแบบ** : ตัวแยกวิเคราะห์ผลลัพธ์สามารถใช้เพื่อแทรกคำแนะนำเพิ่มเติมบางอย่างในพร้อมท์ ซึ่งจะช่วยแนะนำ LLM ให้ส่งออกข้อความในรูปแบบที่ทราบวิธีการแยกวิเคราะห์\n",
        "\n",
        "- **การตรวจสอบความถูกต้องและการแยกวิเคราะห์ผลลัพธ์** : ฟังก์ชันหลักคือการนำเอาต์พุตข้อความของ LLM หรือ Chat Model มาแสดงผลในรูปแบบที่มีโครงสร้างมากขึ้น เช่น รายการ XML เป็นต้น ซึ่งอาจรวมถึงการลบข้อมูลที่ไม่จำเป็น การแก้ไขเอาต์พุตที่ไม่สมบูรณ์ และการตรวจสอบความถูกต้องของค่าที่ถูกแยกวิเคราะห์"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0R0k_s5A3gG",
        "outputId": "6dd9a8d2-abb5-480e-8a31-23ed90c0c101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['apple', 'banana', 'cherry']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "items = parser.invoke(\"apple, banana, cherry\")\n",
        "items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTzl67MPEaaa"
      },
      "source": [
        "# LLM Components\n",
        "**การประกอบส่วนประกอบต่าง ๆ ของแอปพลิเคชัน LLM**\n",
        "\n",
        "ส่วนประกอบสำคัญที่คุณได้เรียนรู้มาจนถึงตอนนี้เป็นส่วนประกอบพื้นฐานที่สำคัญของเฟรมเวิร์ก LangChain ซึ่งนำเราไปสู่คำถามที่สำคัญ: คุณจะรวมส่วนประกอบเหล่านี้เข้าด้วยกันอย่างมีประสิทธิภาพเพื่อสร้างแอปพลิเคชัน LLM ของคุณได้อย่างไร?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C0nyIqOEt2v"
      },
      "source": [
        "**อินเทอร์เฟซทั่วไปที่มีเมธอดเหล่านี้:**\n",
        "\n",
        "- **invoke** : แปลงอินพุตเดียวเป็นเอาต์พุต\n",
        "- **batch**: แปลงอินพุตหลายรายการเป็นเอาต์พุตหลายรายการอย่างมีประสิทธิภาพ\n",
        "- **stream** : สตรีมเอาต์พุตจากอินพุตเดียวขณะที่กำลังผลิต\n",
        "ยูทิลิตี้ในตัวสำหรับการลองอีกครั้ง การสำรอง การจัดรูปแบบข้อมูล และการกำหนดค่าเวลาทำงาน (รับอินพุตเดียวและส่งคืนตัววนซ้ำของส่วนต่างๆ ของเอาต์พุตเมื่อมีให้ใช้งาน)\n",
        "\n",
        "**ใน Python แต่ละเมธอดทั้ง 3 ข้างต้นมีคู่เทียบ asyncio**\n",
        "ดังนั้น ส่วนประกอบทั้งหมดจึงทำงานในลักษณะเดียวกัน และอินเทอร์เฟซที่เรียนรู้สำหรับส่วนประกอบใดส่วนประกอบหนึ่งจะใช้ได้กับทั้งหมด เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPALzmnLEsip",
        "outputId": "103a0481-16c8-4179-bce0-b1b182f3e1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\\n\\n1. Hello\\n2. Greetings\\n3. Hey\\n4. Salutations\\n5. What's up\"\n",
            "['\\n1. Hello\\n2. Hey\\n3. Greetings\\n4. Salutations\\n5. Howdy',\n",
            " '\\n\\n1. Farewell\\n2. Adieu\\n3. See you later\\n4. Goodbye\\n5. So long']\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            ".\n",
            " Hello\n",
            "\n",
            "\n",
            "2\n",
            ".\n",
            " G\n",
            "reetings\n",
            "\n",
            "\n",
            "3\n",
            ".\n",
            " Hey\n",
            "\n",
            "\n",
            "4\n",
            ".\n",
            " Sal\n",
            "utations\n",
            "\n",
            "\n",
            "5\n",
            ".\n",
            " How\n",
            "dy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "model = OpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "completion = model.invoke('Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi')\n",
        "# Hi!\n",
        "pprint(completion)\n",
        "completions = model.batch(['Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi',\n",
        "                           'Please answer maximize only 5 keywords on answer. Question: Synonyme of Bye'])\n",
        "# ['Hi!', 'See you!']\n",
        "pprint(completions)\n",
        "for token in model.stream('Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi'):\n",
        "    print(token)\n",
        "    # Good\n",
        "    # bye\n",
        "    # !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWV_ycbiFbBr"
      },
      "source": [
        "ในบางกรณี หากส่วนประกอบพื้นฐานไม่รองรับเอาต์พุตแบบวนซ้ำ จะมีส่วนเดียวที่มีเอาต์พุตทั้งหมด\n",
        "\n",
        "คุณสามารถรวมส่วนประกอบเหล่านี้ได้สองวิธี:\n",
        "\n",
        "- **แบบ Imperative** : เรียกใช้โดยตรง ตัวอย่างเช่น ด้วย model.invoke(...)\n",
        "- **แบบ Declarative** : ด้วย LangChain Expression Language (LCEL) ซึ่งจะกล่าวถึงในหัวข้อถัดไป\n",
        "\n",
        "| Feature        | Imperative        | Declarative        |\n",
        "|----------------|--------------------|--------------------|\n",
        "| Syntax         | All of Python or JavaScript | LCEL              |\n",
        "| Parallel execution | Python: With threads or coroutines <br> JavaScript: With Promise.all | Automatic          |\n",
        "| Streaming      | With yield keyword | Automatic          |\n",
        "| Async execution | With async functions | Automatic          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WtWsS0kKxX7"
      },
      "source": [
        "## Imperative Composition\n",
        "การประมวลผลแบบ Imperative เป็นเพียงชื่อที่หรูหราสำหรับการเขียนโค้ดที่คุณคุ้นเคย การประกอบส่วนประกอบเหล่านี้เข้าด้วยกันในฟังก์ชันและคลาส ต่อไปนี้เป็นตัวอย่างการรวมพร้อมท์ โมเดล และตัวแยกวิเคราะห์ผลลัพธ์ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iKfc_oz3Ajro"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# combine them in a function\n",
        "## @chain decorator adds the same Runnable interface for any function you write\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    return model.invoke(prompt)\n",
        "\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZz0m_HLdH-",
        "outputId": "f16c07c9-7a60-4cc8-ea5e-9486c791c0d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, Hey, Greetings, Salutations, Sup'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf9h6HtNC7H"
      },
      "source": [
        "สิ่งที่นำเสนอไปก่อนหน้านี้เป็นตัวอย่างที่สมบูรณ์ของแชทบอท โดยใช้พร้อมท์และโมเดลแชท ดังที่คุณเห็น มันใช้ไวยากรณ์ Python ที่คุ้นเคยและรองรับตรรกะแบบกำหนดเองใดๆ ที่คุณอาจต้องการเพิ่มในฟังก์ชันนั้น\n",
        "\n",
        "\n",
        "**ในทางกลับกัน** หากคุณต้องการเปิดใช้งานการสตรีมหรือการรองรับแบบอะซิงโครนัส คุณจะต้องปรับเปลี่ยนฟังก์ชันของคุณเพื่อรองรับ ตัวอย่างเช่น การรองรับการสตรีมสามารถเพิ่มได้ดังนี้ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg07hIBqLqPl",
        "outputId": "965dbecf-fb24-46a0-e356-ceae45eddad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('content', '1. Hello\\n2. Hey\\n3. Greetings\\n4. Salutations\\n5. Howdy')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 23, 'prompt_tokens': 27, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-eb4bd0d8-e861-4607-a6ab-23a51c86207f-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 27, 'output_tokens': 23, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
          ]
        }
      ],
      "source": [
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.invoke(prompt):\n",
        "        yield token\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yPtepbOaBm"
      },
      "source": [
        "## Declarative Composition\n",
        "\n",
        "**การประมวลผลแบบ Declarative**\n",
        "\n",
        "LangChain Expression Language (LCEL) เป็น ภาษาประกาศ สำหรับการประกอบส่วนประกอบของ LangChain LangChain คอมไพล์การประมวลผล LCEL เป็น แผนการดำเนินการที่ปรับให้เหมาะสม พร้อมการประมวลผลแบบขนาน การสตรีม การติดตาม และการรองรับแบบอะซิงโครนัสโดยอัตโนมัติ\n",
        "\n",
        "มาดูตัวอย่างเดียวกันโดยใช้ LCEL เริ่มต้นใน Python:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QCmiUM9gMT-p"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "# combine them with the | operator\n",
        "\n",
        "chatbot = template | model\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHq3AvMOl1z",
        "outputId": "f3e991ca-a319-4386-db91-22606d69fa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'1. Hello\\n2. Hey\\n3. Greetings\\n4. Howdy\\n5. Hi there'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxpCkw5YO_Uy"
      },
      "source": [
        "ที่สำคัญคือ บรรทัดสุดท้ายเหมือนกันในทั้งสองตัวอย่าง นั่นคือ คุณใช้ฟังก์ชันและลำดับ LCEL ในลักษณะเดียวกัน โดยใช้ `invoke/stream/batch` และในเวอร์ชันนี้ คุณไม่จำเป็นต้องทำอะไรเพิ่มเติมเพื่อใช้การสตรีม เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrnHdAwWOwhU",
        "outputId": "9d3cb207-c36f-49f5-a0f1-801fb0e6382b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='Hello' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hey' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' greetings' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hi' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='ya' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content=' hi' additional_kwargs={} response_metadata={} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n",
            "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-e399f125-2360-40f5-8a17-95f19346b41b'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yccWx9AWPJCg",
        "outputId": "c67405fe-3a46-4d85-c246-5fbd9489c175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, Greetings, Hey, Howdy, Hiya'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "result = await chatbot.ainvoke({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "})\n",
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbMjQSKnPfKB"
      },
      "source": [
        "# Summary\n",
        "\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้ คุณได้เรียนรู้เกี่ยวกับส่วนประกอบพื้นฐานและองค์ประกอบสำคัญที่จำเป็นสำหรับการสร้างแอปพลิเคชัน LLM โดยใช้ LangChain แอปพลิเคชัน LLM โดยพื้นฐานแล้วเป็นห่วงโซ่ที่ประกอบด้วยโมเดลภาษาขนาดใหญ่เพื่อทำการคาดการณ์ คำแนะนำพร้อมท์เพื่อนำทางโมเดลไปสู่เอาต์พุตที่ต้องการ และตัวแยกวิเคราะห์ผลลัพธ์ (optional) เพื่อแปลงรูปแบบของเอาต์พุตของโมเดล\n",
        "\n",
        "ส่วนประกอบทั้งหมดของ LangChain ใช้ร่วมกันอินเทอร์เฟซเดียวกันกับเมธอด invoke, stream และ batch เพื่อจัดการอินพุตและเอาต์พุตต่างๆ พวกเขาสามารถรวมและดำเนินการแบบ Imperative ได้โดยการเรียกใช้โดยตรงหรือแบบ Declarative โดยใช้ LangChain Expression Language (LCEL)\n",
        "\n",
        "วิธีการแบบ Imperative มีประโยชน์หากคุณตั้งใจที่จะเขียนตรรกะแบบกำหนดเองจำนวนมาก ในขณะที่วิธีการแบบ Declarative มีประโยชน์สำหรับการประกอบส่วนประกอบที่มีอยู่โดยมีการปรับแต่งจำกัด\n",
        "\n",
        "ในบทต่อไป คุณจะได้เรียนรู้วิธีการจัดเตรียมข้อมูลภายนอกให้กับแชทบอท AI ของคุณในฐานะ บริบท เพื่อให้คุณสามารถสร้างแอปพลิเคชัน LLM ที่ช่วยให้คุณ \"แชท\" กับข้อมูลของคุณได้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Trh4GrPRqw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
