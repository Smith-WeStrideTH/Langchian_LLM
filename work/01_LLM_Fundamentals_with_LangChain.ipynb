{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uOhxPG8qku43",
        "outputId": "0635b6b8-7b77-478a-de46-4ca3b0e7aac2"
      },
      "outputs": [],
      "source": [
        "# !pip install ipython==7.34.0\n",
        "# !pip install jupyter_client==6.1.12\n",
        "# !pip install jupyter_core==5.7.2\n",
        "# !pip install notebook==6.5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aV1cHaeEPpJr",
        "outputId": "776e474d-0489-4281-9fd1-bf8fce5069dd"
      },
      "outputs": [],
      "source": [
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain==0.3.14 langchain_openai langchain_community==0.3.14 langchain-text-splitters langchain-postgres==0.0.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0qrYIf4f2n"
      },
      "source": [
        "# Chapter 1. LLM Fundamentals with LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYOd_z945pA"
      },
      "source": [
        "# Introduction Setup\n",
        "<img align=\"top\" src=\"../work/pics/Figure1-1.png\"     style=\" width:580px; padding: 10px; \" >\n",
        "\n",
        "**การใช้งานตลอด Notebookนี้**\n",
        "\n",
        "- **โมเดลแชท**:  หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Anthropic เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **เวกเตอร์เอมเบดดิ้ง**: หากคุณไม่ต้องการใช้ OpenAI (API เชิงพาณิชย์) เราขอแนะนำ Cohere เป็นทางเลือกเชิงพาณิชย์หรือ Ollama เป็นทางเลือกโอเพนซอร์ส\n",
        "\n",
        "- **ที่เก็บเวกเตอร์**: หากคุณไม่ต้องการใช้ Pgvector (ส่วนขยายโอเพนซอร์สของฐานข้อมูล SQL ที่เป็นที่นิยมอย่าง Postgres) เราขอแนะนำให้ใช้ Weaviate (ที่เก็บเวกเตอร์เฉพาะ) หรือ OpenSearch (คุณสมบัติการค้นหาเวกเตอร์ที่เป็นส่วนหนึ่งของฐานข้อมูลการค้นหาที่เป็นที่นิยม)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YuxZmLB57Re"
      },
      "source": [
        "ความพยายามนี้ไปไกลกว่าเพียงแค่ให้ LLM ทั้งหมดมีวิธีการเดียวกัน โดยมีอาร์กิวเมนต์และค่าส่งคืนที่คล้ายคลึงกัน มาดูตัวอย่างของโมเดลแชทและผู้ให้บริการ LLM ที่ได้รับความนิยมสองรายอย่าง OpenAI และ Anthropic ทั้งคู่มี API แชทที่รับ ข้อความแชท (กำหนดแบบหลวม ๆ ว่าเป็นอ็อบเจ็กต์ที่มีสตริงชนิดและสตริงเนื้อหา) และส่งคืนข้อความใหม่ที่สร้างโดยโมเดล แต่หากคุณพยายามใช้ทั้งสองโมเดลในบทสนทนาเดียวกัน คุณจะพบปัญหาทันที เนื่องจากรูปแบบข้อความแชทของพวกเขาไม่เข้ากันเล็กน้อย LangChain สร้างความเป็นนามธรรมของความแตกต่างเหล่านี้เพื่อเปิดใช้งานการสร้างแอปพลิเคชันที่เป็นอิสระจากผู้ให้บริการโดยแท้จริง ตัวอย่างเช่น ด้วย LangChain บทสนทนาแชทบอทที่คุณใช้ทั้งโมเดล OpenAI และ Anthropic ก็ใช้งานได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWq72Aa6hkT"
      },
      "source": [
        "ทางเลือกอื่น ๆ อินเทอร์เฟซ Chat Model ช่วยให้เกิดการสนทนาแบบโต้ตอบระหว่างผู้ใช้กับโมเดล เหตุผลที่เป็นอินเทอร์เฟซที่แยกต่างหากนั้นเป็นเพราะผู้ให้บริการ LLM ที่ได้รับความนิยม เช่น OpenAI แยกแยะข้อความที่ส่งไปและมาจากโมเดลออกเป็นบทบาทของผู้ใช้ ผู้ช่วย และระบบ (ที่นี่ บทบาทหมายถึงประเภทของเนื้อหาที่ข้อความมี):\n",
        "\n",
        "- **บทบาทระบบ (System role)**\n",
        "\n",
        "ช่วยให้ผู้พัฒนาสามารถระบุคำแนะนำที่โมเดลควรใช้เพื่อตอบคำถามของผู้ใช้\n",
        "\n",
        "- **บทบาทผู้ใช้(User role)**\n",
        "\n",
        "บุคคลที่ถามคำถามและสร้างคำถามที่ส่งไปยังโมเดล\n",
        "\n",
        "- **บทบาทผู้ช่วย(Assistant role)**\n",
        "\n",
        "การตอบสนองของโมเดลต่อคำถามของผู้ใช้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFLbsSz9M0c"
      },
      "source": [
        "## ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjcPWdh7LxkL"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WTWAYJOXTY4",
        "outputId": "36292fef-505f-4203-b80a-539ce1d20bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# prompt: check api from '/credential/cred.env'\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')  # Replace with the actual path if needed\n",
        "\n",
        "# Access the API key\n",
        "api_key = os.getenv(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was loaded successfully\n",
        "if api_key:\n",
        "    print(\"API key loaded successfully.\")\n",
        "    # You can now use the api_key with the OpenAI API\n",
        "else:\n",
        "    print(\"Error: API_KEY not found in .env file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWi7xtUqXsZn"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4QVuB_RPNb",
        "outputId": "1b73f6de-9c18-4a58-9c6e-070ce0de5fd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='เมืองหลวงของประเทศฝรั่งเศสคือปารีส (Paris) ครับ/ค่ะ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 42, 'total_tokens': 88, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b3b63f3b-b86a-490e-9773-c4c9c64acf55-0', usage_metadata={'input_tokens': 42, 'output_tokens': 46, 'total_tokens': 88, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "model = ChatOpenAI()  # The API key will be automatically fetched from the OPENAI_API_KEY environment variable\n",
        "\n",
        "prompt = [HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')]\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgTxKRc4Qkt"
      },
      "source": [
        "แทนที่จะเป็นสตริงพร้อมท์เดียว โมเดลแชทใช้ส่วนต่อประสานข้อความแชทประเภทต่างๆ ที่เกี่ยวข้องกับบทบาทที่กล่าวถึงก่อนหน้านี้ ซึ่งรวมถึงสิ่งต่อไปนี้:\n",
        "\n",
        "- HumanMessage ข้อความที่ส่งจากมุมมองของมนุษย์ โดยมีบทบาทเป็น ผู้ใช้\n",
        "- AIMessage ข้อความที่ส่งจากมุมมองของ AI ที่มนุษย์โต้ตอบด้วย โดยมีบทบาทเป็น ผู้ช่วย\n",
        "- SystemMessage ข้อความที่ตั้งค่าคำแนะนำที่ AI ควรปฏิบัติตาม โดยมีบทบาทเป็น ระบบ\n",
        "- ChatMessage ข้อความที่อนุญาตให้ตั้งค่าบทบาทโดยพลการ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E0IIh1KPx8P",
        "outputId": "56e616ba-2a13-4dbe-874a-07e28bf985a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ปารีส!!!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 108, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e15f22c3-21c8-4367-aca9-05dcf68c0cca-0', usage_metadata={'input_tokens': 108, 'output_tokens': 6, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()\n",
        "system_msg = SystemMessage('คุณเป็นผู้ช่วยที่เป็นประโยชน์ที่ตอบคำถามด้วยเครื่องหมาย exclamation marks 3 ตัว')\n",
        "human_msg = HumanMessage('เมืองหลวงของประเทศฝรั่งเศสคือที่ใด?')\n",
        "completion = model.invoke([system_msg, human_msg])\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBULIqmU74cU"
      },
      "source": [
        "## PromptTemplate\n",
        "โชคดีที่ LangChain มีอินเทอร์เฟซเทมเพลตพร้อมท์ที่ทำให้การสร้างพร้อมท์ที่มีอินพุตแบบไดนามิกเป็นเรื่องง่าย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXC2QSYk64GG",
        "outputId": "6e63371b-bda0-4f30-fd6b-9bd2cd3a36c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\\nContext: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\\nQuestion: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\\nAnswer: ')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisV9Vq-8jaW",
        "outputId": "65b7bf4d-0b7f-4bb1-e44e-82b2d965d75b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'รายชื่อตามลำดับ: Hugging Face, OpenAI, Cohere'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# both `template` and `model` can be reused many times\n",
        "template = PromptTemplate.from_template(\"\"\"ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: \"\"\")\n",
        "\n",
        "model = OpenAI()\n",
        "# `prompt` and `completion` are the results of using template and model once\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDPjs4z9-wi"
      },
      "source": [
        "## ChatPromptTemplate\n",
        "หากคุณกำลังมองหาการสร้างแอปพลิเคชันแชท AI เทมเพลตพร้อมท์แชทสามารถใช้แทนเพื่อให้ข้อมูลอินพุตแบบไดนามิกโดยยึดตามบทบาทของข้อความแชท เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYjexTbO9GVY",
        "outputId": "694ffaa4-2347-453f-ac92-d31a0d4d4779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x67TYDjP-rQ_"
      },
      "source": [
        "สังเกตว่าพร้อมท์ประกอบด้วยคำแนะนำใน SystemMessage และ HumanMessages สองข้อความที่มีตัวแปร context และ question แบบไดนามิก คุณยังคงสามารถจัดรูปแบบเทมเพลตในลักษณะเดียวกัน และรับพร้อมท์แบบคงที่ที่คุณสามารถส่งผ่านไปยังโมเดลภาษาขนาดใหญ่สำหรับผลลัพธ์การทำนาย เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2CASUrW91bE",
        "outputId": "a277d6ac-b3fd-48f0-84c8-29e0a0a072b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nSystem: Hugging Face, OpenAI และ Cohere'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'ตอบคำถามโดยยึดตามบริบทด้านล่าง หากไม่สามารถตอบคำถามโดยใช้ข้อมูลที่ให้ไว้ ให้ตอบว่า \"ฉันไม่รู้\" '),\n",
        "    ('human', 'Context: {context}'),\n",
        "    ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "model = OpenAI()\n",
        "\n",
        "prompt = template.invoke({\n",
        "    \"context\": \"ความก้าวหน้าล่าสุดใน NLP กำลังถูกขับเคลื่อนโดย Large Language Models (LLMs) โมเดลเหล่านี้มีประสิทธิภาพเหนือกว่าโมเดลขนาดเล็กและกลายเป็นสิ่งที่มีคุณค่าอย่างยิ่งสำหรับนักพัฒนาที่กำลังสร้างแอปพลิเคชันที่มีความสามารถด้าน NLP นักพัฒนาสามารถเข้าถึงโมเดลเหล่านี้ได้ผ่านไลบรารี transformers ของ Hugging Face หรือโดยการใช้ข้อเสนอของ OpenAI และ Cohere ผ่านไลบรารี openai และ cohere ตามลำดับ\",\n",
        "    \"question\": \"ผู้ให้บริการโมเดลรายใดบ้างที่ให้บริการ LLM?\"\n",
        "})\n",
        "\n",
        "completion = model.invoke(prompt)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uws0QjeG_KP5"
      },
      "source": [
        "## JSON Output\n",
        "\n",
        "รูปแบบที่พบมากที่สุดในการสร้างด้วย LLM คือ JSON ซึ่งสามารถนำไปใช้เพื่อ เช่น:\n",
        "\n",
        "- ส่งไปยังโค้ดฝั่งไคลเอนต์ของคุณผ่านเครือข่าย\n",
        "\n",
        "- บันทึกไปยังฐานข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieDQgsHd_k6V"
      },
      "source": [
        "เมื่อสร้าง JSON งานแรกคือการกำหนดรูปแบบข้อมูล (schema) ที่คุณต้องการให้ LLM เคารพเมื่อสร้างผลลัพธ์ จากนั้น คุณควรใส่รูปแบบข้อมูลนั้นไว้ในพร้อมท์ พร้อมกับข้อความที่คุณต้องการใช้เป็นแหล่งที่มา มาดูตัวอย่างกัน เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OHoJqkp-2p6",
        "outputId": "095547ed-322a-4f43-8f87-8dad0fe2ec98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='อิฐหนึ่งปอนด์หนักกว่าขนนกหนึ่งปอนด์', justification='อิฐมีความหนาและหนักกว่าขนนก')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้พร้อมเหตุผลสนับสนุนคำตอบ'''\n",
        "    answer: str\n",
        "    '''คำตอบสำหรับคำถามของผู้ใช้'''\n",
        "    justification: str\n",
        "    '''เหตุผลสนับสนุนคำตอบ'''\n",
        "\n",
        "llm = ChatOpenAI(temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"อะไรหนักกว่า ระหว่างอิฐหนึ่งปอนด์กับขนนกหนึ่งปอนด์\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltuwEJE__10",
        "outputId": "9000c0cd-fda3-4794-e4e7-f0407e9669fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='They weigh the same', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    '''An answer to the user question along with justification for the answer.'''\n",
        "    answer: str\n",
        "    '''The answer to the user's question'''\n",
        "    justification: str\n",
        "    '''Justification for the answer'''\n",
        "\n",
        "llm = ChatOpenAI(temperature= 0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxK8v2a_A37q"
      },
      "source": [
        "ดังนั้น ขั้นแรกให้กำหนดรูปแบบข้อมูล (schema) ใน Python การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Pydantic (ไลบรารีที่ใช้สำหรับตรวจสอบความถูกต้องของข้อมูลตามรูปแบบข้อมูล) ใน JS การทำเช่นนี้ทำได้ง่ายที่สุดโดยใช้ Zod (ไลบรารีที่เทียบเท่ากัน)\n",
        "\n",
        "**วิธีการ with_structured_output จะใช้รูปแบบข้อมูลนั้นสำหรับสองสิ่ง:**\n",
        "\n",
        "- รูปแบบข้อมูลจะถูกแปลงเป็นอ็อบเจ็กต์ JSONSchema (รูปแบบ JSON ที่ใช้เพื่ออธิบายรูปร่าง ประเภท ชื่อ คำอธิบาย ของข้อมูล JSON) ซึ่งจะถูกส่งไปยัง LLM LangChain เลือกวิธีที่ดีที่สุดในการทำสิ่งนี้โดยปกติจะเป็นการเรียกใช้ฟังก์ชันหรือการส่งพร้อมท์\n",
        "- รูปแบบข้อมูลจะถูกใช้เพื่อตรวจสอบความถูกต้องของเอาต์พุตที่ส่งคืนโดย LLM ก่อนที่จะส่งคืน ซึ่งจะช่วยให้มั่นใจได้ว่าเอาต์พุตที่สร้างขึ้นเป็นไปตามรูปแบบข้อมูลที่คุณส่งผ่านมาอย่างแน่นอน\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICP-do9_Bn3C"
      },
      "source": [
        "## Other Output\n",
        "**รูปแบบที่อ่านได้ด้วยเครื่องจักรอื่น ๆ ที่มีตัวแยกวิเคราะห์ผลลัพธ์**\n",
        "\n",
        "คุณยังสามารถใช้อัลกอริทึม LLM หรือ Chat Model เพื่อสร้างผลลัพธ์ในรูปแบบอื่น เช่น CSV หรือ XML ซึ่งเป็นที่ที่ตัวแยกวิเคราะห์ผลลัพธ์มีประโยชน์ ตัวแยกวิเคราะห์ผลลัพธ์เป็นคลาสที่ช่วยคุณจัดโครงสร้างการตอบกลับของโมเดลภาษาขนาดใหญ่ พวกมันมีสองฟังก์ชัน:\n",
        "\n",
        "- **การให้คำแนะนำในการจัดรูปแบบ** : ตัวแยกวิเคราะห์ผลลัพธ์สามารถใช้เพื่อแทรกคำแนะนำเพิ่มเติมบางอย่างในพร้อมท์ ซึ่งจะช่วยแนะนำ LLM ให้ส่งออกข้อความในรูปแบบที่ทราบวิธีการแยกวิเคราะห์\n",
        "\n",
        "- **การตรวจสอบความถูกต้องและการแยกวิเคราะห์ผลลัพธ์** : ฟังก์ชันหลักคือการนำเอาต์พุตข้อความของ LLM หรือ Chat Model มาแสดงผลในรูปแบบที่มีโครงสร้างมากขึ้น เช่น รายการ XML เป็นต้น ซึ่งอาจรวมถึงการลบข้อมูลที่ไม่จำเป็น การแก้ไขเอาต์พุตที่ไม่สมบูรณ์ และการตรวจสอบความถูกต้องของค่าที่ถูกแยกวิเคราะห์"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0R0k_s5A3gG",
        "outputId": "581f9211-7ff4-4604-c749-529a9a26fb6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['apple', 'banana', 'cherry']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "items = parser.invoke(\"apple, banana, cherry\")\n",
        "items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTzl67MPEaaa"
      },
      "source": [
        "# LLM Components\n",
        "**การประกอบส่วนประกอบต่าง ๆ ของแอปพลิเคชัน LLM**\n",
        "\n",
        "ส่วนประกอบสำคัญที่คุณได้เรียนรู้มาจนถึงตอนนี้เป็นส่วนประกอบพื้นฐานที่สำคัญของเฟรมเวิร์ก LangChain ซึ่งนำเราไปสู่คำถามที่สำคัญ: คุณจะรวมส่วนประกอบเหล่านี้เข้าด้วยกันอย่างมีประสิทธิภาพเพื่อสร้างแอปพลิเคชัน LLM ของคุณได้อย่างไร?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C0nyIqOEt2v"
      },
      "source": [
        "**อินเทอร์เฟซทั่วไปที่มีเมธอดเหล่านี้:**\n",
        "\n",
        "- **invoke** : แปลงอินพุตเดียวเป็นเอาต์พุต\n",
        "- **batch**: แปลงอินพุตหลายรายการเป็นเอาต์พุตหลายรายการอย่างมีประสิทธิภาพ\n",
        "- **stream** : สตรีมเอาต์พุตจากอินพุตเดียวขณะที่กำลังผลิต\n",
        "ยูทิลิตี้ในตัวสำหรับการลองอีกครั้ง การสำรอง การจัดรูปแบบข้อมูล และการกำหนดค่าเวลาทำงาน (รับอินพุตเดียวและส่งคืนตัววนซ้ำของส่วนต่างๆ ของเอาต์พุตเมื่อมีให้ใช้งาน)\n",
        "\n",
        "**ใน Python แต่ละเมธอดทั้ง 3 ข้างต้นมีคู่เทียบ asyncio**\n",
        "ดังนั้น ส่วนประกอบทั้งหมดจึงทำงานในลักษณะเดียวกัน และอินเทอร์เฟซที่เรียนรู้สำหรับส่วนประกอบใดส่วนประกอบหนึ่งจะใช้ได้กับทั้งหมด เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPALzmnLEsip",
        "outputId": "73fc0295-8989-4a3b-b379-1b098fd28c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\\n\\n1. Greetings\\n2. Salutations\\n3. Hey\\n4. Hello\\n5. Aloha'\n",
            "['\\n\\n1. Greetings\\n2. Hey\\n3. Hello\\n4. Salutations\\n5. Howdy',\n",
            " '\\n\\n1. Farewell\\n2. Goodbye\\n3. Adieu\\n4. See you later\\n5. So long']\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            ".\n",
            " Hello\n",
            "\n",
            "\n",
            "2\n",
            ".\n",
            " G\n",
            "reetings\n",
            "\n",
            "\n",
            "3\n",
            ".\n",
            " Hey\n",
            "\n",
            "\n",
            "4\n",
            ".\n",
            " Sal\n",
            "utations\n",
            "\n",
            "\n",
            "5\n",
            ".\n",
            " How\n",
            "dy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "model = OpenAI()\n",
        "completion = model.invoke('Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi')\n",
        "# Hi!\n",
        "pprint(completion)\n",
        "completions = model.batch(['Please answer maximize only 5 keywords on answer. Question:  Synonyme of Hi',\n",
        "                           'Please answer maximize only 5 keywords on answer. Question: Synonyme of Bye'])\n",
        "# ['Hi!', 'See you!']\n",
        "pprint(completions)\n",
        "for token in model.stream('Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi'):\n",
        "    print(token)\n",
        "    # Good\n",
        "    # bye\n",
        "    # !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWV_ycbiFbBr"
      },
      "source": [
        "ในบางกรณี หากส่วนประกอบพื้นฐานไม่รองรับเอาต์พุตแบบวนซ้ำ จะมีส่วนเดียวที่มีเอาต์พุตทั้งหมด\n",
        "\n",
        "คุณสามารถรวมส่วนประกอบเหล่านี้ได้สองวิธี:\n",
        "\n",
        "- **แบบ Imperative** : เรียกใช้โดยตรง ตัวอย่างเช่น ด้วย model.invoke(...)\n",
        "- **แบบ Declarative** : ด้วย LangChain Expression Language (LCEL) ซึ่งจะกล่าวถึงในหัวข้อถัดไป\n",
        "\n",
        "| Feature        | Imperative        | Declarative        |\n",
        "|----------------|--------------------|--------------------|\n",
        "| Syntax         | All of Python or JavaScript | LCEL              |\n",
        "| Parallel execution | Python: With threads or coroutines <br> JavaScript: With Promise.all | Automatic          |\n",
        "| Streaming      | With yield keyword | Automatic          |\n",
        "| Async execution | With async functions | Automatic          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WtWsS0kKxX7"
      },
      "source": [
        "## Imperative Composition\n",
        "การประมวลผลแบบ Imperative เป็นเพียงชื่อที่หรูหราสำหรับการเขียนโค้ดที่คุณคุ้นเคย การประกอบส่วนประกอบเหล่านี้เข้าด้วยกันในฟังก์ชันและคลาส ต่อไปนี้เป็นตัวอย่างการรวมพร้อมท์ โมเดล และตัวแยกวิเคราะห์ผลลัพธ์ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKfc_oz3Ajro"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# combine them in a function\n",
        "## @chain decorator adds the same Runnable interface for any function you write\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    return model.invoke(prompt)\n",
        "\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZz0m_HLdH-",
        "outputId": "c25c5579-a59d-4315-afc9-7c9f97a72041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Greetings, Hello, Hey, Hey there, Howdy'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf9h6HtNC7H"
      },
      "source": [
        "สิ่งที่นำเสนอไปก่อนหน้านี้เป็นตัวอย่างที่สมบูรณ์ของแชทบอท โดยใช้พร้อมท์และโมเดลแชท ดังที่คุณเห็น มันใช้ไวยากรณ์ Python ที่คุ้นเคยและรองรับตรรกะแบบกำหนดเองใดๆ ที่คุณอาจต้องการเพิ่มในฟังก์ชันนั้น\n",
        "\n",
        "\n",
        "**ในทางกลับกัน** หากคุณต้องการเปิดใช้งานการสตรีมหรือการรองรับแบบอะซิงโครนัส คุณจะต้องปรับเปลี่ยนฟังก์ชันของคุณเพื่อรองรับ ตัวอย่างเช่น การรองรับการสตรีมสามารถเพิ่มได้ดังนี้ เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg07hIBqLqPl",
        "outputId": "a7d6a5bf-7b81-458f-d8c1-a16a504908c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('content', 'Hello, Hey, Greetings, Howdy, Salutations.')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 14, 'prompt_tokens': 27, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-a9a1f549-a19f-4534-908b-6835265927cc-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 27, 'output_tokens': 14, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
          ]
        }
      ],
      "source": [
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.invoke(prompt):\n",
        "        yield token\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yPtepbOaBm"
      },
      "source": [
        "## Declarative Composition\n",
        "\n",
        "**การประมวลผลแบบ Declarative**\n",
        "\n",
        "LangChain Expression Language (LCEL) เป็น ภาษาประกาศ สำหรับการประกอบส่วนประกอบของ LangChain LangChain คอมไพล์การประมวลผล LCEL เป็น แผนการดำเนินการที่ปรับให้เหมาะสม พร้อมการประมวลผลแบบขนาน การสตรีม การติดตาม และการรองรับแบบอะซิงโครนัสโดยอัตโนมัติ\n",
        "\n",
        "มาดูตัวอย่างเดียวกันโดยใช้ LCEL เริ่มต้นใน Python:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCmiUM9gMT-p"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please answer maximize only 5 keywords on answer'),\n",
        "    ('human', '{question}'),\n",
        "])\n",
        "model = ChatOpenAI()\n",
        "# combine them with the | operator\n",
        "\n",
        "chatbot = template | model\n",
        "# use it\n",
        "result = chatbot.invoke({\n",
        "    \"question\": \"Question: Synonyme of Hi\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHq3AvMOl1z",
        "outputId": "9e109d22-88ed-4332-812d-003f1e39b8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, hey, greetings, hiya, howdy'\n"
          ]
        }
      ],
      "source": [
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxpCkw5YO_Uy"
      },
      "source": [
        "ที่สำคัญคือ บรรทัดสุดท้ายเหมือนกันในทั้งสองตัวอย่าง นั่นคือ คุณใช้ฟังก์ชันและลำดับ LCEL ในลักษณะเดียวกัน โดยใช้ `invoke/stream/batch` และในเวอร์ชันนี้ คุณไม่จำเป็นต้องทำอะไรเพิ่มเติมเพื่อใช้การสตรีม เริ่มต้นใน Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrnHdAwWOwhU",
        "outputId": "c6ead437-9a6b-4269-b8e6-5c453bd978d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content='Hello' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=' G' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content='reetings' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=' Hey' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=' How' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content='dy' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=',' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content=' Hi' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content='ya' additional_kwargs={} response_metadata={} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n",
            "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-1015c7b2-f207-42b4-a12c-e07185ce77a8'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "for part in chatbot.stream({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "}):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yccWx9AWPJCg",
        "outputId": "d9e713ae-1d10-41d2-d25e-3e5eb2c31fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello, Hey, Greetings, Salutations, Hiya'\n"
          ]
        }
      ],
      "source": [
        "chatbot = template | model\n",
        "result = await chatbot.ainvoke({\n",
        "    \"question\": \"Please answer maximize only 5 keywords on answer. Question: Synonyme of Hi\"\n",
        "})\n",
        "pprint(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbMjQSKnPfKB"
      },
      "source": [
        "# Summary\n",
        "\n",
        "**สรุป**\n",
        "\n",
        "ในบทนี้ คุณได้เรียนรู้เกี่ยวกับส่วนประกอบพื้นฐานและองค์ประกอบสำคัญที่จำเป็นสำหรับการสร้างแอปพลิเคชัน LLM โดยใช้ LangChain แอปพลิเคชัน LLM โดยพื้นฐานแล้วเป็นห่วงโซ่ที่ประกอบด้วยโมเดลภาษาขนาดใหญ่เพื่อทำการคาดการณ์ คำแนะนำพร้อมท์เพื่อนำทางโมเดลไปสู่เอาต์พุตที่ต้องการ และตัวแยกวิเคราะห์ผลลัพธ์ (optional) เพื่อแปลงรูปแบบของเอาต์พุตของโมเดล\n",
        "\n",
        "ส่วนประกอบทั้งหมดของ LangChain ใช้ร่วมกันอินเทอร์เฟซเดียวกันกับเมธอด invoke, stream และ batch เพื่อจัดการอินพุตและเอาต์พุตต่างๆ พวกเขาสามารถรวมและดำเนินการแบบ Imperative ได้โดยการเรียกใช้โดยตรงหรือแบบ Declarative โดยใช้ LangChain Expression Language (LCEL)\n",
        "\n",
        "วิธีการแบบ Imperative มีประโยชน์หากคุณตั้งใจที่จะเขียนตรรกะแบบกำหนดเองจำนวนมาก ในขณะที่วิธีการแบบ Declarative มีประโยชน์สำหรับการประกอบส่วนประกอบที่มีอยู่โดยมีการปรับแต่งจำกัด\n",
        "\n",
        "ในบทต่อไป คุณจะได้เรียนรู้วิธีการจัดเตรียมข้อมูลภายนอกให้กับแชทบอท AI ของคุณในฐานะ บริบท เพื่อให้คุณสามารถสร้างแอปพลิเคชัน LLM ที่ช่วยให้คุณ \"แชท\" กับข้อมูลของคุณได้"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
