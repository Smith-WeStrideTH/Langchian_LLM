{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b82b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings('ignore')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a94c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('./credential/cred.env')\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "api_key = os.environ.get(\"API_KEY\")\n",
    "\n",
    "# Check if the API key was found\n",
    "if api_key:\n",
    "  print(\"API Key loaded successfully.\")\n",
    "else:\n",
    "  print(\"API Key not found in the environment variables.\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690ac72-5d95-4cbf-875a-ae0e835593c9",
   "metadata": {},
   "source": [
    "# Lesson 1: Simple ReAct Agent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# based on https://til.simonwillison.net/llms/python-react-pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=\"gpt-4o\", \n",
    "                        temperature=0,\n",
    "                        messages=self.messages)\n",
    "        return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "average_dog_weight:\n",
    "e.g. average_dog_weight: Collie\n",
    "returns average weight of a dog when given the breed\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: How much does a Bulldog weigh?\n",
    "Thought: I should look the dogs weight using average_dog_weight\n",
    "Action: average_dog_weight: Bulldog\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: A Bulldog weights 51 lbs\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: A bulldog weights 51 lbs\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcb93-6298-4cfd-b3ce-61dfac7fb35f",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "def average_dog_weight(name):\n",
    "    if name in \"Scottish Terrier\": \n",
    "        return(\"Scottish Terriers average 20 lbs\")\n",
    "    elif name in \"Border Collie\":\n",
    "        return(\"a Border Collies average weight is 37 lbs\")\n",
    "    elif name in \"Toy Poodle\":\n",
    "        return(\"a toy poodles average weight is 7 lbs\")\n",
    "    else:\n",
    "        return(\"An average dog weights 50 lbs\")\n",
    "\n",
    "known_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"average_dog_weight\": average_dog_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932883a4-c722-42bb-aec0-b4f41c5c81a4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot = Agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff362f49-dcf1-4ea1-a86c-e516e9ab897d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "result = abot(\"How much does a toy poodle weigh?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e15a20-83d7-434c-8551-bce8dcc32be0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result = average_dog_weight(\"Toy Poodle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2c74-f32e-490c-a85d-932d11444210",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833d3ce-bd31-4319-811d-decff226b970",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e93cce-6eab-4c7c-ac64-e9993fdb30d6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0990-a932-423f-9ff3-5cada58c5f32",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde654-64e2-48bc-80a9-0ed668ccb7dc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot = Agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871f644-b131-4065-b7ce-b82c20a41f11",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. \\\n",
    "What is their combined weight\"\"\"\n",
    "abot(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d8070-3f36-4cf0-a677-508e54359c8f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(average_dog_weight(\"Border Collie\"))\n",
    "print(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3be1d-cc4c-41fa-9863-3e386e88e305",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a6cc-65d4-4ce7-87aa-4e67d7c23d7b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(average_dog_weight(\"Scottish Terrier\"))\n",
    "print(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b5e62-a203-433c-92a0-3783f490cde1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa923c-7e4f-42d1-965f-0f8ccd50fbd7",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(eval(\"37 + 20\"))\n",
    "print(next_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c6245-2837-4ac5-983b-95f61f3ac10d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot(next_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46f2ac-f717-4ab9-b548-f34b74071d76",
   "metadata": {},
   "source": [
    "### Add loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b910915-b087-4d35-afff-0ec30a5852f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "action_re = re.compile('^Action: (\\w+): (.*)$')   # python regular expression to selection action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feb6cc-5129-4a99-bb45-851bc07b5709",
   "metadata": {
    "height": 421
   },
   "outputs": [],
   "source": [
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [\n",
    "            action_re.match(a) \n",
    "            for a in result.split('\\n') \n",
    "            if action_re.match(a)\n",
    "        ]\n",
    "        if actions:\n",
    "            # There is an action to run\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n",
    "            print(\" -- running {} {}\".format(action, action_input))\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = \"Observation: {}\".format(observation)\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a02b4-96cc-4b01-8792-397a774eb499",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. \\\n",
    "What is their combined weight\"\"\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf503c9",
   "metadata": {},
   "source": [
    "# Lesson 2 : LangGraph Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b86a6-5e20-4252-b1d8-009b8318345a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af925917-b746-48c9-ac74-62fefbe5246c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37abeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67070e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  #reduce inference cost\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "image_bytes = abot.graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
    "\n",
    "display(Image(data=image_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48373b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3687306",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d458ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF and LA?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, the query was modified to produce more consistent results. \n",
    "# Results may vary per run and over time as search information and models change.\n",
    "\n",
    "query = \"Who won the super bowl in 2024? In what state is the winning team headquarters located? \\\n",
    "What is the GDP of that state? Answer each question.\" \n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")  # requires more advanced model\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7134b3",
   "metadata": {},
   "source": [
    "# Lesson 3: Agentic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4924af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# load environment variables from .env file\n",
    "_ = load_dotenv()\n",
    "\n",
    "# connect\n",
    "client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b72b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run search\n",
    "result = client.search(\"What is in Nvidia's new Blackwell GPU?\",\n",
    "                       include_answer=True)\n",
    "\n",
    "# print the answer\n",
    "result[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121353b",
   "metadata": {},
   "source": [
    "## Regular search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose location (try to change to your own city!)\n",
    "\n",
    "city = \"San Francisco\"\n",
    "\n",
    "query = f\"\"\"\n",
    "    what is the current weather in {city}?\n",
    "    Should I travel there today?\n",
    "    \"weather.com\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016043a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "import re\n",
    "\n",
    "ddg = DDGS()\n",
    "\n",
    "def search(query, max_results=6):\n",
    "    try:\n",
    "        results = ddg.text(query, max_results=max_results)\n",
    "        return [i[\"href\"] for i in results]\n",
    "    except Exception as e:\n",
    "        print(f\"returning previous results due to exception reaching ddg.\")\n",
    "        results = [ # cover case where DDG rate limits due to high deeplearning.ai volume\n",
    "            \"https://weather.com/weather/today/l/USCA0987:1:US\",\n",
    "            \"https://weather.com/weather/hourbyhour/l/54f9d8baac32496f6b5497b4bf7a277c3e2e6cc5625de69680e6169e7e38e9a8\",\n",
    "        ]\n",
    "        return results  \n",
    "\n",
    "\n",
    "for i in search(query):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather_info(url):\n",
    "    \"\"\"Scrape content from the given URL\"\"\"\n",
    "    if not url:\n",
    "        return \"Weather information could not be found.\"\n",
    "    \n",
    "    # fetch data\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return \"Failed to retrieve the webpage.\"\n",
    "\n",
    "    # parse result\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use DuckDuckGo to find websites and take the first result\n",
    "url = search(query)[0]\n",
    "\n",
    "# scrape first wesbsite\n",
    "soup = scrape_weather_info(url)\n",
    "\n",
    "print(f\"Website: {url}\\n\\n\")\n",
    "print(str(soup.body)[:50000]) # limit long outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text\n",
    "weather_data = []\n",
    "for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
    "    text = tag.get_text(\" \", strip=True)\n",
    "    weather_data.append(text)\n",
    "\n",
    "# combine all elements into a single string\n",
    "weather_data = \"\\n\".join(weather_data)\n",
    "\n",
    "# remove all spaces from the combined text\n",
    "weather_data = re.sub(r'\\s+', ' ', weather_data)\n",
    "    \n",
    "print(f\"Website: {url}\\n\\n\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c0c72",
   "metadata": {},
   "source": [
    "## Agentic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run search\n",
    "result = client.search(query, max_results=1)\n",
    "\n",
    "# print first result\n",
    "data = result[\"results\"][0][\"content\"]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pygments import highlight, lexers, formatters\n",
    "\n",
    "# parse JSON\n",
    "parsed_json = json.loads(data.replace(\"'\", '\"'))\n",
    "\n",
    "# pretty print JSON with syntax highlighting\n",
    "formatted_json = json.dumps(parsed_json, indent=4)\n",
    "colorful_json = highlight(formatted_json,\n",
    "                          lexers.JsonLexer(),\n",
    "                          formatters.TerminalFormatter())\n",
    "\n",
    "print(colorful_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0361db1",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a75d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef86c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a989e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7ec21",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc25a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3627a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aaf8b2",
   "metadata": {},
   "source": [
    "# Lesson 5: Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8948ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key\n",
    "with the default `operator.add` or `+` reducer, which always\n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9dcde1",
   "metadata": {},
   "source": [
    "## Manual human approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"]\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Whats the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8cefc",
   "metadata": {},
   "source": [
    "### continue after interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c82369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)\n",
    "while abot.graph.get_state(thread).next:\n",
    "    print(\"\\n\", abot.graph.get_state(thread),\"\\n\")\n",
    "    _input = input(\"proceed?\")\n",
    "    if _input != \"y\":\n",
    "        print(\"aborting\")\n",
    "        break\n",
    "    for event in abot.graph.stream(None, thread):\n",
    "        for v in event.values():\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f1ad8",
   "metadata": {},
   "source": [
    "## Modify State\n",
    "Run until the interrupt and then modify the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e38fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values = abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values['messages'][-1].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
    "current_values.values['messages'][-1].tool_calls = [\n",
    "    {'name': 'tavily_search_results_json',\n",
    "  'args': {'query': 'current weather in Louisiana'},\n",
    "  'id': _id}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.update_state(thread, current_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f993ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72585e5a",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for state in abot.graph.get_state_history(thread):\n",
    "    print(state)\n",
    "    print('--')\n",
    "    states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ed77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay = states[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, to_replay.config):\n",
    "    for k, v in event.items():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2189f88",
   "metadata": {},
   "source": [
    "## Go back in time and edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
    "to_replay.values['messages'][-1].tool_calls = [{'name': 'tavily_search_results_json',\n",
    "  'args': {'query': 'current weather in LA, accuweather'},\n",
    "  'id': _id}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_state = abot.graph.update_state(to_replay.config, to_replay.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, branch_state):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2f55c",
   "metadata": {},
   "source": [
    "## Add message to a state at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = to_replay.values['messages'][-1].tool_calls[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_update = {\"messages\": [ToolMessage(\n",
    "    tool_call_id=_id,\n",
    "    name=\"tavily_search_results_json\",\n",
    "    content=\"54 degree celcius\",\n",
    ")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_and_add = abot.graph.update_state(\n",
    "    to_replay.config, \n",
    "    state_update, \n",
    "    as_node=\"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, branch_and_add):\n",
    "    for k, v in event.items():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88e640",
   "metadata": {},
   "source": [
    "# Extra Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db763ab",
   "metadata": {},
   "source": [
    "## Build a small graph\n",
    "This is a small simple graph you can tinker with if you want more insight into controlling state memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccfe67",
   "metadata": {},
   "source": [
    "Define a simple 2 node graph with the following state:\n",
    "-`lnode`: last node\n",
    "-`scratch`: a scratchpad location\n",
    "-`count` : a counter that is incremented each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    lnode: str\n",
    "    scratch: str\n",
    "    count: Annotated[int, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node1(state: AgentState):\n",
    "    print(f\"node1, count:{state['count']}\")\n",
    "    return {\"lnode\": \"node_1\",\n",
    "            \"count\": 1,\n",
    "           }\n",
    "def node2(state: AgentState):\n",
    "    print(f\"node2, count:{state['count']}\")\n",
    "    return {\"lnode\": \"node_2\",\n",
    "            \"count\": 1,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb023",
   "metadata": {},
   "source": [
    "The graph goes N1->N2->N1... but breaks after count reaches 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    return state[\"count\"] < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "\n",
    "builder.add_edge(\"Node1\", \"Node2\")\n",
    "builder.add_conditional_edges(\"Node2\", \n",
    "                              should_continue, \n",
    "                              {True: \"Node1\", False: END})\n",
    "builder.set_entry_point(\"Node1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e395d",
   "metadata": {},
   "source": [
    "### Run it!\n",
    "Now, set the thread and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13423b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "graph.invoke({\"count\":0, \"scratch\":\"hi\"},thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f64a35",
   "metadata": {},
   "source": [
    "### Look at current state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383212d",
   "metadata": {},
   "source": [
    "Get the current state. Note the `values` which are the AgentState. Note the `config` and the `thread_ts`. You will be using those to refer to snapshots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73efab",
   "metadata": {},
   "source": [
    "View all the statesnapshots in memory. You can use the displayed `count` agentstate variable to help track what you see. Notice the most recent snapshots are returned by the iterator first. Also note that there is a handy `step` variable in the metadata that counts the number of steps in the graph execution. This is a bit detailed - but you can also notice that the *parent_config* is the *config* of the previous node. At initial startup, additional states are inserted into memory to create a parent. This is something to check when you branch or *time travel* below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9301d",
   "metadata": {},
   "source": [
    "### Look at state history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in graph.get_state_history(thread):\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a972fb8",
   "metadata": {},
   "source": [
    "Store just the `config` into an list. Note the sequence of counts on the right. `get_state_history` returns the most recent snapshots first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ec7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for state in graph.get_state_history(thread):\n",
    "    states.append(state.config)\n",
    "    print(state.config, state.values['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749da71",
   "metadata": {},
   "source": [
    "Grab an early state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6618bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "states[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102db5d",
   "metadata": {},
   "source": [
    "This is the state after Node1 completed for the first time. Note `next` is `Node2`and `count` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(states[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb64b8d",
   "metadata": {},
   "source": [
    "### Go Back in Time\n",
    "Use that state in `invoke` to go back in time. Notice it uses states[-3] as *current_state* and continues to node2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(None, states[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381a4b9",
   "metadata": {},
   "source": [
    "Notice the new states are now in state history. Notice the counts on the far right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17eae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "for state in graph.get_state_history(thread):\n",
    "    print(state.config, state.values['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f9148",
   "metadata": {},
   "source": [
    "You can see the details below. Lots of text, but try to find the node that start the new branch. Notice the parent *config* is not the previous entry in the stack, but is the entry from state[-3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "for state in graph.get_state_history(thread):\n",
    "    print(state,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22255e",
   "metadata": {},
   "source": [
    "### Modify State\n",
    "Let's start by starting a fresh thread and running to clean out history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb09f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread2 = {\"configurable\": {\"thread_id\": str(2)}}\n",
    "graph.invoke({\"count\":0, \"scratch\":\"hi\"},thread2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states2 = []\n",
    "for state in graph.get_state_history(thread2):\n",
    "    states2.append(state.config)\n",
    "    print(state.config, state.values['count'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5f168",
   "metadata": {},
   "source": [
    "Start by grabbing a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state = graph.get_state(states2[-3])\n",
    "save_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343084f",
   "metadata": {},
   "source": [
    "Now modify the values. One subtle item to note: Recall when agent state was defined, `count` used `operator.add` to indicate that values are *added* to the current value. Here, `-3` will be added to the current count value rather than replace it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a709dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state.values[\"count\"] = -3\n",
    "save_state.values[\"scratch\"] = \"hello\"\n",
    "save_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705500da",
   "metadata": {},
   "source": [
    "Now update the state. This creates a new entry at the *top*, or *latest* entry in memory. This will become the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state(thread2,save_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9b563",
   "metadata": {},
   "source": [
    "Current state is at the top. You can match the `thread_ts`.\n",
    "Notice the `parent_config`, `thread_ts` of the new node - it is the previous node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ba00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "    if i >= 3:  #print latest 3\n",
    "        break\n",
    "    print(state, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51601fc7",
   "metadata": {},
   "source": [
    "### Try again with `as_node`\n",
    "When writing using `update_state()`, you want to define to the graph logic which node should be assumed as the writer. What this does is allow th graph logic to find the node on the graph. After writing the values, the `next()` value is computed by travesing the graph using the new state. In this case, the state we have was written by `Node1`. The graph can then compute the next state as being `Node2`. Note that in some graphs, this may involve going through conditional edges!  Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf64fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state(thread2,save_state.values, as_node=\"Node1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "    if i >= 3:  #print latest 3\n",
    "        break\n",
    "    print(state, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ae234",
   "metadata": {},
   "source": [
    "`invoke` will run from the current state if not given a particular `thread_ts`. This is now the entry that was just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(None,thread2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f73d3e",
   "metadata": {},
   "source": [
    "Print out the state history, notice the `scratch` value change on the latest entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in graph.get_state_history(thread2):\n",
    "    print(state,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233b380",
   "metadata": {},
   "source": [
    "# Essay Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1e839",
   "metadata": {},
   "source": [
    "## Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper.py\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*TqdmWarning.*\")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "    \n",
    "class ewriter():\n",
    "    def __init__(self):\n",
    "        self.model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.PLAN_PROMPT = (\"You are an expert writer tasked with writing a high level outline of a short 3 paragraph essay. \"\n",
    "                            \"Write such an outline for the user provided topic. Give the three main headers of an outline of \"\n",
    "                             \"the essay along with any relevant notes or instructions for the sections. \")\n",
    "        self.WRITER_PROMPT = (\"You are an essay assistant tasked with writing excellent 3 paragraph essays. \"\n",
    "                              \"Generate the best essay possible for the user's request and the initial outline. \"\n",
    "                              \"If the user provides critique, respond with a revised version of your previous attempts. \"\n",
    "                              \"Utilize all the information below as needed: \\n\"\n",
    "                              \"------\\n\"\n",
    "                              \"{content}\")\n",
    "        self.RESEARCH_PLAN_PROMPT = (\"You are a researcher charged with providing information that can \"\n",
    "                                     \"be used when writing the following essay. Generate a list of search \"\n",
    "                                     \"queries that will gather \"\n",
    "                                     \"any relevant information. Only generate 3 queries max.\")\n",
    "        self.REFLECTION_PROMPT = (\"You are a teacher grading an 3 paragraph essay submission. \"\n",
    "                                  \"Generate critique and recommendations for the user's submission. \"\n",
    "                                  \"Provide detailed recommendations, including requests for length, depth, style, etc.\")\n",
    "        self.RESEARCH_CRITIQUE_PROMPT = (\"You are a researcher charged with providing information that can \"\n",
    "                                         \"be used when making any requested revisions (as outlined below). \"\n",
    "                                         \"Generate a list of search queries that will gather any relevant information. \"\n",
    "                                         \"Only generate 2 queries max.\")\n",
    "        self.tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        builder = StateGraph(AgentState)\n",
    "        builder.add_node(\"planner\", self.plan_node)\n",
    "        builder.add_node(\"research_plan\", self.research_plan_node)\n",
    "        builder.add_node(\"generate\", self.generation_node)\n",
    "        builder.add_node(\"reflect\", self.reflection_node)\n",
    "        builder.add_node(\"research_critique\", self.research_critique_node)\n",
    "        builder.set_entry_point(\"planner\")\n",
    "        builder.add_conditional_edges(\n",
    "            \"generate\", \n",
    "            self.should_continue, \n",
    "            {END: END, \"reflect\": \"reflect\"}\n",
    "        )\n",
    "        builder.add_edge(\"planner\", \"research_plan\")\n",
    "        builder.add_edge(\"research_plan\", \"generate\")\n",
    "        builder.add_edge(\"reflect\", \"research_critique\")\n",
    "        builder.add_edge(\"research_critique\", \"generate\")\n",
    "        memory = SqliteSaver(conn=sqlite3.connect(\":memory:\", check_same_thread=False))\n",
    "        self.graph = builder.compile(\n",
    "            checkpointer=memory,\n",
    "            interrupt_after=['planner', 'generate', 'reflect', 'research_plan', 'research_critique']\n",
    "        )\n",
    "\n",
    "\n",
    "    def plan_node(self, state: AgentState):\n",
    "        messages = [\n",
    "            SystemMessage(content=self.PLAN_PROMPT), \n",
    "            HumanMessage(content=state['task'])\n",
    "        ]\n",
    "        response = self.model.invoke(messages)\n",
    "        return {\"plan\": response.content,\n",
    "               \"lnode\": \"planner\",\n",
    "                \"count\": 1,\n",
    "               }\n",
    "    def research_plan_node(self, state: AgentState):\n",
    "        queries = self.model.with_structured_output(Queries).invoke([\n",
    "            SystemMessage(content=self.RESEARCH_PLAN_PROMPT),\n",
    "            HumanMessage(content=state['task'])\n",
    "        ])\n",
    "        content = state['content'] or []  # add to content\n",
    "        for q in queries.queries:\n",
    "            response = self.tavily.search(query=q, max_results=2)\n",
    "            for r in response['results']:\n",
    "                content.append(r['content'])\n",
    "        return {\"content\": content,\n",
    "                \"queries\": queries.queries,\n",
    "               \"lnode\": \"research_plan\",\n",
    "                \"count\": 1,\n",
    "               }\n",
    "    def generation_node(self, state: AgentState):\n",
    "        content = \"\\n\\n\".join(state['content'] or [])\n",
    "        user_message = HumanMessage(\n",
    "            content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=self.WRITER_PROMPT.format(content=content)\n",
    "            ),\n",
    "            user_message\n",
    "            ]\n",
    "        response = self.model.invoke(messages)\n",
    "        return {\n",
    "            \"draft\": response.content, \n",
    "            \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "            \"lnode\": \"generate\",\n",
    "            \"count\": 1,\n",
    "        }\n",
    "    def reflection_node(self, state: AgentState):\n",
    "        messages = [\n",
    "            SystemMessage(content=self.REFLECTION_PROMPT), \n",
    "            HumanMessage(content=state['draft'])\n",
    "        ]\n",
    "        response = self.model.invoke(messages)\n",
    "        return {\"critique\": response.content,\n",
    "               \"lnode\": \"reflect\",\n",
    "                \"count\": 1,\n",
    "        }\n",
    "    def research_critique_node(self, state: AgentState):\n",
    "        queries = self.model.with_structured_output(Queries).invoke([\n",
    "            SystemMessage(content=self.RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state['critique'])\n",
    "        ])\n",
    "        content = state['content'] or []\n",
    "        for q in queries.queries:\n",
    "            response = self.tavily.search(query=q, max_results=2)\n",
    "            for r in response['results']:\n",
    "                content.append(r['content'])\n",
    "        return {\"content\": content,\n",
    "               \"lnode\": \"research_critique\",\n",
    "                \"count\": 1,\n",
    "        }\n",
    "    def should_continue(self, state):\n",
    "        if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "            return END\n",
    "        return \"reflect\"\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "class writer_gui( ):\n",
    "    def __init__(self, graph, share=False):\n",
    "        self.graph = graph\n",
    "        self.share = share\n",
    "        self.partial_message = \"\"\n",
    "        self.response = {}\n",
    "        self.max_iterations = 10\n",
    "        self.iterations = []\n",
    "        self.threads = []\n",
    "        self.thread_id = -1\n",
    "        self.thread = {\"configurable\": {\"thread_id\": str(self.thread_id)}}\n",
    "        #self.sdisps = {} #global    \n",
    "        self.demo = self.create_interface()\n",
    "\n",
    "    def run_agent(self, start,topic,stop_after):\n",
    "        #global partial_message, thread_id,thread\n",
    "        #global response, max_iterations, iterations, threads\n",
    "        if start:\n",
    "            self.iterations.append(0)\n",
    "            config = {'task': topic,\"max_revisions\": 2,\"revision_number\": 0,\n",
    "                      'lnode': \"\", 'planner': \"no plan\", 'draft': \"no draft\", 'critique': \"no critique\", \n",
    "                      'content': [\"no content\",], 'queries': \"no queries\", 'count':0}\n",
    "            self.thread_id += 1  # new agent, new thread\n",
    "            self.threads.append(self.thread_id)\n",
    "        else:\n",
    "            config = None\n",
    "        self.thread = {\"configurable\": {\"thread_id\": str(self.thread_id)}}\n",
    "        while self.iterations[self.thread_id] < self.max_iterations:\n",
    "            self.response = self.graph.invoke(config, self.thread)\n",
    "            self.iterations[self.thread_id] += 1\n",
    "            self.partial_message += str(self.response)\n",
    "            self.partial_message += f\"\\n------------------\\n\\n\"\n",
    "            ## fix\n",
    "            lnode,nnode,_,rev,acount = self.get_disp_state()\n",
    "            yield self.partial_message,lnode,nnode,self.thread_id,rev,acount\n",
    "            config = None #need\n",
    "            #print(f\"run_agent:{lnode}\")\n",
    "            if not nnode:  \n",
    "                #print(\"Hit the end\")\n",
    "                return\n",
    "            if lnode in stop_after:\n",
    "                #print(f\"stopping due to stop_after {lnode}\")\n",
    "                return\n",
    "            else:\n",
    "                #print(f\"Not stopping on lnode {lnode}\")\n",
    "                pass\n",
    "        return\n",
    "    \n",
    "    def get_disp_state(self,):\n",
    "        current_state = self.graph.get_state(self.thread)\n",
    "        lnode = current_state.values[\"lnode\"]\n",
    "        acount = current_state.values[\"count\"]\n",
    "        rev = current_state.values[\"revision_number\"]\n",
    "        nnode = current_state.next\n",
    "        #print  (lnode,nnode,self.thread_id,rev,acount)\n",
    "        return lnode,nnode,self.thread_id,rev,acount\n",
    "    \n",
    "    def get_state(self,key):\n",
    "        current_values = self.graph.get_state(self.thread)\n",
    "        if key in current_values.values:\n",
    "            lnode,nnode,self.thread_id,rev,astep = self.get_disp_state()\n",
    "            new_label = f\"last_node: {lnode}, thread_id: {self.thread_id}, rev: {rev}, step: {astep}\"\n",
    "            return gr.update(label=new_label, value=current_values.values[key])\n",
    "        else:\n",
    "            return \"\"  \n",
    "    \n",
    "    def get_content(self,):\n",
    "        current_values = self.graph.get_state(self.thread)\n",
    "        if \"content\" in current_values.values:\n",
    "            content = current_values.values[\"content\"]\n",
    "            lnode,nnode,thread_id,rev,astep = self.get_disp_state()\n",
    "            new_label = f\"last_node: {lnode}, thread_id: {self.thread_id}, rev: {rev}, step: {astep}\"\n",
    "            return gr.update(label=new_label, value=\"\\n\\n\".join(item for item in content) + \"\\n\\n\")\n",
    "        else:\n",
    "            return \"\"  \n",
    "    \n",
    "    def update_hist_pd(self,):\n",
    "        #print(\"update_hist_pd\")\n",
    "        hist = []\n",
    "        # curiously, this generator returns the latest first\n",
    "        for state in self.graph.get_state_history(self.thread):\n",
    "            if state.metadata['step'] < 1:\n",
    "                continue\n",
    "            thread_ts = state.config['configurable']['thread_ts']\n",
    "            tid = state.config['configurable']['thread_id']\n",
    "            count = state.values['count']\n",
    "            lnode = state.values['lnode']\n",
    "            rev = state.values['revision_number']\n",
    "            nnode = state.next\n",
    "            st = f\"{tid}:{count}:{lnode}:{nnode}:{rev}:{thread_ts}\"\n",
    "            hist.append(st)\n",
    "        return gr.Dropdown(label=\"update_state from: thread:count:last_node:next_node:rev:thread_ts\", \n",
    "                           choices=hist, value=hist[0],interactive=True)\n",
    "    \n",
    "    def find_config(self,thread_ts):\n",
    "        for state in self.graph.get_state_history(self.thread):\n",
    "            config = state.config\n",
    "            if config['configurable']['thread_ts'] == thread_ts:\n",
    "                return config\n",
    "        return(None)\n",
    "            \n",
    "    def copy_state(self,hist_str):\n",
    "        ''' result of selecting an old state from the step pulldown. Note does not change thread. \n",
    "             This copies an old state to a new current state. \n",
    "        '''\n",
    "        thread_ts = hist_str.split(\":\")[-1]\n",
    "        #print(f\"copy_state from {thread_ts}\")\n",
    "        config = self.find_config(thread_ts)\n",
    "        #print(config)\n",
    "        state = self.graph.get_state(config)\n",
    "        self.graph.update_state(self.thread, state.values, as_node=state.values['lnode'])\n",
    "        new_state = self.graph.get_state(self.thread)  #should now match\n",
    "        new_thread_ts = new_state.config['configurable']['thread_ts']\n",
    "        tid = new_state.config['configurable']['thread_id']\n",
    "        count = new_state.values['count']\n",
    "        lnode = new_state.values['lnode']\n",
    "        rev = new_state.values['revision_number']\n",
    "        nnode = new_state.next\n",
    "        return lnode,nnode,new_thread_ts,rev,count\n",
    "    \n",
    "    def update_thread_pd(self,):\n",
    "        #print(\"update_thread_pd\")\n",
    "        return gr.Dropdown(label=\"choose thread\", choices=threads, value=self.thread_id,interactive=True)\n",
    "    \n",
    "    def switch_thread(self,new_thread_id):\n",
    "        #print(f\"switch_thread{new_thread_id}\")\n",
    "        self.thread = {\"configurable\": {\"thread_id\": str(new_thread_id)}}\n",
    "        self.thread_id = new_thread_id\n",
    "        return \n",
    "    \n",
    "    def modify_state(self,key,asnode,new_state):\n",
    "        ''' gets the current state, modifes a single value in the state identified by key, and updates state with it.\n",
    "        note that this will create a new 'current state' node. If you do this multiple times with different keys, it will create\n",
    "        one for each update. Note also that it doesn't resume after the update\n",
    "        '''\n",
    "        current_values = self.graph.get_state(self.thread)\n",
    "        current_values.values[key] = new_state\n",
    "        self.graph.update_state(self.thread, current_values.values,as_node=asnode)\n",
    "        return\n",
    "\n",
    "\n",
    "    def create_interface(self):\n",
    "        with gr.Blocks(theme=gr.themes.Default(spacing_size='sm',text_size=\"sm\")) as demo:\n",
    "            \n",
    "            def updt_disp():\n",
    "                ''' general update display on state change '''\n",
    "                current_state = self.graph.get_state(self.thread)\n",
    "                hist = []\n",
    "                # curiously, this generator returns the latest first\n",
    "                for state in self.graph.get_state_history(self.thread):\n",
    "                    if state.metadata['step'] < 1:  #ignore early states\n",
    "                        continue\n",
    "                    s_thread_ts = state.config['configurable']['thread_ts']\n",
    "                    s_tid = state.config['configurable']['thread_id']\n",
    "                    s_count = state.values['count']\n",
    "                    s_lnode = state.values['lnode']\n",
    "                    s_rev = state.values['revision_number']\n",
    "                    s_nnode = state.next\n",
    "                    st = f\"{s_tid}:{s_count}:{s_lnode}:{s_nnode}:{s_rev}:{s_thread_ts}\"\n",
    "                    hist.append(st)\n",
    "                if not current_state.metadata: #handle init call\n",
    "                    return{}\n",
    "                else:\n",
    "                    return {\n",
    "                        topic_bx : current_state.values[\"task\"],\n",
    "                        lnode_bx : current_state.values[\"lnode\"],\n",
    "                        count_bx : current_state.values[\"count\"],\n",
    "                        revision_bx : current_state.values[\"revision_number\"],\n",
    "                        nnode_bx : current_state.next,\n",
    "                        threadid_bx : self.thread_id,\n",
    "                        thread_pd : gr.Dropdown(label=\"choose thread\", choices=self.threads, value=self.thread_id,interactive=True),\n",
    "                        step_pd : gr.Dropdown(label=\"update_state from: thread:count:last_node:next_node:rev:thread_ts\", \n",
    "                               choices=hist, value=hist[0],interactive=True),\n",
    "                    }\n",
    "            def get_snapshots():\n",
    "                new_label = f\"thread_id: {self.thread_id}, Summary of snapshots\"\n",
    "                sstate = \"\"\n",
    "                for state in self.graph.get_state_history(self.thread):\n",
    "                    for key in ['plan', 'draft', 'critique']:\n",
    "                        if key in state.values:\n",
    "                            state.values[key] = state.values[key][:80] + \"...\"\n",
    "                    if 'content' in state.values:\n",
    "                        for i in range(len(state.values['content'])):\n",
    "                            state.values['content'][i] = state.values['content'][i][:20] + '...'\n",
    "                    if 'writes' in state.metadata:\n",
    "                        state.metadata['writes'] = \"not shown\"\n",
    "                    sstate += str(state) + \"\\n\\n\"\n",
    "                return gr.update(label=new_label, value=sstate)\n",
    "\n",
    "            def vary_btn(stat):\n",
    "                #print(f\"vary_btn{stat}\")\n",
    "                return(gr.update(variant=stat))\n",
    "            \n",
    "            with gr.Tab(\"Agent\"):\n",
    "                with gr.Row():\n",
    "                    topic_bx = gr.Textbox(label=\"Essay Topic\", value=\"Pizza Shop\")\n",
    "                    gen_btn = gr.Button(\"Generate Essay\", scale=0,min_width=80, variant='primary')\n",
    "                    cont_btn = gr.Button(\"Continue Essay\", scale=0,min_width=80)\n",
    "                with gr.Row():\n",
    "                    lnode_bx = gr.Textbox(label=\"last node\", min_width=100)\n",
    "                    nnode_bx = gr.Textbox(label=\"next node\", min_width=100)\n",
    "                    threadid_bx = gr.Textbox(label=\"Thread\", scale=0, min_width=80)\n",
    "                    revision_bx = gr.Textbox(label=\"Draft Rev\", scale=0, min_width=80)\n",
    "                    count_bx = gr.Textbox(label=\"count\", scale=0, min_width=80)\n",
    "                with gr.Accordion(\"Manage Agent\", open=False):\n",
    "                    checks = list(self.graph.nodes.keys())\n",
    "                    checks.remove('__start__')\n",
    "                    stop_after = gr.CheckboxGroup(checks,label=\"Interrupt After State\", value=checks, scale=0, min_width=400)\n",
    "                    with gr.Row():\n",
    "                        thread_pd = gr.Dropdown(choices=self.threads,interactive=True, label=\"select thread\", min_width=120, scale=0)\n",
    "                        step_pd = gr.Dropdown(choices=['N/A'],interactive=True, label=\"select step\", min_width=160, scale=1)\n",
    "                live = gr.Textbox(label=\"Live Agent Output\", lines=5, max_lines=5)\n",
    "        \n",
    "                # actions\n",
    "                sdisps =[topic_bx,lnode_bx,nnode_bx,threadid_bx,revision_bx,count_bx,step_pd,thread_pd]\n",
    "                thread_pd.input(self.switch_thread, [thread_pd], None).then(\n",
    "                                fn=updt_disp, inputs=None, outputs=sdisps)\n",
    "                step_pd.input(self.copy_state,[step_pd],None).then(\n",
    "                              fn=updt_disp, inputs=None, outputs=sdisps)\n",
    "                gen_btn.click(vary_btn,gr.Number(\"secondary\", visible=False), gen_btn).then(\n",
    "                              fn=self.run_agent, inputs=[gr.Number(True, visible=False),topic_bx,stop_after], outputs=[live],show_progress=True).then(\n",
    "                              fn=updt_disp, inputs=None, outputs=sdisps).then( \n",
    "                              vary_btn,gr.Number(\"primary\", visible=False), gen_btn).then(\n",
    "                              vary_btn,gr.Number(\"primary\", visible=False), cont_btn)\n",
    "                cont_btn.click(vary_btn,gr.Number(\"secondary\", visible=False), cont_btn).then(\n",
    "                               fn=self.run_agent, inputs=[gr.Number(False, visible=False),topic_bx,stop_after], \n",
    "                               outputs=[live]).then(\n",
    "                               fn=updt_disp, inputs=None, outputs=sdisps).then(\n",
    "                               vary_btn,gr.Number(\"primary\", visible=False), cont_btn)\n",
    "        \n",
    "            with gr.Tab(\"Plan\"):\n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"Refresh\")\n",
    "                    modify_btn = gr.Button(\"Modify\")\n",
    "                plan = gr.Textbox(label=\"Plan\", lines=10, interactive=True)\n",
    "                refresh_btn.click(fn=self.get_state, inputs=gr.Number(\"plan\", visible=False), outputs=plan)\n",
    "                modify_btn.click(fn=self.modify_state, inputs=[gr.Number(\"plan\", visible=False),\n",
    "                                                          gr.Number(\"planner\", visible=False), plan],outputs=None).then(\n",
    "                                 fn=updt_disp, inputs=None, outputs=sdisps)\n",
    "            with gr.Tab(\"Research Content\"):\n",
    "                refresh_btn = gr.Button(\"Refresh\")\n",
    "                content_bx = gr.Textbox(label=\"content\", lines=10)\n",
    "                refresh_btn.click(fn=self.get_content, inputs=None, outputs=content_bx)\n",
    "            with gr.Tab(\"Draft\"):\n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"Refresh\")\n",
    "                    modify_btn = gr.Button(\"Modify\")\n",
    "                draft_bx = gr.Textbox(label=\"draft\", lines=10, interactive=True)\n",
    "                refresh_btn.click(fn=self.get_state, inputs=gr.Number(\"draft\", visible=False), outputs=draft_bx)\n",
    "                modify_btn.click(fn=self.modify_state, inputs=[gr.Number(\"draft\", visible=False),\n",
    "                                                          gr.Number(\"generate\", visible=False), draft_bx], outputs=None).then(\n",
    "                                fn=updt_disp, inputs=None, outputs=sdisps)\n",
    "            with gr.Tab(\"Critique\"):\n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"Refresh\")\n",
    "                    modify_btn = gr.Button(\"Modify\")\n",
    "                critique_bx = gr.Textbox(label=\"Critique\", lines=10, interactive=True)\n",
    "                refresh_btn.click(fn=self.get_state, inputs=gr.Number(\"critique\", visible=False), outputs=critique_bx)\n",
    "                modify_btn.click(fn=self.modify_state, inputs=[gr.Number(\"critique\", visible=False),\n",
    "                                                          gr.Number(\"reflect\", visible=False), \n",
    "                                                          critique_bx], outputs=None).then(\n",
    "                                fn=updt_disp, inputs=None, outputs=sdisps)\n",
    "            with gr.Tab(\"StateSnapShots\"):\n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"Refresh\")\n",
    "                snapshots = gr.Textbox(label=\"State Snapshots Summaries\")\n",
    "                refresh_btn.click(fn=get_snapshots, inputs=None, outputs=snapshots)\n",
    "        return demo\n",
    "\n",
    "    def launch(self, share=None):\n",
    "        if port := os.getenv(\"PORT1\"):\n",
    "            self.demo.launch(share=True, server_port=int(port), server_name=\"0.0.0.0\")\n",
    "        else:\n",
    "            self.demo.launch(share=self.share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4ac2c",
   "metadata": {},
   "source": [
    "## Start Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec68d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7988b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f13bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741504b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a729a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69effc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e14f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5781d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e84c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40c9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddba9f53",
   "metadata": {},
   "source": [
    "## Essay Writer Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from helper import ewriter, writer_gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800956df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9f63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
