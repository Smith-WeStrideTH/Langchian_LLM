{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb4kWFwWYHZh"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8fm6rFbgqWP"
      },
      "outputs": [],
      "source": [
        "# !pip install pyppeteer # use to show graph png\n",
        "# !pip install graphviz # use to show graph png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNaO6lWKYh_4",
        "outputId": "da6e3d9b-d793-4fea-bafd-030a5e7cb6e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('./credential/cred.env')\n",
        "\n",
        "# Get the API key from the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "\n",
        "# Check if the API key was found\n",
        "if api_key:\n",
        "  print(\"API Key loaded successfully.\")\n",
        "else:\n",
        "  print(\"API Key not found in the environment variables.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEwCrsj7W-7l"
      },
      "source": [
        "# Chapter 4. Using LangGraph to Add Memory to Your Chatbot\n",
        "\n",
        "ในบทที่ 3 คุณได้เรียนรู้วิธีการจัดหาบริบทที่ทันสมัยและเกี่ยวข้องกับแอปพลิเคชันแชทบอท AI ของคุณ ซึ่งช่วยให้แชทบอทของคุณสามารถสร้างการตอบกลับที่ถูกต้องตามอินพุตของผู้ใช้ได้ แต่เพียงเท่านี้ยังไม่เพียงพอสำหรับการสร้างแอปพลิเคชันที่พร้อมใช้งานจริง คุณจะเปิดใช้งานแอปพลิเคชันของคุณให้ \"แชท\" ไปกลับกับผู้ใช้ได้อย่างไร ในขณะที่จำบทสนทนาและบริบทที่เกี่ยวข้องก่อนหน้านี้\n",
        "\n",
        "**โมเดลภาษาขนาดใหญ่ไม่มีสถานะ (stateless)**\n",
        "ซึ่งหมายความว่าทุกครั้งที่มีการกระตุ้นให้โมเดลสร้างการตอบกลับใหม่ โมเดลจะไม่มีหน่วยความจำของพรอมต์หรือการตอบกลับของโมเดลก่อนหน้า เพื่อที่จะจัดหาข้อมูลประวัติศาสตร์นี้ให้กับโมเดล เราจำเป็นต้องมีระบบหน่วยความจำที่แข็งแกร่งซึ่งจะติดตามการสนทนาและบริบทก่อนหน้า ข้อมูลประวัติศาสตร์นี้สามารถรวมอยู่ในพรอมต์สุดท้ายที่ส่งไปยัง LLM ได้ ทำให้โมเดลมี \"หน่วยความจำ\" รูปดังกล่าว แสดงให้เห็น\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure4-1.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**ในบทนี้ คุณจะได้เรียนรู้วิธีการสร้างระบบหน่วยความจำที่สำคัญนี้โดยใช้โมดูลที่มีอยู่ภายในของ LangChain เพื่อทำให้กระบวนการพัฒนาง่ายขึ้น**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlN08XvGXqZv"
      },
      "source": [
        "# Building a Chatbot Memory System\n",
        "\n",
        "**การสร้างระบบหน่วยความจำสำหรับแชทบอท**\n",
        "\n",
        "มีการตัดสินใจออกแบบหลักสองประการอยู่เบื้องหลังระบบหน่วยความจำที่แข็งแกร่งทุกระบบ:\n",
        "\n",
        "1. วิธีการจัดเก็บสถานะ\n",
        "2. วิธีการค้นหาสถานะ\n",
        "\n",
        "วิธีง่ายๆ ในการสร้างระบบหน่วยความจำสำหรับแชทบอทที่รวมโซลูชันที่มีประสิทธิภาพสำหรับการตัดสินใจออกแบบเหล่านี้คือ การจัดเก็บและนำประวัติการโต้ตอบทั้งหมดระหว่างผู้ใช้และโมเดลมาใช้ใหม่ สถานะของระบบหน่วยความจำนี้สามารถ:\n",
        "\n",
        "จัดเก็บเป็นรายการข้อความ (ดูข้อมูลเพิ่มเติมเกี่ยวกับข้อความใน บทที่ 1)\n",
        "อัปเดตโดยการเพิ่มข้อความล่าสุดหลังจากแต่ละเทิร์น\n",
        "เพิ่มลงในพรอมต์โดยการแทรกข้อความลงในพรอมต์\n",
        "ดังรูป แสดงให้เห็นระบบหน่วยความจำอย่างง่ายนี้\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure4-2.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8xXr-MjX_re"
      },
      "source": [
        "นี่คือตัวอย่างโค้ดที่แสดงให้เห็นถึงระบบหน่วยความจำในรูปแบบง่ายๆ โดยใช้ LangChain:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9iAVUofWo_Y",
        "outputId": "cb3414a1-8833-4cb4-eac4-6a7d7427c61a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I said \"J\\'adore la programmation\" which means \"I love programming\" in French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 61, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3c1a98f0-a073-46d7-987b-d67e1ab7b747-0', usage_metadata={'input_tokens': 61, 'output_tokens': 22, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
        "    (\"placeholder\", \"{messages}\"),\n",
        "])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\n",
        "    \"messages\": [\n",
        "        (\"human\",\"Translate this sentence from English to French: I love programming.\"),\n",
        "        (\"ai\", \"J'adore la programmation.\"),\n",
        "        (\"human\", \"What did you just say?\"),\n",
        "    ],\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmYyiplbZDwn"
      },
      "source": [
        "โปรดสังเกตว่าการรวมบทสนทนาก่อนหน้านี้ในเชนช่วยให้โมเดลสามารถตอบคำถามติดตามได้อย่างเข้าใจบริบท\n",
        "แม้ว่าวิธีนี้จะง่ายและใช้งานได้ แต่เมื่อนำแอปพลิเคชันของคุณไปใช้งานจริง คุณจะพบกับความท้าทายเพิ่มเติมที่เกี่ยวข้องกับการจัดการหน่วยความจำในระดับสูง เช่น:\n",
        "1. คุณจะต้องอัปเดตหน่วยความจำหลังจากการโต้ตอบทุกครั้ง โดยเป็นอะตอมิก (กล่าวคือ ไม่บันทึกเฉพาะคำถามหรือคำตอบเท่านั้นในกรณีที่เกิดความล้มเหลว)\n",
        "2. คุณจะต้องจัดเก็บหน่วยความจำเหล่านี้ในที่เก็บข้อมูลถาวร เช่น ฐานข้อมูลเชิงสัมพันธ์\n",
        "3. คุณจะต้องควบคุมจำนวนและข้อความใดที่จะถูกจัดเก็บไว้สำหรับการโต้ตอบในภายหลัง และจำนวนข้อความเหล่านี้ที่ถูกนำมาใช้สำหรับการโต้ตอบใหม่\n",
        "4. คุณจะต้องตรวจสอบและปรับเปลี่ยนสถานะนี้ (สำหรับตอนนี้ เพียงแค่รายการข้อความ) ภายนอกการเรียกใช้งาน LLM\n",
        "\n",
        "**ตอนนี้เราจะแนะนำเครื่องมือที่ดีกว่า ซึ่งจะช่วยในเรื่องนี้และในบทต่อไปทั้งหมด**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnjNCM5ZarG"
      },
      "source": [
        "# Introducing LangGraph\n",
        "\n",
        "\n",
        "**แนะนำ LangGraph**\n",
        "\n",
        "ตลอดช่วงที่เหลือของบทนี้และบทต่อไป เราจะเริ่มใช้ LangGraph ซึ่งเป็นไลบรารีโอเพนซอร์สที่เขียนโดย LangChain LangGraph ได้รับการออกแบบมาเพื่อให้ผู้พัฒนาสามารถใช้งานสถาปัตยกรรมความรู้แบบหลายตัวกระทำ หลายขั้นตอน และมีสถานะ ซึ่งเรียกว่า กราฟ คำศัพท์เหล่านี้บรรจุอยู่ในประโยคสั้น ๆ มาดูทีละคำกัน รูปนี้ แสดงให้เห็นถึงลักษณะหลายตัวกระทำ\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure4-3.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnIkSgTpZ70N"
      },
      "source": [
        "ทีมผู้เชี่ยวชาญสามารถสร้างสิ่งต่างๆ ร่วมกันได้ ซึ่งแต่ละคนไม่สามารถสร้างได้ด้วยตนเอง หลักการเดียวกันนี้ใช้ได้กับแอปพลิเคชัน LLM: พรอมต์ LLM (ยอดเยี่ยมสำหรับการสร้างคำตอบ การวางแผนงาน และอีกมากมาย) มีประสิทธิภาพมากขึ้นเมื่อจับคู่กับเครื่องมือค้นหา (ดีที่สุดในการค้นหาข้อเท็จจริงปัจจุบัน) หรือแม้แต่เมื่อจับคู่กับพรอมต์ LLM อื่นๆ เราได้เห็นนักพัฒนาสร้างแอปพลิเคชันที่น่าทึ่ง เช่น **Perplexity หรือ Arc** Search เมื่อพวกเขาผสมผสานส่วนประกอบทั้งสองนี้ (และส่วนอื่นๆ) เข้าด้วยกันในรูปแบบใหม่ๆ\n",
        "\n",
        "และเช่นเดียวกับทีมมนุษย์ที่ต้องการการประสานงานมากกว่าบุคคลหนึ่งที่ทำงานด้วยตนเองแอปพลิเคชันที่มีตัวกระทำหลายตัวจำเป็นต้องมีเลเยอร์การประสานงานเพื่อทำสิ่งเหล่านี้:\n",
        "- กำหนดตัวกระทำที่เกี่ยวข้อง (โหนดในกราฟ) และวิธีการส่งมอบงานระหว่างกัน (ขอบในกราฟนั้น)\n",
        "- จัดกำหนดการดำเนินการของแต่ละตัวกระทำในเวลาที่เหมาะสม โดยสามารถทำงานแบบขนานได้หากจำเป็น พร้อมผลลัพธ์ที่กำหนดได้\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT6zm5wyaFSH"
      },
      "source": [
        "รูปนี้ แสดงให้เห็นมิติหลายขั้นตอน\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"./pics/Figure4-4.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---\n",
        "\n",
        "เมื่อแต่ละตัวกระทำส่งมอบงานให้กับอีกตัวหนึ่ง (ตัวอย่างเช่น พรอมต์ LLM ขอผลลัพธ์ของคิวรีการค้นหาที่กำหนดจากเครื่องมือค้นหา) เราจำเป็นต้องเข้าใจการโต้ตอบระหว่างตัวกระทำหลายตัว เราจำเป็นต้องทราบว่าลำดับการเกิดขึ้นเป็นอย่างไร แต่ละตัวกระทำถูกเรียกใช้กี่ครั้ง เป็นต้น เพื่อที่จะทำเช่นนี้ เราสามารถจำลองปฏิสัมพันธ์ระหว่างตัวกระทำว่าเกิดขึ้นในหลายขั้นตอนที่ไม่ต่อเนื่องในเวลา เมื่อตัวกระทำหนึ่งส่งมอบงานให้กับตัวกระทำอื่น ผลลัพธ์ที่ได้คือการจัดกำหนดการขั้นตอนถัดไปของการคำนวณ และดำเนินต่อไปจนกว่าจะไม่มีตัวกระทำใดส่งมอบงานให้กับตัวกระทำอื่นอีก และเราได้ผลลัพธ์ขั้นสุดท้าย"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDd7pVzibjKW"
      },
      "source": [
        "รูปนี้ แสดงให้เห็นถึงลักษณะที่มีสถานะ\n",
        "\n",
        "---\n",
        "\n",
        "<img align=\"top\" src=\"../work/pics/Figure4-5.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "\n",
        "---\n",
        "\n",
        "การสื่อสารข้ามขั้นตอนจำเป็นต้องติดตามสถานะบางอย่าง มิฉะนั้น เมื่อคุณเรียกใช้ตัวกระทำ LLM ครั้งที่สอง คุณจะได้รับผลลัพธ์เช่นเดียวกับครั้งแรก ปรากฎว่าการดึงสถานะนี้ออกจากแต่ละตัวกระทำและให้ตัวกระทำทั้งหมดร่วมกันอัปเดตสถานะส่วนกลางเพียงสถานะเดียวมีประโยชน์มาก ด้วยสถานะส่วนกลางเพียงสถานะเดียว เราสามารถทำสิ่งต่อไปนี้ได้:\n",
        "ถ่ายภาพและจัดเก็บระหว่างหรือหลังการคำนวณแต่ละครั้ง\n",
        "หยุดชั่วคราวและดำเนินการต่อ ซึ่งทำให้การกู้คืนจากข้อผิดพลาดเป็นเรื่องง่าย\n",
        "ใช้การควบคุมของมนุษย์ในลูป (ข้อมูลเพิ่มเติมใน บทที่ 8)\n",
        "\n",
        "กราฟแต่ละกราฟจึงประกอบด้วยสิ่งต่อไปนี้:\n",
        "\n",
        "- **Status (สถานะ)**\n",
        "ข้อมูลที่ได้รับจากภายนอกแอปพลิเคชัน แก้ไข และสร้างโดยแอปพลิเคชันขณะที่แอปพลิเคชันกำลังทำงาน\n",
        "\n",
        "- **Node (โหนด)**\n",
        "แต่ละขั้นตอนที่จะดำเนินการ โหนดเป็นเพียงฟังก์ชัน Python/JS ซึ่งรับสถานะปัจจุบันเป็นอินพุตและสามารถส่งคืนการอัปเดตไปยังสถานะนั้น (นั่นคือ สามารถเพิ่มและแก้ไขหรือลบข้อมูลที่มีอยู่)\n",
        "\n",
        "- **Edge (ขอบ)**\n",
        "การเชื่อมต่อระหว่างโหนด ขอบกำหนดเส้นทางที่ดำเนินการจากโหนดแรกไปยังโหนดสุดท้าย และสามารถคงที่ (นั่นคือ หลังโหนด B ให้ไปที่โหนด D เสมอ) หรือมีเงื่อนไข (ประเมินฟังก์ชันเพื่อตัดสินใจโหนดถัดไปที่จะเข้าชมหลังโหนด C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7TnS5jHb60k"
      },
      "source": [
        "LangGraph มียูทิลิตี้สำหรับการแสดงภาพกราฟเหล่านี้ และมีคุณสมบัติมากมายสำหรับแก้จุดบกพร่องในการทำงานขณะพัฒนา กราฟเหล่านี้สามารถปรับใช้เพื่อให้บริการงานผลิตในระดับสูงได้อย่างง่ายดาย\n",
        "\n",
        "หากคุณทำตามคำแนะนำใน บทที่ 1 คุณจะมี LangGraph ติดตั้งอยู่แล้ว หากไม่ คุณสามารถติดตั้งได้โดยเรียกใช้คำสั่งต่อไปนี้ในเทอร์มินัลของคุณ:\n",
        "\n",
        "`pip install langgraph`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixIhYFCPdAog"
      },
      "source": [
        "เพื่อช่วยให้คุณคุ้นเคยกับการใช้ LangGraph เราจะสร้างแชทบอทอย่างง่ายโดยใช้ LangGraph ซึ่งเป็นตัวอย่างที่ดีเยี่ยมของสถาปัตยกรรมการเรียกใช้งาน LLM ที่ใช้ LLM เพียงครั้งเดียว แชทบอทนี้จะตอบกลับข้อความของผู้ใช้โดยตรง แม้ว่าจะง่าย แต่ก็แสดงให้เห็นถึงแนวคิดหลักของการสร้างด้วย LangGraph\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLm-itQydFMb"
      },
      "source": [
        "## Creating a StateGraph\n",
        "\n",
        "เริ่มต้นด้วยการสร้าง StateGraph เราจะเพิ่มโหนดเพื่อแทนการเรียกใช้งาน LLM:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M97Qcu-ac1u-"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "\t# Messages have the type \"list\". The `add_messages`\n",
        "    # function in the annotation defines how this state should\n",
        "    # be updated (in this case, it appends new messages to the\n",
        "    # list, rather than replacing the previous messages)\n",
        "\tmessages: Annotated[list, add_messages]\n",
        "\n",
        "builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlGChd8esJ6"
      },
      "source": [
        "**สิ่งแรกที่คุณทำเมื่อกำหนดกราฟคือการกำหนดสถานะของกราฟ** สถานะประกอบด้วยรูปร่างหรือรูปแบบของสถานะกราฟ รวมถึงฟังก์ชันลดทอนที่ระบุวิธีการใช้การอัปเดตกับสถานะ\n",
        "\n",
        "- ในตัวอย่างนี้ สถานะเป็นพจนานุกรมที่มีคีย์เดียว: `messages` คีย์ `messages` มีการระบุด้วยฟังก์ชันลดทอน `add_messages` ซึ่งบอกให้ LangGraph เพิ่มข้อความใหม่ลงในรายการที่มีอยู่ แทนที่จะเขียนทับ\n",
        "\n",
        "คีย์สถานะที่ไม่มีการระบุจะถูกเขียนทับด้วยการอัปเดตแต่ละครั้ง จัดเก็บค่าล่าสุด คุณสามารถเขียนฟังก์ชันลดทอนของคุณเอง ซึ่งเป็นเพียงฟังก์ชันที่รับอาร์กิวเมนต์เป็นอาร์กิวเมนต์\n",
        "- อาร์กิวเมนต์ 1 คือสถานะปัจจุบัน\n",
        "- อาร์กิวเมนต์ 2 คือค่าถัดไปที่จะถูกเขียนลงในสถานะ และควรส่งคืนสถานะถัดไป\n",
        "\n",
        "**นั่นคือ ผลลัพธ์ของการผสานสถานะปัจจุบันกับค่าใหม่ ตัวอย่างที่ง่ายที่สุดคือฟังก์ชันที่เพิ่มค่าถัดไปลงในรายการและส่งคืนรายการนั้น**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7C5YDrhfV0G"
      },
      "source": [
        "**ดังนั้นตอนนี้กราฟของเรารู้สองสิ่ง:**\n",
        "- ทุกโหนดที่เรากำหนดจะได้รับ State ปัจจุบันเป็นอินพุตและส่งคืนค่าที่อัปเดตสถานะนั้น\n",
        "- `messages` จะถูกเพิ่มลงในรายการปัจจุบัน แทนที่จะถูกเขียนทับโดยตรง สิ่งนี้ถูกสื่อสารผ่านฟังก์ชัน `add_message` ที่สร้างไว้ล่วงหน้าในไวยากรณ์ Annotated\n",
        "\n",
        "**ถัดไป เพิ่มโหนด chatbot โหนดแทนหน่วยงาน พวกมันมักจะเป็นเพียงฟังก์ชัน:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15zjcVoiZajM",
        "outputId": "18129c40-15b3-466a-a200-c8b7f600928f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x2180ef7ff50>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "def chatbot(state: State):\n",
        "  answer = model.invoke(state[\"messages\"])\n",
        "  return {\"messages\": [answer]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or Runnable to run\n",
        "builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v42VSH8ggE5N"
      },
      "source": [
        "โหนดนี้รับสถานะปัจจุบัน เรียกใช้งาน LLM หนึ่งครั้ง จากนั้นส่งคืนการอัปเดตไปยังสถานะที่มีข้อความใหม่ที่สร้างโดย LLM ฟังก์ชันลดทอน add_messages เพิ่มข้อความนี้ลงในข้อความที่มีอยู่ในสถานะอยู่แล้ว\n",
        "และในที่สุดเราก็เพิ่มขอบ:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6qSucQ3Zadc"
      },
      "outputs": [],
      "source": [
        "builder.add_edge(START, 'chatbot')\n",
        "builder.add_edge('chatbot', END)\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5eCfg94gQ5o"
      },
      "source": [
        "สิ่งนี้ทำได้หลายอย่าง:\n",
        "- บอกกราฟว่าจะเริ่มทำงานที่ใดทุกครั้งที่คุณเรียกใช้\n",
        "- สิ่งนี้สั่งให้กราฟออกจากตำแหน่งใด (อันที่จริงแล้วเป็นทางเลือก เนื่องจาก LangGraph จะหยุดการดำเนินการเมื่อไม่มีโหนดใดๆ ที่ต้องเรียกใช้)\n",
        "- รวบรวมกราฟเป็นอ็อบเจ็กต์ที่สามารถเรียกใช้ได้ ด้วยเมธอด invoke และ stream ที่คุ้นเคย\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdDe6Qo2ganb"
      },
      "source": [
        "เราสามารถวาดภาพแสดงภาพกราฟได้เช่นกัน:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "9ErEpr31hCfe",
        "outputId": "ae2449cc-7b2d-461f-8e20-4731966068f8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "image_bytes = graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
        "display(Image(data=image_bytes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmanJghhh0as"
      },
      "source": [
        "คุณสามารถเรียกใช้ได้ด้วยเมธอด `stream(`ที่คุ้นเคยซึ่งคุณได้เห็นในบทก่อนหน้า)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk2l5F9JgcHG",
        "outputId": "c0e98bf5-6ba5-495c-e3e0-fb19e750bfce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chatbot': {'messages': [AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b79e5718-86eb-4c0d-bc35-63d9d562fbbe-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "input = {\"messages\": [HumanMessage('hi!')]}\n",
        "for chunk in graph.stream(input):\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4HDl9RiRaG"
      },
      "source": [
        "สังเกตว่าอินพุตไปยังกราฟมีรูปร่างเหมือนกับ State object ที่เราได้กำหนดไว้ก่อนหน้านี้ นั่นคือ เราส่งรายการข้อความในคีย์ messages ของพจนานุกรม"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCVB4Gu6icM6"
      },
      "source": [
        "## Adding Memory to StateGraph\n",
        "\n",
        "LangGraph มีการคงอยู่ในตัว ซึ่งใช้ในลักษณะเดียวกันสำหรับกราฟที่ง่ายที่สุดไปจนถึงกราฟที่ซับซ้อนที่สุด มาดูกันว่าการนำไปใช้กับสถาปัตยกรรมแรกนี้เป็นอย่างไร เราจะรวบรวมกราฟของเราใหม่ โดยแนบเช็คพอยต์ ซึ่งเป็นอะแดปเตอร์จัดเก็บข้อมูลสำหรับ LangGraph LangGraph มาพร้อมกับคลาสพื้นฐานที่ผู้ใช้สามารถสร้างคลาสย่อยเพื่อสร้างอะแดปเตอร์สำหรับฐานข้อมูลที่ชื่นชอบของตนเอง ในขณะที่เขียน LangGraph มาพร้อมกับอะแดปเตอร์หลายตัวที่ดูแลรักษาโดย LangChain:\n",
        "\n",
        "- **อะแดปเตอร์ในหน่วยความจำ** ซึ่งเราจะใช้สำหรับตัวอย่างของเราที่นี่\n",
        "\n",
        "- **อะแดปเตอร์ SQLite** โดยใช้ฐานข้อมูลในกระบวนการที่เป็นที่นิยม เหมาะสำหรับแอปพลิเคชันภายในเครื่องและการทดสอบ\n",
        "\n",
        "- **อะแดปเตอร์ Postgres** ซึ่งได้รับการปรับให้เหมาะสมสำหรับฐานข้อมูลเชิงสัมพันธ์ที่เป็นที่นิยม และเหมาะสำหรับแอปพลิเคชันขนาดใหญ่\n",
        "\n",
        "นักพัฒนาหลายคนได้เขียนอะแดปเตอร์สำหรับระบบฐานข้อมูลอื่นๆ เช่น Redis หรือ MySQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XJfwXHph6eo"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = builder.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp3QSr48i5bQ"
      },
      "source": [
        "สิ่งนี้ส่งคืนอ็อบเจ็กต์ที่สามารถเรียกใช้ได้ด้วยเมธอดเดียวกับที่เราใช้ข้างต้น แต่ตอนนี้ มันจัดเก็บสถานะที่จุดสิ้นสุดของแต่ละขั้นตอน ดังนั้น การเรียกใช้งานทุกครั้งหลังจากครั้งแรกจะไม่เริ่มต้นจากสภาพที่ว่างเปล่า ทุกครั้งที่เรียกใช้กราฟ มันจะเริ่มต้นโดยใช้เช็คพอยต์เพื่อดึงสถานะล่าสุดที่บันทึกไว้ หากมี และรวมอินพุตใหม่เข้ากับสถานะก่อนหน้า จากนั้นจึงดำเนินการโหนดแรก\n",
        "มาดูความแตกต่างในการใช้งาน:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXHhUaqmi0jA",
        "outputId": "d037afac-55b5-420d-bbd2-4c105e89ff98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='e5cb52ec-6b6c-4739-b375-8003c288c938'),\n",
              "  AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6fce38ef-7d42-4748-8a69-e6006c18e6d0-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='d2af023b-8191-444a-a5b1-304982e28a2b'),\n",
              "  AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 37, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-181a9f2d-01fa-4e39-9698-c54c6eed182b-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='ae1ecc0a-d248-4b0f-95cd-4eeef7086c03'),\n",
              "  AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 57, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e45d507e-ff05-465a-b455-48d696e63787-0', usage_metadata={'input_tokens': 57, 'output_tokens': 11, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='78bc6e29-0b38-4bf3-bca3-a234fb3b7de6'),\n",
              "  AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-efef63f3-74e5-4485-8941-d8432922b5cf-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "result_1 = graph.invoke({\"messages\": [HumanMessage('hi, my name is Jack!')] },  thread1)\n",
        "# // { \"chatbot\": { \"messages\": [AIMessage(\"How can I help you, Jack?\")] } }\n",
        "\n",
        "result_2 = graph.invoke({\"messages\": [HumanMessage('what is my name?')] }, thread1)\n",
        "# // { \"chatbot\": { \"messages\": [AIMessage(\"Your name is Jack\")] } }\n",
        "result_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUlrv5lujkM0"
      },
      "source": [
        "สังเกตวัตถุ thread1 ซึ่งระบุปฏิสัมพันธ์ปัจจุบันว่าเป็นของประวัติการโต้ตอบโดยเฉพาะ ซึ่งใน LangGraph เรียกว่า `threads` เธรดถูกสร้างขึ้นโดยอัตโนมัติเมื่อใช้งานครั้งแรก สตริงใดๆ ก็ตามเป็นตัวระบุที่ถูกต้องสำหรับเธรด (โดยปกติจะใช้ Universally Unique Identifiers [UUIDs]) การมีอยู่ของเธรดช่วยให้คุณบรรลุเป้าหมายสำคัญในแอปพลิเคชัน LLM ของคุณ\n",
        "\n",
        "**ตอนนี้สามารถใช้งานได้โดยผู้ใช้หลายคนที่มีการสนทนาที่เป็นอิสระซึ่งไม่เคยผสมกัน**\n",
        "\n",
        "เช่นเดียวกับก่อนหน้านี้ โหนด chatbot ถูกเรียกใช้ครั้งแรกด้วยข้อความเดียว ข้อความที่เราเพิ่งส่งผ่านและส่งคืนข้อความอื่นซึ่งทั้งสองข้อความจะถูกบันทึกไว้ในสถานะ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZhZVpflnZX"
      },
      "source": [
        "\n",
        "ครั้งที่สองที่เราเรียกใช้กราฟ บนเธรดเดียวกัน\n",
        "โหนด `chatbot` ถูกเรียกใช้ด้วยข้อความสามข้อความ ข้อความสองข้อความที่บันทึกไว้จากการเรียกใช้ครั้งแรก และคำถามถัดไปจากผู้ใช้ นี่คือสาระสำคัญของหน่วยความจำ: สถานะก่อนหน้ายังคงอยู่ ซึ่งทำให้สามารถตอบคำถามเกี่ยวกับสิ่งที่พูดมาก่อนหน้านี้ได้ (และทำสิ่งที่น่าสนใจอีกมากมาย ซึ่งเราจะได้เห็นเพิ่มเติมในภายหลัง)\n",
        "\n",
        "คุณยังสามารถตรวจสอบและอัปเดตสถานะโดยตรงได้ มาดูกันว่าอย่างไร:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1tp7af6jE7R",
        "outputId": "af77359d-460e-4d04-fd90-0521972e3126"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='e5cb52ec-6b6c-4739-b375-8003c288c938'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6fce38ef-7d42-4748-8a69-e6006c18e6d0-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='d2af023b-8191-444a-a5b1-304982e28a2b'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 37, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-181a9f2d-01fa-4e39-9698-c54c6eed182b-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='ae1ecc0a-d248-4b0f-95cd-4eeef7086c03'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 57, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e45d507e-ff05-465a-b455-48d696e63787-0', usage_metadata={'input_tokens': 57, 'output_tokens': 11, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='78bc6e29-0b38-4bf3-bca3-a234fb3b7de6'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-efef63f3-74e5-4485-8941-d8432922b5cf-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efda0b1-0a86-69f7-800a-6e93554ad08f'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-efef63f3-74e5-4485-8941-d8432922b5cf-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '1', 'step': 10, 'parents': {}}, created_at='2025-01-24T04:24:13.074277+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efda0b1-03fe-66a1-8009-b493b617ac7d'}}, tasks=())"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.get_state(thread1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1H7apOLl5VL",
        "outputId": "fd2c31b2-1772-4e88-be3d-f855cce510a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efda0cc-5c3f-6a90-800b-aab2b827fb08'}}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.update_state(thread1, {\"messages\": [HumanMessage('I like LLMs!')]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe4oOQdmmEhP",
        "outputId": "bc95e043-4f33-4113-e3e8-e1709a14f51b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='e5cb52ec-6b6c-4739-b375-8003c288c938'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6fce38ef-7d42-4748-8a69-e6006c18e6d0-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='d2af023b-8191-444a-a5b1-304982e28a2b'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 37, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-181a9f2d-01fa-4e39-9698-c54c6eed182b-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='ae1ecc0a-d248-4b0f-95cd-4eeef7086c03'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 57, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e45d507e-ff05-465a-b455-48d696e63787-0', usage_metadata={'input_tokens': 57, 'output_tokens': 11, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='78bc6e29-0b38-4bf3-bca3-a234fb3b7de6'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-efef63f3-74e5-4485-8941-d8432922b5cf-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='I like LLMs!', additional_kwargs={}, response_metadata={}, id='7aa0241a-6900-4bb6-80df-ef86b5d4a47d')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efda0cc-5c3f-6a90-800b-aab2b827fb08'}}, metadata={'source': 'update', 'writes': {'chatbot': {'messages': [HumanMessage(content='I like LLMs!', additional_kwargs={}, response_metadata={}, id='7aa0241a-6900-4bb6-80df-ef86b5d4a47d')]}}, 'thread_id': '1', 'step': 11, 'parents': {}}, created_at='2025-01-24T04:36:26.419265+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efda0b1-0a86-69f7-800a-6e93554ad08f'}}, tasks=())"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.get_state(thread1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBpFbIJcnuKn"
      },
      "source": [
        "นี่จะเพิ่มข้อความอีกหนึ่งข้อความลงในรายการข้อความในสถานะ เพื่อใช้ในครั้งต่อไปที่คุณเรียกใช้กราฟบนเธรดนี้\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcL22Q-KonFI"
      },
      "source": [
        "# Modifying Chat History\n",
        "**การแก้ไขประวัติการแชท**\n",
        "\n",
        "ในหลายกรณี ข้อความประวัติการแชทไม่ได้อยู่ในสถานะหรือรูปแบบที่ดีที่สุดสำหรับการสร้างการตอบกลับที่ถูกต้องจากโมเดล เพื่อแก้ไขปัญหานี้ เราสามารถแก้ไขประวัติการแชทได้สามวิธีหลัก ได้แก่ **การตัดแต่ง การกรอง และการผสานข้อความ**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdNBIDqlo2sc"
      },
      "source": [
        "## Trimming Messages​\n",
        "\n",
        "**การตัดแต่งข้อความ**\n",
        "\n",
        "LLM มีหน้าต่างบริบทที่จำกัด กล่าวคือ จำนวนโทเค็นสูงสุดที่สามารถรับเป็นพรอมต์ได้ ดังนั้น พรอมต์สุดท้ายที่ส่งไปยังโมเดลไม่ควรเกินขีดจำกัดนั้น (เฉพาะเจาะจงสำหรับแต่ละโหมด) เนื่องจากโมเดลจะปฏิเสธพรอมต์ที่ยาวเกินไปหรือตัดทอน นอกจากนี้ ข้อมูลพรอมต์ที่มากเกินไปอาจทำให้โมเดลเสียสมาธิและนำไปสู่การหลอน\n",
        "\n",
        "วิธีแก้ปัญหาที่มีประสิทธิภาพสำหรับปัญหานี้คือการจำกัดจำนวนข้อความที่ดึงมาจากประวัติการแชทและเพิ่มลงในพรอมต์ ในทางปฏิบัติ เราจำเป็นต้องโหลดและจัดเก็บเฉพาะข้อความล่าสุดเท่านั้น มาใช้ประวัติการแชทตัวอย่างที่มีข้อความโหลดไว้ล่วงหน้าบางส่วนกัน\n",
        "\n",
        "โชคดีที่ LangChain มีตัวช่วย `trim_messages` ในตัว ซึ่งรวมกลยุทธ์ต่างๆ เพื่อตอบสนองความต้องการเหล่านี้ ตัวอย่างเช่น ตัวช่วยตัวตัดแต่งช่วยให้สามารถระบุจำนวนโทเค็นที่เราต้องการเก็บหรือลบออกจากประวัติการแชทได้\n",
        "\n",
        "นี่คือตัวอย่างที่ดึงโทเค็นล่าสุด `max_tokens` ในรายการข้อความโดยการตั้งค่าพารามิเตอร์กลยุทธ์เป็น `\"last\"`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f1iK8DHmRbd",
        "outputId": "f876d770-449a-4d8b-ed96-d5661e8269a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"what's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=65,\n",
        "    strategy=\"last\",\n",
        "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"what's 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoCcj-DipyfJ"
      },
      "source": [
        "**หมายเหตุต่อไปนี้:**\n",
        "- พารามิเตอร์ `strategy` ควบคุมว่าจะเริ่มต้นจากจุดเริ่มต้นหรือจุดสิ้นสุดของรายการ โดยปกติ คุณจะต้องการให้ความสำคัญกับข้อความล่าสุดและตัดข้อความที่เก่ากว่าออกหากไม่พอดี กล่าวคือ เริ่มต้นจากจุดสิ้นสุดของรายการ สำหรับพฤติกรรมนี้ เลือกค่า last ตัวเลือกอื่นที่พร้อมใช้งานคือ first ซึ่งจะให้ความสำคัญกับข้อความที่เก่าที่สุดและตัดข้อความที่ใหม่กว่าออกหากไม่พอดี\n",
        "- `token_counter` คือ LLM หรือโมเดลแชท ซึ่งจะใช้ในการนับโทเค็นโดยใช้ตัวโทเคไนเซอร์ที่เหมาะสมกับโมเดลนั้น\n",
        "เราสามารถเพิ่มพารามิเตอร์ `include_system=True` เพื่อให้แน่ใจว่าตัวตัดแต่งจะเก็บข้อความระบบไว้\n",
        "- พารามิเตอร์ `allow_partial` กำหนดว่าจะตัดเนื้อหาของข้อความสุดท้ายเพื่อให้พอดีกับขีดจำกัดหรือไม่ ในตัวอย่างของเรา เราตั้งค่านี้เป็น `false` ซึ่งจะลบข้อความที่ส่งผลให้เกินขีดจำกัดออกทั้งหมด\n",
        "- พารามิเตอร์ `start_on=\"human\"` ช่วยให้แน่ใจว่าเราจะไม่ลบ AIMessage (นั่นคือ การตอบกลับจากโมเดล) โดยไม่ลบ `HumanMessage` ที่สอดคล้องกัน (คำถามสำหรับการตอบกลับนั้น)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZifP0IqlSr"
      },
      "source": [
        "## Filtering Messages\n",
        "**การกรองข้อความ**\n",
        "เมื่อรายการข้อความประวัติการแชทเติบโตขึ้น อาจมีการใช้ประเภท เชนย่อย และโมเดลที่หลากหลายมากขึ้น ตัวช่วย filter_messages ของ LangChain ช่วยให้การกรองข้อความประวัติการแชทตามประเภท ID หรือชื่อทำได้ง่ายขึ้น\n",
        "\n",
        "นี่คือตัวอย่างที่เรากรองสำหรับข้อความของมนุษย์:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6d0rj1wpYvK",
        "outputId": "4232efaf-3bc3-45a0-b92b-ed9cce3c863a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
              " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    filter_messages,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
        "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
        "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
        "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
        "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
        "]\n",
        "\n",
        "filter_messages(messages, include_types=\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9URb-Avvq8gf"
      },
      "source": [
        "มาลองดูตัวอย่างอื่นที่เรากรองเพื่อไม่รวมผู้ใช้และ ID และรวมประเภทข้อความ:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMkUdwXYquI0",
        "outputId": "be3e458e-504f-4543-ef7c-07d5c9705a4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n[HumanMessage(content='example input', name='example_user', id='2'),\\n HumanMessage(content='real input', name='bob', id='4'),\\n AIMessage(content='real output', name='alice', id='5')]\\n\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n",
        "\n",
        "\"\"\"\n",
        "[SystemMessage(content='you are a good assistant', id='1'),\n",
        "HumanMessage(content='real input', name='bob', id='4'),\n",
        "AIMessage(content='real output', name='alice', id='5')]\n",
        "\"\"\"\n",
        "\n",
        "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])\n",
        "\n",
        "\"\"\"\n",
        "[HumanMessage(content='example input', name='example_user', id='2'),\n",
        " HumanMessage(content='real input', name='bob', id='4'),\n",
        " AIMessage(content='real output', name='alice', id='5')]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezOhmaxSrQHY"
      },
      "source": [
        "ตัวช่วย `filter_messages` ยังสามารถใช้ได้ทั้งแบบบอกและแบบประกาศ ซึ่งทำให้การประสมประสานกับส่วนประกอบอื่นๆ ในเชนทำได้ง่าย:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqbo3mbmrPvq"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI()\n",
        "\n",
        "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
        "\n",
        "chain = filter_ | model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvYx8q2wriIH"
      },
      "source": [
        "## Merging Consecutive Messages\n",
        "\n",
        "**การรวมข้อความต่อเนื่อง**\n",
        "\n",
        "มีบางโมเดลที่ไม่รองรับอินพุต รวมถึงข้อความต่อเนื่องประเภทเดียวกัน ตัวอย่างเช่น โมเดลแชท Anthropic ยูทิลิตี้ merge_message_runs ของ LangChain ช่วยให้การรวมข้อความต่อเนื่องประเภทเดียวกันทำได้ง่าย:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfsJTDIPrhjz",
        "outputId": "a28e0379-fc12-4457-c5d8-d526bc68f7b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's calledlangchain\"}, 'and who is harrison chasing anyway'], additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    merge_message_runs,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"you're a good assistant.\"),\n",
        "    SystemMessage(\"you always respond with a joke.\"),\n",
        "    HumanMessage([{\"type\": \"text\", \"text\": \"i wonder why it's calledlangchain\"}]),\n",
        "    HumanMessage(\"and who is harrison chasing anyway\"),\n",
        "    AIMessage('Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'),\n",
        "    AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),\n",
        "]\n",
        "\n",
        "merge_message_runs(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xRdq4VJsPiC"
      },
      "source": [
        "สังเกตว่าหากเนื้อหาของข้อความใดข้อความหนึ่งที่จะรวมกันเป็นรายการของบล็อกเนื้อหา ข้อความที่รวมกันจะมีรายการของบล็อกเนื้อหา และหากข้อความทั้งสองที่จะรวมกันมีเนื้อหาเป็นสตริง สตริงเหล่านั้นจะถูกต่อกันด้วยอักขระใหม่บรรทัด\n",
        "\n",
        "ตัวช่วย `merge_message_runs` สามารถใช้ได้ทั้งแบบบอกและแบบประกาศ ซึ่งทำให้การประสมประสานกับส่วนประกอบอื่นๆ ในเชนทำได้ง่าย:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxti8OsUrAcm"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI()\n",
        "merger = merge_message_runs()\n",
        "chain = merger | model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nau7tKzssfiT"
      },
      "source": [
        "# Summary\n",
        "\n",
        "**สรุป**\n",
        "\n",
        "บทนี้กล่าวถึงพื้นฐานของการสร้างระบบหน่วยความจำอย่างง่ายที่ช่วยให้แชทบอท AI ของคุณสามารถจำการสนทนากับผู้ใช้ได้ เราได้กล่าวถึงวิธีการอัตโนมัติในการจัดเก็บและอัปเดตประวัติการแชทโดยใช้ LangGraph เพื่อให้ง่ายขึ้น นอกจากนี้ เรายังได้กล่าวถึงความสำคัญของการแก้ไขประวัติการแชทและสำรวจกลยุทธ์ต่างๆ ในการตัดแต่ง กรอง และสรุปข้อความแชท\n",
        "\n",
        "**ใน บทที่ 5 คุณจะเริ่มเรียนรู้วิธีการเปิดใช้งานแชทบอท AI ของคุณให้ทำมากกว่าการแชทกลับ เช่น การตัดสินใจ เลือกการกระทำ และสะท้อนการพูดในอดีตของตนเอง**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
